[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi, my name is Pranav. The purpose of this blog is to document my projects and thoughts."
  },
  {
    "objectID": "posts/fastai-lesson5/fastai-lesson5.html",
    "href": "posts/fastai-lesson5/fastai-lesson5.html",
    "title": "FastAI Lesson 5: From-scratch model",
    "section": "",
    "text": "All of this code was written by Jeremy Howard and the FastAI team and I modified it slightly to include my own print statements and additional helper functions based on Jeremy’s code. This is the source for the original code Linear model and neural net from scratch and Why you should use a framework."
  },
  {
    "objectID": "posts/fastai-lesson5/fastai-lesson5.html#acknowledgements",
    "href": "posts/fastai-lesson5/fastai-lesson5.html#acknowledgements",
    "title": "FastAI Lesson 5: From-scratch model",
    "section": "",
    "text": "All of this code was written by Jeremy Howard and the FastAI team and I modified it slightly to include my own print statements and additional helper functions based on Jeremy’s code. This is the source for the original code Linear model and neural net from scratch and Why you should use a framework."
  },
  {
    "objectID": "posts/fastai-lesson5/fastai-lesson5.html#summary",
    "href": "posts/fastai-lesson5/fastai-lesson5.html#summary",
    "title": "FastAI Lesson 5: From-scratch model",
    "section": "Summary",
    "text": "Summary\nIn this lesson, Jeremy goes over training a model from scratch using a linear model, neural network and deep learning before finally walking through training a model using fastai + pytorch and an ensemble. This lesson is actually lesson 3, lesson 5 and part of lesson 6 so I had to go back to review lesson 3 to make sure I understood the material for lesson 5. I highly recommend going over lesson 3 and chapter 4 before this lesson because Jeremy doesn’t go too deep into the meaning of tensor shape and rank as he does in chapter 4.\nThis lesson was really exciting from the programming side because I learned more about python and numerical programming with partials, broadcasting, data cleaning with pandas, and feature engineering."
  },
  {
    "objectID": "posts/fastai-lesson5/fastai-lesson5.html#titanic---machine-learning-from-disaster",
    "href": "posts/fastai-lesson5/fastai-lesson5.html#titanic---machine-learning-from-disaster",
    "title": "FastAI Lesson 5: From-scratch model",
    "section": "Titanic - Machine Learning From Disaster",
    "text": "Titanic - Machine Learning From Disaster\nThe Titanic - Machine Learning from Disaster Competition is used as the case study for this lesson. More information about the data can be found here Titanic - Machine Learning from Disaster."
  },
  {
    "objectID": "posts/fastai-lesson5/fastai-lesson5.html#load-data-and-libraries",
    "href": "posts/fastai-lesson5/fastai-lesson5.html#load-data-and-libraries",
    "title": "FastAI Lesson 5: From-scratch model",
    "section": "Load Data and Libraries",
    "text": "Load Data and Libraries\n\n# import libraries and files\n\n# required libraries + packages for any ml/data science project\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport torch\n\n# fastai library contains all the packages above and wraps them in the fastai library\n!pip install -Uqq fastai\n\n# kaggle API package install\n!pip install kaggle\n\nRequirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (1.5.16)\nRequirement already satisfied: six&gt;=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle) (1.16.0)\nRequirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from kaggle) (2023.11.17)\nRequirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.8.2)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.31.0)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kaggle) (4.66.1)\nRequirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle) (8.0.1)\nRequirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.0.7)\nRequirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle) (6.1.0)\nRequirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach-&gt;kaggle) (0.5.1)\nRequirement already satisfied: text-unidecode&gt;=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify-&gt;kaggle) (1.3)\nRequirement already satisfied: charset-normalizer&lt;4,&gt;=2 in /usr/local/lib/python3.10/dist-packages (from requests-&gt;kaggle) (3.3.2)\nRequirement already satisfied: idna&lt;4,&gt;=2.5 in /usr/local/lib/python3.10/dist-packages (from requests-&gt;kaggle) (3.6)\n\n\n\nfrom fastai.imports import *\nimport os\nfrom pathlib import Path\nimport zipfile\n\n\n'''Function for loading kaggle datasets locally or on kaggle\nReturns a local path to data files\n- input: Kaggle API Login Credentials, Kaggle Contest Name '''\ndef loadData(creds, dataFile):\n    # variable to check whether we're running on kaggle website or not\n    iskaggle = os.environ.get('KAGGLE_KERNEL_RUN_TYPE', '')\n\n    # path for kaggle API credentials\n    cred_path = Path('~/.kaggle/kaggle.json').expanduser()\n\n    if not cred_path.exists():\n        cred_path.parent.mkdir(exist_ok=True)\n        cred_path.write_text(creds)\n        cred_path.chmod(0o600)\n\n    # Download data from Kaggle to path and extract files at path location\n\n    # local machine\n    path = Path(dataFile)\n    if not iskaggle and not path.exists():\n        import kaggle\n        kaggle.api.competition_download_cli(str(path))\n        zipfile.ZipFile(f'{path}.zip').extractall(path)\n\n    # kaggle\n    if iskaggle:\n        fileName = '../input/' + dataFile\n        path = fileName\n\n    return path\n\n\ncreds = '{\"username\":\"greenvegetable320\",\"key\":\"f56b8411bbf4c1ca7832e004bc998125\"}'\ndataFile = 'titanic'\npath = loadData(creds, dataFile)\n\nDownloading titanic.zip to /content\n\n\n\n100%|██████████| 34.1k/34.1k [00:00&lt;00:00, 38.6MB/s]\n\n\n\n# check data files\n!ls {path}\n\ngender_submission.csv  test.csv  train.csv\n\n\n\n# set up default settings\nimport warnings, logging, torch\ntorch.set_printoptions(linewidth=140, sci_mode=False, edgeitems=7)\npd.set_option('display.width', 140)\nwarnings.simplefilter('ignore')\nlogging.disable(logging.WARNING)"
  },
  {
    "objectID": "posts/fastai-lesson5/fastai-lesson5.html#problem-statement",
    "href": "posts/fastai-lesson5/fastai-lesson5.html#problem-statement",
    "title": "FastAI Lesson 5: From-scratch model",
    "section": "Problem Statement",
    "text": "Problem Statement\nProblem: what sorts of people were more likely to survive the Titanic Disaster - use machine learning to create a model that predicts which passengers survived the Titanic Shipwreck using passenger data(name, age, gender, socio-economic class, etc)\nTraining Data - contains subset of the passengers on board Titanic (891 passengers) with information on whether they survived or not(ground truth)\nTest(Inference) Data - contains same information as train data but does disclose the ground truth (whether passenger survived or not) - using patterns found in train data, predict whether the other 418 passsengers on board (test data) survived\nEvaluation Goal - Predict if a passenger survived the sinking of the titanic or not - For each value in test set, predict 0 or 1 value for the variable\nEvaluation Metric - Score is the percentage of passenger correctly predicted (accuracy)\nSubmission Format - PassengerID, Survived (contains binary predictions: 1 for survived, 0 for deceased)"
  },
  {
    "objectID": "posts/fastai-lesson5/fastai-lesson5.html#exploratory-data-analysis",
    "href": "posts/fastai-lesson5/fastai-lesson5.html#exploratory-data-analysis",
    "title": "FastAI Lesson 5: From-scratch model",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\n\nExploratory Data Analysis: Data Processing\n\n# load data and view data\ndf = pd.read_csv(path/'train.csv')\ndf\n\n\n  \n    \n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Thayer)\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n886\n887\n0\n2\nMontvila, Rev. Juozas\nmale\n27.0\n0\n0\n211536\n13.0000\nNaN\nS\n\n\n887\n888\n1\n1\nGraham, Miss. Margaret Edith\nfemale\n19.0\n0\n0\n112053\n30.0000\nB42\nS\n\n\n888\n889\n0\n3\nJohnston, Miss. Catherine Helen \"Carrie\"\nfemale\nNaN\n1\n2\nW./C. 6607\n23.4500\nNaN\nS\n\n\n889\n890\n1\n1\nBehr, Mr. Karl Howell\nmale\n26.0\n0\n0\n111369\n30.0000\nC148\nC\n\n\n890\n891\n0\n3\nDooley, Mr. Patrick\nmale\n32.0\n0\n0\n370376\n7.7500\nNaN\nQ\n\n\n\n\n\n891 rows × 12 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\n\n# Count number of missing values in each category\n# - 1 - represents NaN value\n# - summation tells how many NaN values are in each column\ndf.isna().sum()\n\nPassengerId      0\nSurvived         0\nPclass           0\nName             0\nSex              0\nAge            177\nSibSp            0\nParch            0\nTicket           0\nFare             0\nCabin          687\nEmbarked         2\ndtype: int64\n\n\n\n# Replace NaN with mode\n# - replace missing values with something meaningful -&gt; mean, median, mode etc\n# in case of ties select first value\n\n\n# find modes in different categories\nmodes = df.mode().iloc[0]\nmodes\n\nPassengerId                      1\nSurvived                       0.0\nPclass                         3.0\nName           Abbing, Mr. Anthony\nSex                           male\nAge                           24.0\nSibSp                          0.0\nParch                          0.0\nTicket                        1601\nFare                          8.05\nCabin                      B96 B98\nEmbarked                         S\nName: 0, dtype: object\n\n\n\n# Replace NaN with mode and verify there are no NaN values\ndf.fillna(modes, inplace=True)\ndf.isna().sum()\n\nPassengerId    0\nSurvived       0\nPclass         0\nName           0\nSex            0\nAge            0\nSibSp          0\nParch          0\nTicket         0\nFare           0\nCabin          0\nEmbarked       0\ndtype: int64\n\n\n\n\nExploratory Data Analysis: Numeric Data\n\n# summary of all numeric columns in data\ndf.describe(include=(np.number))\n\n\n  \n    \n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nAge\nSibSp\nParch\nFare\n\n\n\n\ncount\n891.000000\n891.000000\n891.000000\n891.000000\n891.000000\n891.000000\n891.000000\n\n\nmean\n446.000000\n0.383838\n2.308642\n28.566970\n0.523008\n0.381594\n32.204208\n\n\nstd\n257.353842\n0.486592\n0.836071\n13.199572\n1.102743\n0.806057\n49.693429\n\n\nmin\n1.000000\n0.000000\n1.000000\n0.420000\n0.000000\n0.000000\n0.000000\n\n\n25%\n223.500000\n0.000000\n2.000000\n22.000000\n0.000000\n0.000000\n7.910400\n\n\n50%\n446.000000\n0.000000\n3.000000\n24.000000\n0.000000\n0.000000\n14.454200\n\n\n75%\n668.500000\n1.000000\n3.000000\n35.000000\n1.000000\n0.000000\n31.000000\n\n\nmax\n891.000000\n1.000000\n3.000000\n80.000000\n8.000000\n6.000000\n512.329200\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\n# histogram of fare data\n# long tail to the right histogram\ndf['Fare'].hist();\n\n\n\n\n\n# log histogram of fare data to center data\ndf['LogFare'] = np.log(df['Fare'] + 1)\ndf['LogFare'].hist();\n\n\n\n\n\n\nExploratory Data Analysis: Categorical Data\n\n# Passenger Classes\npclasses = sorted(df.Pclass.unique())\npclasses\n\n[1, 2, 3]\n\n\n\n# non-numeric data summary\ndf.describe(include=[object])\n\n\n  \n    \n\n\n\n\n\n\nName\nSex\nTicket\nCabin\nEmbarked\n\n\n\n\ncount\n891\n891\n891\n891\n891\n\n\nunique\n891\n2\n681\n147\n3\n\n\ntop\nBraund, Mr. Owen Harris\nmale\n347082\nB96 B98\nS\n\n\nfreq\n1\n577\n7\n691\n646\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\n# Convert Categorical Data to Numerical Data - Dummy Variables\n# - Dummy variable is a column that contains 1 where a particular columns contains a particular value\n# and 0 otherwise\n\n\n# create dummy variables for categorical variables\ndf = pd.get_dummies(df, columns=[\"Sex\",\"Pclass\",\"Embarked\"])\ndf.columns\n\nIndex(['PassengerId', 'Survived', 'Name', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'LogFare', 'Sex_female', 'Sex_male',\n       'Pclass_1', 'Pclass_2', 'Pclass_3', 'Embarked_C', 'Embarked_Q', 'Embarked_S'],\n      dtype='object')\n\n\n\n# new data with dummy variables\nadded_cols = ['Sex_male', 'Sex_female', 'Pclass_1', 'Pclass_2', 'Pclass_3', 'Embarked_C', 'Embarked_Q', 'Embarked_S']\ndf[added_cols].head()\n\n\n  \n    \n\n\n\n\n\n\nSex_male\nSex_female\nPclass_1\nPclass_2\nPclass_3\nEmbarked_C\nEmbarked_Q\nEmbarked_S\n\n\n\n\n0\n1\n0\n0\n0\n1\n0\n0\n1\n\n\n1\n0\n1\n1\n0\n0\n1\n0\n0\n\n\n2\n0\n1\n0\n0\n1\n0\n0\n1\n\n\n3\n0\n1\n1\n0\n0\n0\n0\n1\n\n\n4\n1\n0\n0\n0\n1\n0\n0\n1"
  },
  {
    "objectID": "posts/fastai-lesson5/fastai-lesson5.html#linear-model",
    "href": "posts/fastai-lesson5/fastai-lesson5.html#linear-model",
    "title": "FastAI Lesson 5: From-scratch model",
    "section": "Linear Model",
    "text": "Linear Model\n\nLinear Model Variables\n\nIndependent Variables - predictors: all continuous variables + dummy variables\nDependent Variables - target: survived\n\n\n# Linear Model Data Processing\nfrom torch import tensor\n\n# independent(predictors)\nindep_cols = ['Age', 'SibSp', 'Parch', 'LogFare'] + added_cols\nt_indep = tensor(df[indep_cols].values, dtype=torch.float)\n\n# dependent(target) variables - Survived\nt_dep = tensor(df.Survived)\n\n# print information about tensors\nprint(f\"Indendent Tensors Shape: {t_indep.shape}\")\nprint(f\"Independent Tensors Rank: {len(t_indep.shape)}\")\nprint(f\"Dependent Tensors Shape: {t_dep.shape}\")\nprint(f\"Dependent Tensors Rank: {len(t_dep.shape)}\")\n\nIndendent Tensors Shape: torch.Size([891, 12])\nIndependent Tensors Rank: 2\nDependent Tensors Shape: torch.Size([891])\nDependent Tensors Rank: 1\n\n\n\n# do not use this in practice -&gt; this is to ensure reproducibility in experimentation\n# do not seed manually when done with experimentation\ntorch.manual_seed(442)\n\nn_coeff = t_indep.shape[1]\ncoeffs = torch.rand(n_coeff) - 0.5\nprint(f\"Coefficients shape: {coeffs.shape}\")\nprint(f\"Coefficients rank: {len(coeffs.shape)}\")\nprint(f\"Coefficients: {coeffs}\")\n\nCoefficients shape: torch.Size([12])\nCoefficients rank: 1\nCoefficients: tensor([-0.4629,  0.1386,  0.2409, -0.2262, -0.2632, -0.3147,  0.4876,  0.3136,  0.2799, -0.4392,  0.2103,  0.3625])\n\n\n\n # element wise multiplication using broadcasting - multiply every row by coefficients\n#  - can be interpreted as looping 891 times and multiplying each row value by corresponding coeff value\nt_indep * coeffs\n\ntensor([[-10.1838,   0.1386,   0.0000,  -0.4772,  -0.2632,  -0.0000,   0.0000,   0.0000,   0.2799,  -0.0000,   0.0000,   0.3625],\n        [-17.5902,   0.1386,   0.0000,  -0.9681,  -0.0000,  -0.3147,   0.4876,   0.0000,   0.0000,  -0.4392,   0.0000,   0.0000],\n        [-12.0354,   0.0000,   0.0000,  -0.4950,  -0.0000,  -0.3147,   0.0000,   0.0000,   0.2799,  -0.0000,   0.0000,   0.3625],\n        [-16.2015,   0.1386,   0.0000,  -0.9025,  -0.0000,  -0.3147,   0.4876,   0.0000,   0.0000,  -0.0000,   0.0000,   0.3625],\n        [-16.2015,   0.0000,   0.0000,  -0.4982,  -0.2632,  -0.0000,   0.0000,   0.0000,   0.2799,  -0.0000,   0.0000,   0.3625],\n        [-11.1096,   0.0000,   0.0000,  -0.5081,  -0.2632,  -0.0000,   0.0000,   0.0000,   0.2799,  -0.0000,   0.2103,   0.0000],\n        [-24.9966,   0.0000,   0.0000,  -0.8973,  -0.2632,  -0.0000,   0.4876,   0.0000,   0.0000,  -0.0000,   0.0000,   0.3625],\n        ...,\n        [-11.5725,   0.0000,   0.0000,  -0.4717,  -0.2632,  -0.0000,   0.0000,   0.0000,   0.2799,  -0.0000,   0.0000,   0.3625],\n        [-18.0531,   0.0000,   1.2045,  -0.7701,  -0.0000,  -0.3147,   0.0000,   0.0000,   0.2799,  -0.0000,   0.2103,   0.0000],\n        [-12.4983,   0.0000,   0.0000,  -0.5968,  -0.2632,  -0.0000,   0.0000,   0.3136,   0.0000,  -0.0000,   0.0000,   0.3625],\n        [ -8.7951,   0.0000,   0.0000,  -0.7766,  -0.0000,  -0.3147,   0.4876,   0.0000,   0.0000,  -0.0000,   0.0000,   0.3625],\n        [-11.1096,   0.1386,   0.4818,  -0.7229,  -0.0000,  -0.3147,   0.0000,   0.0000,   0.2799,  -0.0000,   0.0000,   0.3625],\n        [-12.0354,   0.0000,   0.0000,  -0.7766,  -0.2632,  -0.0000,   0.4876,   0.0000,   0.0000,  -0.4392,   0.0000,   0.0000],\n        [-14.8128,   0.0000,   0.0000,  -0.4905,  -0.2632,  -0.0000,   0.0000,   0.0000,   0.2799,  -0.0000,   0.2103,   0.0000]])\n\n\n\n# Sum of each row are dominated by Age since Age is larger than all the other variables\n# center data to between 0 and 1 by averaging each column\n\n# find max val in each row\nvals,indices = t_indep.max(dim=0)\nprint(f\"vals shape {vals.shape}\")\nprint(f\"vals rank: {len(vals.shape)}\")\n\n# - can be interpreted as looping 891 times and dividing each row value by corresponding value in vals\nt_indep = t_indep / vals\n\nvals shape torch.Size([12])\nvals rank: 1\n\n\n\n# Recompute coefficients using new new centered independent variable values\nt_indep * coeffs\n\ntensor([[-0.1273,  0.0173,  0.0000, -0.0765, -0.2632, -0.0000,  0.0000,  0.0000,  0.2799, -0.0000,  0.0000,  0.3625],\n        [-0.2199,  0.0173,  0.0000, -0.1551, -0.0000, -0.3147,  0.4876,  0.0000,  0.0000, -0.4392,  0.0000,  0.0000],\n        [-0.1504,  0.0000,  0.0000, -0.0793, -0.0000, -0.3147,  0.0000,  0.0000,  0.2799, -0.0000,  0.0000,  0.3625],\n        [-0.2025,  0.0173,  0.0000, -0.1446, -0.0000, -0.3147,  0.4876,  0.0000,  0.0000, -0.0000,  0.0000,  0.3625],\n        [-0.2025,  0.0000,  0.0000, -0.0798, -0.2632, -0.0000,  0.0000,  0.0000,  0.2799, -0.0000,  0.0000,  0.3625],\n        [-0.1389,  0.0000,  0.0000, -0.0814, -0.2632, -0.0000,  0.0000,  0.0000,  0.2799, -0.0000,  0.2103,  0.0000],\n        [-0.3125,  0.0000,  0.0000, -0.1438, -0.2632, -0.0000,  0.4876,  0.0000,  0.0000, -0.0000,  0.0000,  0.3625],\n        ...,\n        [-0.1447,  0.0000,  0.0000, -0.0756, -0.2632, -0.0000,  0.0000,  0.0000,  0.2799, -0.0000,  0.0000,  0.3625],\n        [-0.2257,  0.0000,  0.2008, -0.1234, -0.0000, -0.3147,  0.0000,  0.0000,  0.2799, -0.0000,  0.2103,  0.0000],\n        [-0.1562,  0.0000,  0.0000, -0.0956, -0.2632, -0.0000,  0.0000,  0.3136,  0.0000, -0.0000,  0.0000,  0.3625],\n        [-0.1099,  0.0000,  0.0000, -0.1244, -0.0000, -0.3147,  0.4876,  0.0000,  0.0000, -0.0000,  0.0000,  0.3625],\n        [-0.1389,  0.0173,  0.0803, -0.1158, -0.0000, -0.3147,  0.0000,  0.0000,  0.2799, -0.0000,  0.0000,  0.3625],\n        [-0.1504,  0.0000,  0.0000, -0.1244, -0.2632, -0.0000,  0.4876,  0.0000,  0.0000, -0.4392,  0.0000,  0.0000],\n        [-0.1852,  0.0000,  0.0000, -0.0786, -0.2632, -0.0000,  0.0000,  0.0000,  0.2799, -0.0000,  0.2103,  0.0000]])\n\n\n\n# Calculate Prediction\npreds = (t_indep * coeffs).sum(axis=1)\nprint(f\"first few predictions: {preds[:10]}\")\n\nfirst few predictions: tensor([ 0.1927, -0.6239,  0.0979,  0.2056,  0.0968,  0.0066,  0.1306,  0.3476,  0.1613, -0.6285])\n\n\n\n# Loss Function -&gt; Mean Absolute Error\n# - Loss function is required for doing gradient descent\nloss = torch.abs(preds - t_dep).mean()\nprint(f\"Loss: {loss}\")\n\nLoss: 0.5382388234138489\n\n\n\n# Functions for computing predictions and loss\n\n# Compute Predictions\ndef calc_preds(coeffs, indeps):\n  return (indeps * coeffs).sum(axis=1)\n\n# Compute Loss\ndef calc_loss(coeffs, indeps, deps):\n  return torch.abs(calc_preds(coeffs, indeps) - deps).mean()\n\n\n\nGradient Descent\n\n# Tell pytorch to calculate gradients\ncoeffs.requires_grad_()\n\ntensor([-0.4629,  0.1386,  0.2409, -0.2262, -0.2632, -0.3147,  0.4876,  0.3136,  0.2799, -0.4392,  0.2103,  0.3625], requires_grad=True)\n\n\n\n# Calculate loss\nloss = calc_loss(coeffs, t_indep, t_dep)\nloss\n\ntensor(0.5382, grad_fn=&lt;MeanBackward0&gt;)\n\n\n\n# Calculate gradients\nloss.backward()\n\n\n# Gradients\ncoeffs.grad\n\ntensor([-0.0106,  0.0129, -0.0041, -0.0484,  0.2099, -0.2132, -0.1212, -0.0247,  0.1425, -0.1886, -0.0191,  0.2043])\n\n\n\n# - each call to backward, gradients are added to the value stored in grad attribute\nloss = calc_loss(coeffs, t_indep, t_dep)\nloss.backward()\ncoeffs.grad\n\ntensor([-0.0212,  0.0258, -0.0082, -0.0969,  0.4198, -0.4265, -0.2424, -0.0494,  0.2851, -0.3771, -0.0382,  0.4085])\n\n\n\n# reset gradients to zero after doing a single gradient step\nloss = calc_loss(coeffs, t_indep, t_dep)\nloss.backward()\nwith torch.no_grad():\n  coeffs.sub_(coeffs.grad * 0.1)\n  coeffs.grad.zero_()\n  print(calc_loss(coeffs, t_indep, t_dep))\n\ntensor(0.4945)\n\n\n\n\nLinear Model Training\n\n# Data split\nfrom fastai.data.transforms import RandomSplitter\ntrn_split,val_split=RandomSplitter(seed=42)(df)\n\nprint(f\"Training Data Size: {len(trn_split)}\")\nprint(f\"Validation Data Size: {len(val_split)}\")\nprint(f\"Training Data Indices: {trn_split}\")\nprint(f\"Validation Data Indices: {val_split}\")\n\nTraining Data Size: 713\nValidation Data Size: 178\nTraining Data Indices: [788, 525, 821, 253, 374, 98, 215, 313, 281, 305, 701, 812, 76, 50, 387, 47, 516, 564, 434, 117, 150, 513, 676, 470, 569, 603, 816, 719, 120, 88, 204, 617, 615, 61, 648, 139, 840, 831, 302, 118, 58, 257, 404, 24, 618, 730, 371, 104, 370, 592, 548, 633, 216, 682, 157, 103, 512, 574, 650, 312, 757, 225, 241, 557, 808, 827, 334, 208, 23, 2, 28, 319, 463, 77, 34, 637, 842, 30, 460, 888, 217, 405, 10, 66, 852, 291, 249, 872, 75, 450, 597, 377, 178, 207, 737, 318, 573, 64, 415, 220, 184, 49, 384, 97, 121, 111, 568, 873, 343, 495, 611, 712, 723, 829, 871, 29, 641, 69, 844, 383, 560, 394, 817, 643, 820, 832, 409, 645, 441, 732, 636, 848, 475, 317, 884, 881, 367, 562, 689, 841, 805, 716, 81, 54, 44, 136, 364, 35, 796, 373, 342, 550, 543, 851, 185, 85, 451, 826, 761, 399, 16, 125, 264, 162, 197, 309, 804, 42, 545, 846, 622, 177, 273, 559, 815, 74, 248, 809, 403, 728, 613, 96, 549, 258, 192, 780, 263, 109, 803, 20, 715, 487, 375, 570, 108, 628, 153, 474, 572, 306, 341, 763, 877, 227, 454, 535, 767, 33, 193, 793, 62, 311, 285, 861, 687, 738, 498, 235, 507, 5, 743, 499, 160, 686, 445, 368, 660, 748, 813, 614, 431, 534, 152, 810, 314, 423, 166, 567, 447, 296, 621, 688, 147, 176, 662, 481, 407, 22, 878, 702, 542, 219, 604, 448, 866, 508, 267, 786, 190, 339, 222, 473, 365, 363, 469, 746, 554, 349, 428, 60, 421, 25, 331, 429, 39, 752, 419, 553, 879, 558, 802, 360, 372, 19, 156, 483, 251, 801, 882, 547, 744, 704, 602, 164, 608, 78, 750, 201, 722, 142, 171, 762, 3, 828, 482, 472, 458, 130, 576, 231, 468, 843, 43, 647, 11, 132, 760, 785, 206, 556, 72, 886, 413, 293, 284, 348, 751, 759, 488, 68, 529, 503, 389, 381, 883, 432, 830, 238, 345, 269, 555, 83, 672, 14, 818, 167, 651, 819, 718, 502, 316, 836, 410, 99, 625, 278, 352, 640, 295, 765, 741, 112, 847, 539, 165, 616, 772, 596, 320, 546, 181, 709, 395, 401, 230, 347, 510, 626, 223, 127, 205, 756, 355, 52, 773, 697, 565, 610, 337, 158, 329, 795, 15, 749, 792, 486, 789, 430, 416, 544, 149, 73, 210, 855, 254, 776, 522, 739, 114, 661, 777, 380, 600, 107, 36, 856, 396, 838, 354, 161, 351, 708, 590, 734, 393, 632, 406, 703, 138, 426, 845, 674, 668, 500, 247, 680, 659, 653, 620, 262, 41, 781, 652, 307, 350, 382, 577, 40, 209, 357, 17, 717, 527, 666, 196, 174, 116, 124, 530, 82, 133, 607, 679, 850, 272, 771, 849, 137, 57, 457, 195, 268, 601, 497, 145, 745, 95, 839, 453, 561, 287, 684, 858, 571, 675, 433, 876, 422, 654, 186, 627, 673, 511, 493, 214, 520, 308, 338, 720, 323, 727, 496, 101, 234, 692, 455, 13, 663, 237, 665, 203, 724, 501, 449, 521, 261, 860, 514, 609, 259, 726, 169, 731, 541, 80, 335, 420, 86, 612, 794, 536, 244, 332, 669, 328, 664, 655, 524, 667, 677, 461, 834, 271, 92, 199, 243, 631, 155, 696, 46, 392, 325, 711, 379, 523, 494, 578, 9, 200, 485, 93, 437, 800, 221, 87, 154, 27, 255, 290, 833, 53, 327, 678, 425, 48, 583, 198, 0, 835, 194, 491, 94, 6, 353, 594, 79, 100, 863, 90, 310, 336, 859, 402, 775, 444, 159, 634, 791, 623, 466, 528, 649, 340, 624, 887, 294, 240, 478, 823, 588, 714, 889, 671, 862, 356, 551, 304, 135, 378, 346, 552, 629, 733, 517, 212, 7, 321, 265, 398, 442, 400, 84, 228, 436, 725, 783, 681, 411, 700, 67, 584, 635, 91, 277, 875, 388, 239, 582, 774, 440, 141, 694, 4, 766, 857, 424, 105, 683, 279, 867, 55, 51, 479, 747, 657, 242, 110, 326, 260, 59, 297, 180, 408, 63, 32, 822, 106, 270, 427, 698, 806, 755, 799, 824, 693, 465, 256, 418, 446, 695, 276, 26, 885, 599, 707, 585, 646, 595, 758, 189, 8, 170, 144, 280, 146, 391, 807, 179, 173, 462, 358, 532, 344]\nValidation Data Indices: [303, 778, 531, 385, 134, 476, 691, 443, 386, 128, 579, 65, 869, 359, 202, 187, 456, 880, 705, 797, 656, 467, 581, 754, 131, 768, 172, 252, 163, 300, 417, 71, 566, 814, 322, 119, 515, 369, 89, 729, 226, 563, 148, 218, 12, 638, 509, 333, 825, 376, 245, 480, 182, 784, 619, 70, 126, 605, 37, 735, 452, 721, 630, 115, 854, 102, 864, 811, 188, 706, 870, 140, 439, 868, 168, 764, 589, 224, 191, 586, 183, 742, 753, 250, 526, 685, 505, 435, 874, 658, 769, 45, 537, 798, 644, 292, 397, 330, 289, 143, 175, 274, 519, 606, 690, 288, 286, 670, 484, 477, 506, 18, 740, 412, 464, 366, 736, 790, 699, 299, 266, 587, 459, 233, 598, 782, 246, 315, 282, 236, 504, 639, 518, 123, 31, 283, 232, 713, 122, 211, 213, 591, 129, 890, 229, 275, 471, 533, 853, 362, 38, 593, 56, 787, 301, 324, 837, 113, 21, 538, 710, 361, 490, 390, 1, 779, 580, 865, 438, 492, 642, 575, 151, 770, 414, 540, 298, 489]\n\n\n\n# Training Data, Validation Data\ntrn_indep, val_indep = t_indep[trn_split], t_indep[val_split]\ntrn_dep, val_dep = t_dep[trn_split], t_dep[val_split]\n\nprint(f\"Training Independent Data Size: {len(trn_indep)}\")\nprint(f\"Training Dependent Data Size: {len(trn_dep)}\")\nprint(f\"Validation Indepdent Data Size: {len(val_indep)}\")\nprint(f\"Validation Dependent Data Size: {len(val_dep)}\")\n\nTraining Independent Data Size: 713\nTraining Dependent Data Size: 713\nValidation Indepdent Data Size: 178\nValidation Dependent Data Size: 178\n\n\n\n# Randomly initialize coefficients\ndef init_coeffs():\n  return (torch.rand(n_coeff) - 0.5).requires_grad_()\n\n# Update coefficents\ndef update_coeffs(coeffs, lr):\n    coeffs.sub_(coeffs.grad * lr)\n    coeffs.grad.zero_()\n\n# One full gradient descent step\ndef one_epoch(coeffs, lr):\n    loss = calc_loss(coeffs, trn_indep, trn_dep)\n    loss.backward()\n    with torch.no_grad():\n        update_coeffs(coeffs, lr)\n    print(f\"{loss:.3f}\", end=\"; \")\n\n# Train model\ndef train_model(epochs=30, lr=0.01):\n    torch.manual_seed(442)\n    coeffs = init_coeffs()\n    for i in range(epochs):\n      one_epoch(coeffs, lr=lr)\n    return coeffs\n\n# Calculate average accuracy of model\ndef acc(coeffs):\n  return (val_dep.bool() == (calc_preds(coeffs, val_indep) &gt; 0.5)).float().mean()\n\n# Show coefficients for each column\ndef show_coeffs():\n  return dict(zip(indep_cols, coeffs.requires_grad_(False)))\n\n\n# Train Model\ncoeffs = train_model(18, lr=0.2)\n\n0.536; 0.502; 0.477; 0.454; 0.431; 0.409; 0.388; 0.367; 0.349; 0.336; 0.330; 0.326; 0.329; 0.304; 0.314; 0.296; 0.300; 0.289; \n\n\n\n# Coefficients for every column\nshow_coeffs()\n\n{'Age': tensor(-0.2694),\n 'SibSp': tensor(0.0901),\n 'Parch': tensor(0.2359),\n 'LogFare': tensor(0.0280),\n 'Sex_male': tensor(-0.3990),\n 'Sex_female': tensor(0.2345),\n 'Pclass_1': tensor(0.7232),\n 'Pclass_2': tensor(0.4112),\n 'Pclass_3': tensor(0.3601),\n 'Embarked_C': tensor(0.0955),\n 'Embarked_Q': tensor(0.2395),\n 'Embarked_S': tensor(0.2122)}\n\n\n\n# Calculate accuracy\npreds = calc_preds(coeffs, val_indep)\n\n# - assume that any passenger with score &gt; 0.5 is predicted to survive\n# - correct for  each row where preds &gt; 0.5 is the same as dependent variable\nresults = val_dep.bool()==(preds &gt; 0.5)\nprint(f\"First 16 results: {results[:16]}\")\n\n# Average accuracy\navg_acc = results.float().mean()\nprint(f\"Average Accuracy: {avg_acc}\")\n\nFirst 16 results: tensor([ True,  True,  True,  True,  True,  True,  True,  True,  True,  True, False, False, False,  True,  True, False])\nAverage Accuracy: 0.7865168452262878\n\n\n\n\nSigmoid\n\n# - some of the predictions of the survival probability are &gt; 1 or &lt; 0\n# - can fix this issue by passing prediction through sigmoid function\npreds[:28]\n\ntensor([ 0.8160,  0.1295, -0.0148,  0.1831,  0.1520,  0.1350,  0.7279,  0.7754,  0.3222,  0.6740,  0.0753,  0.0389,  0.2216,  0.7631,\n         0.0678,  0.3997,  0.3324,  0.8278,  0.1078,  0.7126,  0.1023,  0.3627,  0.9937,  0.8050,  0.1153,  0.1455,  0.8652,  0.3425])\n\n\n\n# Sigmoid function has a minimum of 0 and max at 1\nimport sympy\nsympy.plot(\"1/(1+exp(-x))\", xlim=(-5,5));\n\n\n\n\n\n# Update calc predictions to use sigmoid\ndef calc_preds(coeffs, indeps):\n  return torch.sigmoid((indeps*coeffs).sum(axis=1))\n\n\n# Train Model with Sigmoid Predictions\ncoeffs = train_model(lr=100)\n\n0.510; 0.327; 0.294; 0.207; 0.201; 0.199; 0.198; 0.197; 0.196; 0.196; 0.196; 0.195; 0.195; 0.195; 0.195; 0.195; 0.195; 0.195; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; \n\n\n\n# check coeffcients\nshow_coeffs()\n\n{'Age': tensor(-1.5061),\n 'SibSp': tensor(-1.1575),\n 'Parch': tensor(-0.4267),\n 'LogFare': tensor(0.2543),\n 'Sex_male': tensor(-10.3320),\n 'Sex_female': tensor(8.4185),\n 'Pclass_1': tensor(3.8389),\n 'Pclass_2': tensor(2.1398),\n 'Pclass_3': tensor(-6.2331),\n 'Embarked_C': tensor(1.4771),\n 'Embarked_Q': tensor(2.1168),\n 'Embarked_S': tensor(-4.7958)}\n\n\n\n# Check Accuracy\nacc(coeffs)\n\ntensor(0.8258)\n\n\n\n\nFinal Linear Model Setup\n\n# number of coefficients\nn_coeff = t_indep.shape[1]\n\n# Randomly initialize coefficients\ndef init_coeffs():\n  return (torch.rand(n_coeff)-0.5).requires_grad_()\n\n# Loss Function - MAE (Mean Absolute Error)\ndef calc_loss(coeffs, indeps, deps):\n  return torch.abs(calc_preds(coeffs, indeps) - deps).mean()\n\n# Update coefficents\ndef update_coeffs(coeffs, lr):\n    coeffs.sub_(coeffs.grad * lr)\n    coeffs.grad.zero_()\n\n# One full gradient descent step\ndef one_epoch(coeffs, lr):\n    loss = calc_loss(coeffs, trn_indep, trn_dep)\n    loss.backward()\n    with torch.no_grad():\n      update_coeffs(coeffs, lr)\n    print(f\"{loss:.3f}\", end=\"; \")\n\n# Train model\ndef train_model(epochs=30, lr=0.01):\n    torch.manual_seed(442)\n    coeffs = init_coeffs()\n    for i in range(epochs):\n      one_epoch(coeffs, lr=lr)\n    return coeffs\n\n# Calculate average accuracy of model\ndef acc(coeffs):\n  return (val_dep.bool() == (calc_preds(coeffs, val_indep) &gt; 0.5)).float().mean()\nacc(coeffs)\n\n# Calculate predictions\ndef calc_preds(coeffs, indeps):\n  return torch.sigmoid((indeps * coeffs).sum(axis=1))\n\n# Show coefficients for each column\ndef show_coeffs():\n  return dict(zip(indep_cols, coeffs.requires_grad_(False)))\n\n\ncoeffs = train_model(lr=100)\n\n0.510; 0.327; 0.294; 0.207; 0.201; 0.199; 0.198; 0.197; 0.196; 0.196; 0.196; 0.195; 0.195; 0.195; 0.195; 0.195; 0.195; 0.195; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; \n\n\n\n\nTest model on inference data\n\nInference Data Processing\n\n# load data\ntst_df = pd.read_csv(path/'test.csv')\n\n# Fare data is missing one passenger -&gt; substitute 0 to fix the issue\ntst_df['Fare'] = tst_df.Fare.fillna(0)\ntst_df\n\n\n  \n    \n\n\n\n\n\n\nPassengerId\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n892\n3\nKelly, Mr. James\nmale\n34.5\n0\n0\n330911\n7.8292\nNaN\nQ\n\n\n1\n893\n3\nWilkes, Mrs. James (Ellen Needs)\nfemale\n47.0\n1\n0\n363272\n7.0000\nNaN\nS\n\n\n2\n894\n2\nMyles, Mr. Thomas Francis\nmale\n62.0\n0\n0\n240276\n9.6875\nNaN\nQ\n\n\n3\n895\n3\nWirz, Mr. Albert\nmale\n27.0\n0\n0\n315154\n8.6625\nNaN\nS\n\n\n4\n896\n3\nHirvonen, Mrs. Alexander (Helga E Lindqvist)\nfemale\n22.0\n1\n1\n3101298\n12.2875\nNaN\nS\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n413\n1305\n3\nSpector, Mr. Woolf\nmale\nNaN\n0\n0\nA.5. 3236\n8.0500\nNaN\nS\n\n\n414\n1306\n1\nOliva y Ocana, Dona. Fermina\nfemale\n39.0\n0\n0\nPC 17758\n108.9000\nC105\nC\n\n\n415\n1307\n3\nSaether, Mr. Simon Sivertsen\nmale\n38.5\n0\n0\nSOTON/O.Q. 3101262\n7.2500\nNaN\nS\n\n\n416\n1308\n3\nWare, Mr. Frederick\nmale\nNaN\n0\n0\n359309\n8.0500\nNaN\nS\n\n\n417\n1309\n3\nPeter, Master. Michael J\nmale\nNaN\n1\n1\n2668\n22.3583\nNaN\nC\n\n\n\n\n\n418 rows × 11 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\n\n# - these steps follow the same process as the training data processing\ntst_df.fillna(modes, inplace=True)\ntst_df['LogFare'] = np.log(tst_df['Fare'] + 1)\ntst_df = pd.get_dummies(tst_df, columns=[\"Sex\",\"Pclass\",\"Embarked\"])\n\nadded_cols = ['Sex_male', 'Sex_female', 'Pclass_1', 'Pclass_2', 'Pclass_3', 'Embarked_C', 'Embarked_Q', 'Embarked_S']\nindep_cols = ['Age', 'SibSp', 'Parch', 'LogFare'] + added_cols\n\ntst_indep = tensor(tst_df[indep_cols].values, dtype=torch.float)\ntst_indep = tst_indep / vals\n\n\n# Calculate predictions of which passengers survived in titanic dataset\ntst_df['Survived'] = (calc_preds(tst_indep, coeffs) &gt; 0.5).int()\n\n\n\n\nLinear Model: Submit Results to Kaggle\n\n# Submit to Kaggle\nsub_df = tst_df[['PassengerId','Survived']]\nsub_df.to_csv('sub.csv', index=False)\n\n# check first few rows\n!head sub.csv\n\nPassengerId,Survived\n892,0\n893,0\n894,0\n895,0\n896,0\n897,0\n898,1\n899,0\n900,1"
  },
  {
    "objectID": "posts/fastai-lesson5/fastai-lesson5.html#cleaning-up-linear-model-code",
    "href": "posts/fastai-lesson5/fastai-lesson5.html#cleaning-up-linear-model-code",
    "title": "FastAI Lesson 5: From-scratch model",
    "section": "Cleaning up Linear Model Code",
    "text": "Cleaning up Linear Model Code\n\n# - Multiplying elements together and then adding across rows is the same as matrix-vector multiply\n\n\n# Original Matrix-Vector multiply\n(val_indep*coeffs).sum(axis=1)\n\ntensor([ 12.3288, -14.8119, -15.4540, -13.1513, -13.3512, -13.6469,   3.6248,   5.3429, -22.0878,   3.1233, -21.8742, -15.6421, -21.5504,\n          3.9393, -21.9190, -12.0010, -12.3775,   5.3550, -13.5880,  -3.1015, -21.7237, -12.2081,  12.9767,   4.7427, -21.6525, -14.9135,\n         -2.7433, -12.3210, -21.5886,   3.9387,   5.3890,  -3.6196, -21.6296, -21.8454,  12.2159,  -3.2275, -12.0289,  13.4560, -21.7230,\n         -3.1366, -13.2462, -21.7230, -13.6831,  13.3092, -21.6477,  -3.5868, -21.6854, -21.8316, -14.8158,  -2.9386,  -5.3103, -22.2384,\n        -22.1097, -21.7466, -13.3780, -13.4909, -14.8119, -22.0690, -21.6666, -21.7818,  -5.4439, -21.7407, -12.6551, -21.6671,   4.9238,\n        -11.5777, -13.3323, -21.9638, -15.3030,   5.0243, -21.7614,   3.1820, -13.4721, -21.7170, -11.6066, -21.5737, -21.7230, -11.9652,\n        -13.2382, -13.7599, -13.2170,  13.1347, -21.7049, -21.7268,   4.9207,  -7.3198,  -5.3081,   7.1065,  11.4948, -13.3135, -21.8723,\n        -21.7230,  13.3603, -15.5670,   3.4105,  -7.2857, -13.7197,   3.6909,   3.9763, -14.7227, -21.8268,   3.9387, -21.8743, -21.8367,\n        -11.8518, -13.6712, -21.8299,   4.9440,  -5.4471, -21.9666,   5.1333,  -3.2187, -11.6008,  13.7920, -21.7230,  12.6369,  -3.7268,\n        -14.8119, -22.0637,  12.9468, -22.1610,  -6.1827, -14.8119,  -3.2838, -15.4540, -11.6950,  -2.9926,  -3.0110, -21.5664, -13.8268,\n          7.3426, -21.8418,   5.0744,   5.2582,  13.3415, -21.6289, -13.9898, -21.8112,  -7.3316,   5.2296, -13.4453,  12.7891, -22.1235,\n        -14.9625,  -3.4339,   6.3089, -21.9839,   3.1968,   7.2400,   2.8558,  -3.1187,   3.7965,   5.4667, -15.1101, -15.0597, -22.9391,\n        -21.7230,  -3.0346, -13.5206, -21.7011,  13.4425,  -7.2690, -21.8335, -12.0582,  13.0489,   6.7993,   5.2160,   5.0794, -12.6957,\n        -12.1838,  -3.0873, -21.6070,   7.0744, -21.7170, -22.1001,   6.8159, -11.6002, -21.6310], grad_fn=&lt;SumBackward1&gt;)\n\n\n\n# Pytorch optimized matrix-vector multiply\n# - python uses @ operator to indicate matrix products and is supported by pytorch tensors\nval_indep@coeffs\n\ntensor([ 12.3288, -14.8119, -15.4540, -13.1513, -13.3511, -13.6468,   3.6248,   5.3429, -22.0878,   3.1233, -21.8742, -15.6421, -21.5504,\n          3.9393, -21.9190, -12.0010, -12.3775,   5.3550, -13.5880,  -3.1015, -21.7237, -12.2081,  12.9767,   4.7427, -21.6525, -14.9135,\n         -2.7433, -12.3210, -21.5886,   3.9387,   5.3890,  -3.6196, -21.6296, -21.8454,  12.2159,  -3.2275, -12.0289,  13.4560, -21.7230,\n         -3.1366, -13.2462, -21.7230, -13.6831,  13.3092, -21.6477,  -3.5868, -21.6854, -21.8316, -14.8158,  -2.9386,  -5.3103, -22.2384,\n        -22.1097, -21.7466, -13.3780, -13.4909, -14.8119, -22.0690, -21.6666, -21.7818,  -5.4439, -21.7407, -12.6551, -21.6671,   4.9238,\n        -11.5777, -13.3323, -21.9638, -15.3030,   5.0243, -21.7614,   3.1820, -13.4721, -21.7170, -11.6066, -21.5737, -21.7230, -11.9652,\n        -13.2382, -13.7599, -13.2170,  13.1347, -21.7049, -21.7268,   4.9207,  -7.3198,  -5.3081,   7.1065,  11.4948, -13.3135, -21.8723,\n        -21.7230,  13.3603, -15.5670,   3.4105,  -7.2857, -13.7197,   3.6909,   3.9763, -14.7227, -21.8268,   3.9387, -21.8743, -21.8367,\n        -11.8518, -13.6712, -21.8299,   4.9440,  -5.4471, -21.9666,   5.1333,  -3.2187, -11.6008,  13.7920, -21.7230,  12.6369,  -3.7268,\n        -14.8119, -22.0637,  12.9468, -22.1610,  -6.1827, -14.8119,  -3.2838, -15.4540, -11.6950,  -2.9926,  -3.0110, -21.5664, -13.8268,\n          7.3426, -21.8418,   5.0744,   5.2582,  13.3415, -21.6289, -13.9898, -21.8112,  -7.3316,   5.2296, -13.4453,  12.7891, -22.1235,\n        -14.9625,  -3.4339,   6.3089, -21.9839,   3.1968,   7.2400,   2.8558,  -3.1187,   3.7965,   5.4667, -15.1101, -15.0597, -22.9391,\n        -21.7230,  -3.0346, -13.5206, -21.7011,  13.4425,  -7.2690, -21.8335, -12.0582,  13.0489,   6.7993,   5.2160,   5.0794, -12.6957,\n        -12.1838,  -3.0873, -21.6070,   7.0744, -21.7170, -22.1001,   6.8159, -11.6002, -21.6310], grad_fn=&lt;MvBackward0&gt;)\n\n\n\nLinear Model: PyTorch Matrix-Vector Multiply\n\n# number of coefficients\nn_coeff = t_indep.shape[1]\n\n# Randomly initialize coefficients\ndef init_coeffs():\n  # - 1 turns torch.rand() into a column vector\n  return (torch.rand(n_coeff, 1) * 0.1).requires_grad_()\n\n# Loss Function - MAE (Mean Absolute Error)\ndef calc_loss(coeffs, indeps, deps):\n  return torch.abs(calc_preds(coeffs, indeps) - deps).mean()\n\n# Update coefficents\ndef update_coeffs(coeffs, lr):\n    coeffs.sub_(coeffs.grad * lr)\n    coeffs.grad.zero_()\n\n# One full gradient descent step\ndef one_epoch(coeffs, lr):\n    loss = calc_loss(coeffs, trn_indep, trn_dep)\n    loss.backward()\n    with torch.no_grad():\n      update_coeffs(coeffs, lr)\n    print(f\"{loss:.3f}\", end=\"; \")\n\n# Train model\ndef train_model(epochs=30, lr=0.01):\n    torch.manual_seed(442)\n    coeffs = init_coeffs()\n    for i in range(epochs):\n      one_epoch(coeffs, lr=lr)\n    return coeffs\n\n# Calculate average accuracy of model\ndef acc(coeffs):\n  return (val_dep.bool() == (calc_preds(coeffs, val_indep) &gt; 0.5)).float().mean()\nacc(coeffs)\n\n# Calculate predictions\ndef calc_preds(coeffs, indeps):\n  return torch.sigmoid(indeps@coeffs)\n\n# Show coefficients for each column\ndef show_coeffs():\n  return dict(zip(indep_cols, coeffs.requires_grad_(False)))\n\n\n# change dependent variable into a column vector - rank 2 tensor\ntrn_dep = trn_dep[:,None]\nval_dep = val_dep[:,None]\nprint(f\"Training Data Shape: {trn_dep.shape}\")\nprint(f\"Training Data Rank: {len(trn_dep.shape)}\")\nprint(f\"Validation Data Shape: {val_dep.shape}\")\nprint(f\"Validation Data Rank: {len(val_dep.shape)}\")\n\nTraining Data Shape: torch.Size([713, 1])\nTraining Data Rank: 2\nValidation Data Shape: torch.Size([178, 1])\nValidation Data Rank: 2\n\n\n\ncoeffs = train_model(lr=100)\nprint()\nprint(f\"coefficients shape: {coeffs.shape}\")\nprint(f\"coefficients rank: {len(coeffs.shape)}\")\nprint(f\"accuracy: {acc(coeffs)}\")\n\n0.512; 0.323; 0.290; 0.205; 0.200; 0.198; 0.197; 0.197; 0.196; 0.196; 0.196; 0.195; 0.195; 0.195; 0.195; 0.195; 0.195; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; \ncoefficients shape: torch.Size([12, 1])\ncoefficients rank: 2\naccuracy: 0.8258426785469055"
  },
  {
    "objectID": "posts/fastai-lesson5/fastai-lesson5.html#neural-network",
    "href": "posts/fastai-lesson5/fastai-lesson5.html#neural-network",
    "title": "FastAI Lesson 5: From-scratch model",
    "section": "Neural Network",
    "text": "Neural Network\n\nDefine coefficients for each layer of the neural network\n\nn hidden - higher number gives more flexibility for neural network to approximate data but slower and harder to train\nFirst Layer input - n_coeff values output - n_hidden values (input to second layer) - need matrix of size n_coeffs by n_hidden - divide coefficients by n_hidden so that when we sum them up in the next layer so that we end up with similar magnitude numbers to what we started with\nSecond Layer input - n_hidden values (output of first layer) output - 1 value - need n_hidden by 1 + constant term\nSteps 1. Two matrix products - indeps@l1 and res@l2 (res is output of first layer) 2. First layer output is passed to F.relu (non-linearity) 3. Second layer output is passed to sigmoid\n\ndef init_coeffs(n_hidden=20):\n  # set of coefficients to go from input to hidden\n    layer1 = (torch.rand(n_coeff, n_hidden) - 0.5) / n_hidden\n\n    # set of coefficients to from hiddent to an output\n    layer2 = torch.rand(n_hidden, 1) - 0.3\n    const = torch.rand(1)[0]\n\n    # return a tuple of layer1 gradient, layer2 gradient, and constant gradient\n    return layer1.requires_grad_(), layer2.requires_grad_(), const.requires_grad_()\n\n\nimport torch.nn.functional as F\n\n# neural network\ndef calc_preds(coeffs, indeps):\n    l1, l2, const = coeffs\n\n    # layer 1\n    # replace negative values with zeroes\n    res = F.relu(indeps@l1)\n\n    # layer 2\n    res = res@l2 + const\n    return torch.sigmoid(res)\n\n\n# Update Coefficients\n# - Three sets of coefficients to update per epoch(layer1, layer2, constant)\ndef update_coeffs(coeffs, lr):\n  for layer in coeffs:\n    layer.sub_(layer.grad * lr)\n    layer.grad.zero_()\n\n\n# Train Model\ncoeffs = train_model(lr=1.4)\n\n0.543; 0.532; 0.520; 0.505; 0.487; 0.466; 0.439; 0.407; 0.373; 0.343; 0.319; 0.301; 0.286; 0.274; 0.264; 0.256; 0.250; 0.245; 0.240; 0.237; 0.234; 0.231; 0.229; 0.227; 0.226; 0.224; 0.223; 0.222; 0.221; 0.220; \n\n\n\n# Train Model\ncoeffs = train_model(lr=20)\n\n0.543; 0.400; 0.260; 0.390; 0.221; 0.211; 0.197; 0.195; 0.193; 0.193; 0.193; 0.193; 0.193; 0.193; 0.193; 0.193; 0.193; 0.192; 0.192; 0.192; 0.192; 0.192; 0.192; 0.192; 0.192; 0.192; 0.192; 0.192; 0.192; 0.192; \n\n\n\n# Check Accuracy\nacc(coeffs)\n\ntensor(0.8258)"
  },
  {
    "objectID": "posts/fastai-lesson5/fastai-lesson5.html#deep-learning",
    "href": "posts/fastai-lesson5/fastai-lesson5.html#deep-learning",
    "title": "FastAI Lesson 5: From-scratch model",
    "section": "Deep Learning",
    "text": "Deep Learning\n\ndef init_coeffs():\n  # size of each hidden layer\n  # two hidden layers - 10 activations in each layer\n    hiddens = [10, 10]\n  # - n_coeffs to 10\n  # - 10 to 10\n  # - 10 to 1\n    sizes = [n_coeff] + hiddens + [1]\n    n = len(sizes)\n    layers = [(torch.rand(sizes[i], sizes[i + 1])- 0.3)/sizes[i + 1]*4 for i in range(n - 1)]\n    consts = [(torch.rand(1)[0] - 0.5) * 0.1 for i in range(n - 1)]\n    for l in layers + consts:\n      l.requires_grad_()\n    return layers,consts\n\n\nimport torch.nn.functional as F\n\ndef calc_preds(coeffs, indeps):\n    layers,consts = coeffs\n    n = len(layers)\n    res = indeps\n    for i,l in enumerate(layers):\n        res = res@l + consts[i]\n        # RELU for every layer except for last layer\n        if i != n - 1:\n          res = F.relu(res)\n        # sigmoid only for the last layer\n    return torch.sigmoid(res)\n\n\ndef update_coeffs(coeffs, lr):\n    layers,consts = coeffs\n    for layer in layers + consts:\n        layer.sub_(layer.grad * lr)\n        layer.grad.zero_()\n\n\n# Train Model\ncoeffs = train_model(lr=4)\n\n0.521; 0.483; 0.427; 0.379; 0.379; 0.379; 0.379; 0.378; 0.378; 0.378; 0.378; 0.378; 0.378; 0.378; 0.378; 0.378; 0.377; 0.376; 0.371; 0.333; 0.239; 0.224; 0.208; 0.204; 0.203; 0.203; 0.207; 0.197; 0.196; 0.195; \n\n\n\n# Check Accuracy\nacc(coeffs)\n\ntensor(0.8258)"
  },
  {
    "objectID": "posts/fastai-lesson5/fastai-lesson5.html#framework-fastai-pytorch",
    "href": "posts/fastai-lesson5/fastai-lesson5.html#framework-fastai-pytorch",
    "title": "FastAI Lesson 5: From-scratch model",
    "section": "Framework: fastai + PyTorch",
    "text": "Framework: fastai + PyTorch\n\n# load stuff\nfrom fastai.tabular.all import *\n\npd.options.display.float_format = '{:.2f}'.format\nset_seed(42)\n\n\n# Load Data\ndf = pd.read_csv(path/'train.csv')\n\n# Feature Engineering\ndef add_features(df):\n    df['LogFare'] = np.log1p(df['Fare'])\n    df['Deck'] = df.Cabin.str[0].map(dict(A=\"ABC\", B=\"ABC\", C=\"ABC\", D=\"DE\", E=\"DE\", F=\"FG\", G=\"FG\"))\n    df['Family'] = df.SibSp+df.Parch\n    df['Alone'] = df.Family==0\n    df['TicketFreq'] = df.groupby('Ticket')['Ticket'].transform('count')\n    df['Title'] = df.Name.str.split(', ', expand=True)[1].str.split('.', expand=True)[0]\n    df['Title'] = df.Title.map(dict(Mr=\"Mr\",Miss=\"Miss\",Mrs=\"Mrs\",Master=\"Master\"))\n\nadd_features(df)\n\n\n# Data Split\nsplits = RandomSplitter(seed=42)(df)\n\n\n# Tabular Dataloaders\ndls = TabularPandas(\n    # splits for indices of training and validation sets\n    df, splits=splits,\n    # Turn strings into categories, fill missing values in numeric columns with the median, normalise all numeric columns\n    procs = [Categorify, FillMissing, Normalize],\n    # categorical independent variables\n    cat_names=[\"Sex\",\"Pclass\",\"Embarked\",\"Deck\", \"Title\"],\n    # continuous independent variables\n    cont_names=['Age', 'SibSp', 'Parch', 'LogFare', 'Alone', 'TicketFreq', 'Family'],\n    # dependent variable\n    y_names=\"Survived\",\n    # dependent variable is categorical(build a classification model)\n    y_block = CategoryBlock(),\n).dataloaders(path=\".\")\n\n\n\n# Train Model\n# - data + model = Learner\n# - dls -&gt; data\n# - layers -&gt; size of each hidden layer\n# - metrics -&gt; any metric we want to use for loss function\nlearn = tabular_learner(dls, metrics=accuracy, layers=[10,10])\n\n\n# find learning rate\nlearn.lr_find(suggest_funcs=(slide, valley))\n\n\n\n\n\n\n\n\nSuggestedLRs(slide=0.05754399299621582, valley=0.013182567432522774)\n\n\n\n\n\n\n# specify number of epochs and learning rate and train model\nlearn.fit(16, 0.03)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.577146\n0.582949\n0.606742\n00:00\n\n\n1\n0.510818\n0.498523\n0.786517\n00:00\n\n\n2\n0.467023\n0.459841\n0.797753\n00:00\n\n\n3\n0.439957\n0.468547\n0.797753\n00:00\n\n\n4\n0.427232\n0.415261\n0.825843\n00:00\n\n\n5\n0.416340\n0.437362\n0.820225\n00:00\n\n\n6\n0.408347\n0.413253\n0.848315\n00:00\n\n\n7\n0.400442\n0.406075\n0.803371\n00:00\n\n\n8\n0.397265\n0.443730\n0.820225\n00:00\n\n\n9\n0.392389\n0.432267\n0.831461\n00:00\n\n\n10\n0.389977\n0.416234\n0.837079\n00:00\n\n\n11\n0.386176\n0.424609\n0.814607\n00:00\n\n\n12\n0.382634\n0.440516\n0.837079\n00:00\n\n\n13\n0.378518\n0.432545\n0.825843\n00:00\n\n\n14\n0.374405\n0.434594\n0.825843\n00:00\n\n\n15\n0.372745\n0.426979\n0.837079\n00:00\n\n\n\n\n\n\nTest fastai model on inference data\n\n# Inference Data Processing\ntst_df = pd.read_csv(path/'test.csv')\ntst_df['Fare'] = tst_df.Fare.fillna(0)\nadd_features(tst_df)\n\n\n# apply data modeling information from learner to inference\ntst_dl = learn.dls.test_dl(tst_df)\n\n\n# get predictions for the inference data\npreds,_ = learn.get_preds(dl=tst_dl)\n\n\n\n\n\n\n\n\n\n\nfastai Model: Submit to Kaggle\n\n# submit to kaggle\ntst_df['Survived'] = (preds[:,1] &gt; 0.5).int()\nsub_df = tst_df[['PassengerId','Survived']]\nsub_df.to_csv('framework_sub.csv', index=False)\n\n# check predictions file\n!head framework_sub.csv\n\nPassengerId,Survived\n892,0\n893,0\n894,0\n895,0\n896,1\n897,0\n898,1\n899,0\n900,1"
  },
  {
    "objectID": "posts/fastai-lesson5/fastai-lesson5.html#ensembling",
    "href": "posts/fastai-lesson5/fastai-lesson5.html#ensembling",
    "title": "FastAI Lesson 5: From-scratch model",
    "section": "Ensembling",
    "text": "Ensembling\n\ncreate multiple models and combine predictions\n\n\ndef ensemble():\n    learn = tabular_learner(dls, metrics=accuracy, layers=[10,10])\n    with learn.no_bar(),learn.no_logging():\n      learn.fit(16, lr=0.03)\n    return learn.get_preds(dl=tst_dl)[0]\n\n\n# create a set of 5 different predictions\nlearns = [ensemble() for _ in range(5)]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# take average of all predictions\nens_preds = torch.stack(learns).mean(0)\n\n\nEnsembling: Submit to Kaggle\n\n# submit to kaggle\ntst_df['Survived'] = (preds[:,1] &gt; 0.5).int()\nsub_df = tst_df[['PassengerId','Survived']]\nsub_df.to_csv('ensemble_sub.csv', index=False)\n\n# check predictions file\n!head ensemble_sub.csv\n\nPassengerId,Survived\n892,0\n893,0\n894,0\n895,0\n896,1\n897,0\n898,1\n899,0\n900,1"
  },
  {
    "objectID": "posts/fastai-lesson5/fastai-lesson5.html#resources",
    "href": "posts/fastai-lesson5/fastai-lesson5.html#resources",
    "title": "FastAI Lesson 5: From-scratch model",
    "section": "Resources",
    "text": "Resources\n\nFastAI Lesson 3\nFastAI Lesson 5\nFastAI Chapter 4\nFastAI Chapter 9\nTitanic - Machine Learning from Disaster\nHow does a neural net really work?\nLinear model and neural net from scratch\nWhy you should use a framework\nTitanic - Advanced Feature Engineering Tutorial\nJeremy Howard FastAI Live Coding\nfast.ai docs"
  },
  {
    "objectID": "posts/fastai-lesson2/fastai-lesson2.html",
    "href": "posts/fastai-lesson2/fastai-lesson2.html",
    "title": "FastAI Lesson 2: Production",
    "section": "",
    "text": "Finally back! Had some deadlines followed by issues with gradio, kaggle, Firefox gtk, quarto and jupyter but now everything is working for me to publish this post. I’d like to give a shoutout to Kevin Liu for his help in getting up to speed with the updated gradio API. Kevin reviewed my gradio example and took some helpful notes about the errors he ran into which helped me debug the differences between the current version of gradio and that used in Tanishq’s example."
  },
  {
    "objectID": "posts/fastai-lesson2/fastai-lesson2.html#announcments",
    "href": "posts/fastai-lesson2/fastai-lesson2.html#announcments",
    "title": "FastAI Lesson 2: Production",
    "section": "",
    "text": "Finally back! Had some deadlines followed by issues with gradio, kaggle, Firefox gtk, quarto and jupyter but now everything is working for me to publish this post. I’d like to give a shoutout to Kevin Liu for his help in getting up to speed with the updated gradio API. Kevin reviewed my gradio example and took some helpful notes about the errors he ran into which helped me debug the differences between the current version of gradio and that used in Tanishq’s example."
  },
  {
    "objectID": "posts/fastai-lesson2/fastai-lesson2.html#summary",
    "href": "posts/fastai-lesson2/fastai-lesson2.html#summary",
    "title": "FastAI Lesson 2: Production",
    "section": "Summary",
    "text": "Summary\nIn this lesson, Jeremy walks through a bear classification model example and then spends most of the lesson focusing on how to deploy a model into production. The production example in this lesson works well for personal project but is different from what the process is in industry. I thought it was a useful lesson for how to quickly get a model and simple web interface up and running."
  },
  {
    "objectID": "posts/fastai-lesson2/fastai-lesson2.html#my-thoughts",
    "href": "posts/fastai-lesson2/fastai-lesson2.html#my-thoughts",
    "title": "FastAI Lesson 2: Production",
    "section": "My Thoughts",
    "text": "My Thoughts\nAlthough Jeremy didn’t directly address this in the lecture video, designing user interfaces for data science tasks is difficult (this is based on my personal experience) because there are many different requirements to satisfy. If your audience is a boss, manager, team, data scientists, they probably want to interact with a model ASAP. To save development time and get a basic application working tools like gradio, streamlit, altair and colab do a great job but trade performance, user experience and features for iteration speed and immediate results. Another audience is enthusiasts and expert users who want a core set of features that address performance, utility, interaction, and end user experience who may be willing to trade development time for a well crafted interactive data experience. For these needs, tools like D3, Svelte, Vue, React will give greater control and freedom for creative expression and delivery but require significant development time and testing.\nI would probably start off with gradio or streamlit for prototyping and then gradually work with a designer/web developer to build a scalable performant web interface to interact with the model. Through personal experience, I’ve learned that debugging python code that renders javascript, html and css for the web can be a nightmare especially when working with 1,000+ datapoints for interactive web data interfaces.\nGradio reminded me a lot of streamlit. I liked streamlit for its simplicity in creating components but the downside was that its simplicity offered few opportunities for cusomizing components beyond what streamlit offered. The syntax for both streamlit and gradio were difficult for me to follow initially because during debugging I sometimes ran into web rendering bugs that were caused by the way I used a function from the streamlit/gradio library. Through trial and error I figured out what works but the process took a long time because I couldn’t come up with an effective development process.\nI try to avoid using streamlit and gradio whenever I can but the process of learning how to set up gradio and deploying a model + interface on hugging face space was definitely worth it. I might come back to this lesson in the future once I finish the rest of the course to experiment with how to build an interface for a model."
  },
  {
    "objectID": "posts/fastai-lesson2/fastai-lesson2.html#jeremy-howards-advice",
    "href": "posts/fastai-lesson2/fastai-lesson2.html#jeremy-howards-advice",
    "title": "FastAI Lesson 2: Production",
    "section": "Jeremy Howard’s Advice",
    "text": "Jeremy Howard’s Advice\n\nTrain a model before data cleaning because it helps find data issues more quickly and easily."
  },
  {
    "objectID": "posts/fastai-lesson2/fastai-lesson2.html#terminology",
    "href": "posts/fastai-lesson2/fastai-lesson2.html#terminology",
    "title": "FastAI Lesson 2: Production",
    "section": "Terminology",
    "text": "Terminology\nObject Recognition - Computers can recognize what items are in an image at least as well as people can.\nObject Detection - Computers can recgonize where objects in an image are, and can highlight their locations and name each found object.\nSegmentation - A sub-area of object detection where every pixel is categorized on what kind of object it is part of\nData Augmentation - creating random variations of our input data such that they appear different but do not change the information in the data. Augmentation techniques for images include rotation, flipping, perspective warping, brightness, contrast."
  },
  {
    "objectID": "posts/fastai-lesson2/fastai-lesson2.html#picasso-or-matisse-model",
    "href": "posts/fastai-lesson2/fastai-lesson2.html#picasso-or-matisse-model",
    "title": "FastAI Lesson 2: Production",
    "section": "Picasso or Matisse Model",
    "text": "Picasso or Matisse Model\nInspired by Jeremy’s bear example, I changed my Picasso Braque example from my previous post and decided to instead try two different artists who weren’t so similar, in this case I chose Henri Matisse and Pablo Picasso. I trained the model the same way I did the Picasso Braques example but one thing I noticed was that some of the images seemed off for a duckduckgo search where duckduckgo would pull up a portrait of Picasso that didn’t look like it was from the same period I was searching. To try and fix this I tried to refresh the notebook runs multiple times until I got base images from actual Picasso and Matisse paintings. Similar to the Picasso Braque example, my Picasso predictions were off even though the label was correct but the Matisse ones were accurate.\nAfter I tried Picasso and Matisse paintings, I tried different Picasso artworks such as his sculptures and got correct classifications. One person I shared the model with tried uploading their family thanksgiving photo to see what happened and the model classified the photo as 97%. This was quite intriguing to me because I had not even trained the model on images other than picasso and matisse paintings. I’m only writing on Lesson 2, but Jeremy’s discussion about transfer learning in Lesson 1 and the book has me wondering if the classification of the 97% Picasso Thanksgiving photo is a bug and use case I didn’t consider or if it has something to do with the resnet-18 base model with how it picked up the color, line and movement.\n\n# install the latest libraries\n!pip install -Uqq fastai duckduckgo_search\n\n\nStep 1: Gather Data\n\nfrom duckduckgo_search import ddg_images\nfrom fastcore.all import *\n\n# helper function for searching for images\ndef search_images(term, max_images=30):\n    print(f\"Searching for '{term}'\")\n    return L(ddg_images(term, max_results=max_images)).itemgot('image')\n\n\nurls = search_images('Pablo Picasso Fauvism Paintings', max_images=1)\nurls[0]\n\nSearching for 'Pablo Picasso Fauvism Paintings'\n\n\n/usr/local/lib/python3.10/dist-packages/duckduckgo_search/compat.py:40: UserWarning: ddg_images is deprecated. Use DDGS().images() generator\n  warnings.warn(\"ddg_images is deprecated. Use DDGS().images() generator\")\n\n\n'https://i.pinimg.com/736x/b5/69/e1/b569e151ba0a9adf0136f5bdd7d4401b.jpg'\n\n\n\nfrom fastdownload import download_url\nfrom fastai.vision.all import *\ndest = 'picasso.jpg'\ndownload_url(urls[0], dest, show_progress=False)\nim = Image.open(dest)\nim.to_thumb(256, 256)\n\n\n\n\n\ndownload_url(search_images('Henri Matisse Fauvism Paintings', max_images=1)[0],\n            'matisse.jpg',\n            show_progress=False)\nImage.open('matisse.jpg').to_thumb(256, 256)\n\nSearching for 'Henri Matisse Fauvism Paintings'\n\n\n/usr/local/lib/python3.10/dist-packages/duckduckgo_search/compat.py:40: UserWarning: ddg_images is deprecated. Use DDGS().images() generator\n  warnings.warn(\"ddg_images is deprecated. Use DDGS().images() generator\")\n\n\n\n\n\n\nsearches = 'Pablo Picasso', 'Henri Matisse'\npath = Path('picasso_or_matisse')\nfrom time import sleep\n\nfor o in searches:\n    dest = (path/o)\n    dest.mkdir(exist_ok=True, parents=True)\n    download_images(dest,urls=search_images(f'{o} fauvism paintings'))\n    sleep(10) # sleep between searches to avoid spamming server\n    download_images(dest,urls=search_images(f'{o} still life fauvism paintings'))\n    sleep(10) # sleep between searches to avoid spamming server\n    download_images(dest,urls=search_images(f'{o} fauvism scenic paintings'))\n    sleep(10) # sleep between searches to avoid spamming server\n    resize_images(path/o, max_size=400, dest=path/o)\n\nSearching for 'Pablo Picasso fauvism paintings'\nSearching for 'Pablo Picasso still life fauvism paintings'\nSearching for 'Pablo Picasso fauvism scenic paintings'\nSearching for 'Henri Matisse fauvism paintings'\nSearching for 'Henri Matisse still life fauvism paintings'\nSearching for 'Henri Matisse fauvism scenic paintings'\n\n\n\n\nStep 2: Train Model\n\n# remove images that failed to download properly\nfailed = verify_images(get_image_files(path))\nfailed.map(Path.unlink)\nlen(failed)\n\n0\n\n\n\n# split data into training set, validation set\ndls = DataBlock(\n    # specify input type(image), output type(category aka label)\n    blocks=(ImageBlock, CategoryBlock),\n    get_items=get_image_files,\n    # split data into 80% training data, and 20% validation data\n    splitter=RandomSplitter(valid_pct=0.2, seed=42),\n    # define the label\n    get_y=parent_label,\n    # standardize and resize all images to 192 x 192\n    item_tfms=[Resize(192, method='squish')]\n).dataloaders(path, bs=32)\n\ndls.show_batch(max_n=6)\n\n\n\n\n\n# train a resnet model on the data\nlearn = vision_learner(dls, resnet18, metrics=error_rate)\nlearn.fine_tune(4)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n1.003311\n0.399668\n0.160377\n00:04\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n0.362328\n0.233523\n0.084906\n00:02\n\n\n1\n0.323960\n0.221361\n0.075472\n00:03\n\n\n2\n0.253416\n0.133047\n0.037736\n00:04\n\n\n3\n0.199347\n0.111490\n0.037736\n00:02\n\n\n\n\n\n\n\nStep 3: Test Model\n\nis_picasso,_,probs = learn.predict(PILImage.create('picasso.jpg'))\nprint(f\"This is a: {is_picasso}.\")\nprint(f\"Probability it's a picasso: {probs[0]:.4f}\")\n\n\n\n\n\n\n\n\nThis is a: Pablo Picasso.\nProbability it's a picasso: 0.0005\n\n\n\nis_matisse,_,probs = learn.predict(PILImage.create('matisse.jpg'))\nprint(f\"This is a: {is_matisse}.\")\nprint(f\"Probability it's a matisse: {probs[0]:.4f}\")\n\n\n\n\n\n\n\n\nThis is a: Henri Matisse.\nProbability it's a matisse: 0.9936\n\n\n\n\nStep 4: Save and Export Model\n\nlearn.export('model.pkl')"
  },
  {
    "objectID": "posts/fastai-lesson2/fastai-lesson2.html#gradio-hugging-face-spaces",
    "href": "posts/fastai-lesson2/fastai-lesson2.html#gradio-hugging-face-spaces",
    "title": "FastAI Lesson 2: Production",
    "section": "Gradio + Hugging Face Spaces",
    "text": "Gradio + Hugging Face Spaces\nThe following code was originally written by Dr. Tanishq Abraham and published in the blog post: Gradio + Hugging Face Spaces: A Tutorial. It was modified by me and Kevin Liu to work with the current version of the gradio api. Currently the code works on Hugging Face Spaces but may break in the future as gradio continues updating its API.\nMy recommendation to get gradio and Hugging Face Spaces working is to start off with Tanishq’s article and consult the gradio documentation to figure out the differences between the current version of the API and the version used in the article. I tried getting the pet classifier example working first before moving on to applying gradio to my Picasso Matisse Model which saved a lot of headache trying to figure out how git LFS and Hugging Face Spaces worked with my example.\n\nimport gradio as gr\nfrom fastai.vision.all import *\nimport skimage\n\n# load model \nlearn = load_learner('model.pkl') \n\n# define prediction function \nlabels = learn.dls.vocab\ndef predict(img):\n    img = PILImage.create(img)\n    pred,pred_idx,probs = learn.predict(img)\n    return {labels[i]: float(probs[i]) for i in range(len(labels))}\n\n# define interface structure \ntitle = \"Picasso or Mattise Classifier\"\ndescription = \"A classifier trained on Pablo Picasso and Henri Mattise paintings with fast.ai.\"\nexamples = ['picasso.jpg', 'matisse.jpg']\ngr.Interface(fn=predict, inputs=[gr.Image(type=\"pil\")], outputs=gr.Label(num_top_classes=3)).launch(share=True)\n\n# interpretation = 'default'\n# enable_queue = True \n\n\n# def greet(name):\n#     return \"Hello \" + name + \"!!\"\n\n# iface = gr.Interface(fn=greet, inputs=\"text\", outputs=\"text\")\n# iface.launch()"
  },
  {
    "objectID": "posts/fastai-lesson2/fastai-lesson2.html#resources",
    "href": "posts/fastai-lesson2/fastai-lesson2.html#resources",
    "title": "FastAI Lesson 2: Production",
    "section": "Resources",
    "text": "Resources\n\nPicasso or Mattise Gradio + Hugging Face Spaces Application\nFastAI Lesson 2\nFastAI Chapter 2\nGradio + Hugging Face Spaces: A Tutorial\nHow to Set Up an SSH Key(GitHub, Hugging Face Spaces)\nHugging Face Spaces\nGradio docs\nJeremy Howard FastAI Live Coding\nfastai docs"
  },
  {
    "objectID": "posts/fastai-lesson4/fastai-lesson4.html",
    "href": "posts/fastai-lesson4/fastai-lesson4.html",
    "title": "FastAI Lesson 4: Natural Language(NLP)",
    "section": "",
    "text": "Happy New Year! I’m finally back to FastAI after taking a break due to job applications, trying to learn Golang and reflecting on my job search. This lesson was particularly challenging for me because I ran into a bunch of weird bugs and updates to the hugging face transformer API. I had no idea what the code was doing in the FastAI example despite copying Jeremy’s code verbatim and printing print statements. After spending some time trying to debug and understand what each part of the code setup was doing with hugging face and Jeremy’s code I finally feel comfortable trying to write a blog post about this lesson."
  },
  {
    "objectID": "posts/fastai-lesson4/fastai-lesson4.html#announcements",
    "href": "posts/fastai-lesson4/fastai-lesson4.html#announcements",
    "title": "FastAI Lesson 4: Natural Language(NLP)",
    "section": "",
    "text": "Happy New Year! I’m finally back to FastAI after taking a break due to job applications, trying to learn Golang and reflecting on my job search. This lesson was particularly challenging for me because I ran into a bunch of weird bugs and updates to the hugging face transformer API. I had no idea what the code was doing in the FastAI example despite copying Jeremy’s code verbatim and printing print statements. After spending some time trying to debug and understand what each part of the code setup was doing with hugging face and Jeremy’s code I finally feel comfortable trying to write a blog post about this lesson."
  },
  {
    "objectID": "posts/fastai-lesson4/fastai-lesson4.html#acknowledgements",
    "href": "posts/fastai-lesson4/fastai-lesson4.html#acknowledgements",
    "title": "FastAI Lesson 4: Natural Language(NLP)",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nAll of this code was written by Jeremy Howard and the FastAI team and I modified it slightly to include my own print statements and additional helper functions based on Jeremy’s code. This is the source for the original code Getting Start With NLP for Absolute Begginers and Iterate Like A Grandmaster."
  },
  {
    "objectID": "posts/fastai-lesson4/fastai-lesson4.html#summary",
    "href": "posts/fastai-lesson4/fastai-lesson4.html#summary",
    "title": "FastAI Lesson 4: Natural Language(NLP)",
    "section": "Summary",
    "text": "Summary\nIn this lesson, Jeremy gives an overview of his pioneering work with ULMFit, how to use Transformers and the set up for participating on kaggle.\nI’m no NLP expert and don’t have an interest in NLP but what really excited me was that I finally understood what the iteration and development process was for a Kaggle Competition. This had been one of my goals when I embarked on FastAI."
  },
  {
    "objectID": "posts/fastai-lesson4/fastai-lesson4.html#jeremy-howards-advice",
    "href": "posts/fastai-lesson4/fastai-lesson4.html#jeremy-howards-advice",
    "title": "FastAI Lesson 4: Natural Language(NLP)",
    "section": "Jeremy Howard’s Advice",
    "text": "Jeremy Howard’s Advice\nKnow the tools for your trade really well - Data Science Core Tools: Python, Numpy, Pandas Matplotlib, ScikitLearn, Pytorch, Scipy"
  },
  {
    "objectID": "posts/fastai-lesson4/fastai-lesson4.html#data-sciencekaggle-competition-workflow",
    "href": "posts/fastai-lesson4/fastai-lesson4.html#data-sciencekaggle-competition-workflow",
    "title": "FastAI Lesson 4: Natural Language(NLP)",
    "section": "Data Science/Kaggle Competition Workflow",
    "text": "Data Science/Kaggle Competition Workflow\nThis is the workflow that I figured out from this chapter.\n\nImport data and check files\nExploratory Data Analysis\nData Representation (Wrangling, Tokenization, Numericalization)\nMetrics\nTraining Model\nEvaluate Model Performance\n\nI found the steps similar to the visualization process and could have dedicated some more time to iterating and fine-tuning my model for scoring high on this kaggle competition but in this chapter I was aiming to get an understanding of the general workflow and process that goes into building and iterating a model."
  },
  {
    "objectID": "posts/fastai-lesson4/fastai-lesson4.html#us-patent-phrase-to-phrase-matching-competition",
    "href": "posts/fastai-lesson4/fastai-lesson4.html#us-patent-phrase-to-phrase-matching-competition",
    "title": "FastAI Lesson 4: Natural Language(NLP)",
    "section": "US Patent Phrase to Phrase Matching Competition",
    "text": "US Patent Phrase to Phrase Matching Competition\nThe US Patent Phrase to Phrase Matching Kaggle Competition is used as the case study for learning the basics of NLP, how to use transformers and the kaggle workflow. More information about the data and competition can be found here US Patent Phrase to Phrase Matching Competition"
  },
  {
    "objectID": "posts/fastai-lesson4/fastai-lesson4.html#load-data-and-libraries",
    "href": "posts/fastai-lesson4/fastai-lesson4.html#load-data-and-libraries",
    "title": "FastAI Lesson 4: Natural Language(NLP)",
    "section": "Load Data and Libraries",
    "text": "Load Data and Libraries\n\n# import libraries and files\n\n# required libraries + packages for any ml/data science project\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport torch\n\n# fastai library contains all the packages above and wraps them in the fastai library\n!pip install -Uqq fastai\n\n# kaggle API package install\n!pip install kaggle\n\n# Hugging Face required libraries + packages\n!pip install -q datasets\n!pip install transformers sentencepiece\n!pip install transformers[torch]\n\nRequirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (1.5.16)\nRequirement already satisfied: six&gt;=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle) (1.16.0)\nRequirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from kaggle) (2023.11.17)\nRequirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.8.2)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.31.0)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kaggle) (4.66.1)\nRequirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle) (8.0.1)\nRequirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.0.7)\nRequirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle) (6.1.0)\nRequirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach-&gt;kaggle) (0.5.1)\nRequirement already satisfied: text-unidecode&gt;=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify-&gt;kaggle) (1.3)\nRequirement already satisfied: charset-normalizer&lt;4,&gt;=2 in /usr/local/lib/python3.10/dist-packages (from requests-&gt;kaggle) (3.3.2)\nRequirement already satisfied: idna&lt;4,&gt;=2.5 in /usr/local/lib/python3.10/dist-packages (from requests-&gt;kaggle) (3.6)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 507.1/507.1 kB 6.7 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 115.3/115.3 kB 11.3 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 134.8/134.8 kB 9.9 MB/s eta 0:00:00\nRequirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\nCollecting sentencepiece\n  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 13.0 MB/s eta 0:00:00\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\nRequirement already satisfied: huggingface-hub&lt;1.0,&gt;=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.2)\nRequirement already satisfied: numpy&gt;=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\nRequirement already satisfied: packaging&gt;=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\nRequirement already satisfied: pyyaml&gt;=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\nRequirement already satisfied: tokenizers&lt;0.19,&gt;=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\nRequirement already satisfied: safetensors&gt;=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.1)\nRequirement already satisfied: tqdm&gt;=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\nRequirement already satisfied: fsspec&gt;=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub&lt;1.0,&gt;=0.16.4-&gt;transformers) (2023.6.0)\nRequirement already satisfied: typing-extensions&gt;=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub&lt;1.0,&gt;=0.16.4-&gt;transformers) (4.5.0)\nRequirement already satisfied: charset-normalizer&lt;4,&gt;=2 in /usr/local/lib/python3.10/dist-packages (from requests-&gt;transformers) (3.3.2)\nRequirement already satisfied: idna&lt;4,&gt;=2.5 in /usr/local/lib/python3.10/dist-packages (from requests-&gt;transformers) (3.6)\nRequirement already satisfied: urllib3&lt;3,&gt;=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests-&gt;transformers) (2.0.7)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests-&gt;transformers) (2023.11.17)\nInstalling collected packages: sentencepiece\nSuccessfully installed sentencepiece-0.1.99\nRequirement already satisfied: transformers[torch] in /usr/local/lib/python3.10/dist-packages (4.35.2)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (3.13.1)\nRequirement already satisfied: huggingface-hub&lt;1.0,&gt;=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.20.2)\nRequirement already satisfied: numpy&gt;=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (1.23.5)\nRequirement already satisfied: packaging&gt;=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (23.2)\nRequirement already satisfied: pyyaml&gt;=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2023.6.3)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.31.0)\nRequirement already satisfied: tokenizers&lt;0.19,&gt;=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.15.0)\nRequirement already satisfied: safetensors&gt;=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.4.1)\nRequirement already satisfied: tqdm&gt;=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (4.66.1)\nRequirement already satisfied: torch!=1.12.0,&gt;=1.10 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.1.0+cu121)\nCollecting accelerate&gt;=0.20.3 (from transformers[torch])\n  Downloading accelerate-0.26.1-py3-none-any.whl (270 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 270.9/270.9 kB 6.2 MB/s eta 0:00:00\nRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate&gt;=0.20.3-&gt;transformers[torch]) (5.9.5)\nRequirement already satisfied: fsspec&gt;=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub&lt;1.0,&gt;=0.16.4-&gt;transformers[torch]) (2023.6.0)\nRequirement already satisfied: typing-extensions&gt;=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub&lt;1.0,&gt;=0.16.4-&gt;transformers[torch]) (4.5.0)\nRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,&gt;=1.10-&gt;transformers[torch]) (1.12)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,&gt;=1.10-&gt;transformers[torch]) (3.2.1)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,&gt;=1.10-&gt;transformers[torch]) (3.1.3)\nRequirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,&gt;=1.10-&gt;transformers[torch]) (2.1.0)\nRequirement already satisfied: charset-normalizer&lt;4,&gt;=2 in /usr/local/lib/python3.10/dist-packages (from requests-&gt;transformers[torch]) (3.3.2)\nRequirement already satisfied: idna&lt;4,&gt;=2.5 in /usr/local/lib/python3.10/dist-packages (from requests-&gt;transformers[torch]) (3.6)\nRequirement already satisfied: urllib3&lt;3,&gt;=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests-&gt;transformers[torch]) (2.0.7)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests-&gt;transformers[torch]) (2023.11.17)\nRequirement already satisfied: MarkupSafe&gt;=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2-&gt;torch!=1.12.0,&gt;=1.10-&gt;transformers[torch]) (2.1.3)\nRequirement already satisfied: mpmath&gt;=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy-&gt;torch!=1.12.0,&gt;=1.10-&gt;transformers[torch]) (1.3.0)\nInstalling collected packages: accelerate\nSuccessfully installed accelerate-0.26.1\n\n\n\n# import stuff from fastai\nfrom fastai.imports import *\nimport os\nfrom pathlib import Path\nimport zipfile\n\n\n'''Function for loading kaggle datasets locally or on kaggle\nReturns a local path to data files\n- input: Kaggle API Login Credentials, Kaggle Contest Name '''\ndef loadData(creds, dataFile):\n    # variable to check whether we're running on kaggle website or not\n    iskaggle = os.environ.get('KAGGLE_KERNEL_RUN_TYPE', '')\n\n    # path for kaggle API credentials\n    cred_path = Path('~/.kaggle/kaggle.json').expanduser()\n\n    if not cred_path.exists():\n        cred_path.parent.mkdir(exist_ok=True)\n        cred_path.write_text(creds)\n        cred_path.chmod(0o600)\n\n    # Download data from Kaggle to path and extract files at path location\n\n    # local machine\n    path = Path(dataFile)\n    if not iskaggle and not path.exists():\n        import kaggle\n        kaggle.api.competition_download_cli(str(path))\n        zipfile.ZipFile(f'{path}.zip').extractall(path)\n\n    # kaggle\n    if iskaggle:\n        fileName = '../input/' + dataFile\n        path = fileName\n\n    return path\n\n\ncreds = '{\"username\":\"greenvegetable320\",\"key\":\"f56b8411bbf4c1ca7832e004bc998125\"}'\ndataFile = 'us-patent-phrase-to-phrase-matching'\npath = loadData(creds, dataFile)\n\nDownloading us-patent-phrase-to-phrase-matching.zip to /content\n\n\n\n100%|██████████| 682k/682k [00:00&lt;00:00, 727kB/s]\n\n\nDocument - A file containing some text\nLarge Documents - One text file per document, often organized into one folder per category\nSmaller Documents - One document per row in a CSV File\n\n# check data files\n!ls {path}\n\nsample_submission.csv  test.csv  train.csv"
  },
  {
    "objectID": "posts/fastai-lesson4/fastai-lesson4.html#problem-statement",
    "href": "posts/fastai-lesson4/fastai-lesson4.html#problem-statement",
    "title": "FastAI Lesson 4: Natural Language(NLP)",
    "section": "Problem Statement",
    "text": "Problem Statement\nProblem: given a pairs of phrases (an anchor or target phrase), rate how similar they are on a scale of 0(not similar) to 1(identical in meaning). Similarity has been scored within a patent’s context specifically the CPC Classification which indicates the subject to which the patent relates.\nTraining Data - contains phrases, contexts and similarity scores\nTest(Inference) Data - unseen test contains approximately 12k pairs of phrases. Small public test set has been provided for testing purposes but is not use in scoring. - contains identitical structure to training data but without the score\nEvaluation Goal - Build model to match phrases in order to extract contextual information helping the patent community connect the dots between millions of patent documents\nEvaluation Metric - Submissions are evaluated on the Pearson Correlation Coefficient between predicted and actual similarity scores.\nSubmission Format - id(representing a pair of phrases) in test set, score representing the similarity score\n\nA score of 1 is considered that the two inputs have identical meaning\nA score of 0 is considered that the two inputs have totally different meaning\nA score in between ie. 0.5 means they’re somewhat similar but not identical\nProblem Type - NLP Classification Problem -&gt; classify document automatically into some category"
  },
  {
    "objectID": "posts/fastai-lesson4/fastai-lesson4.html#exploratory-data-analysis",
    "href": "posts/fastai-lesson4/fastai-lesson4.html#exploratory-data-analysis",
    "title": "FastAI Lesson 4: Natural Language(NLP)",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\n\n# set up data path\n\n# training data path\ndf = pd.read_csv(path/'train.csv')\n\n# testing data path\neval_df = pd.read_csv(path/'test.csv')\n\n# sample submission data path\nsubmit_df = pd.read_csv(path/'sample_submission.csv')\n\n\n# Test Data Length\nprint(f\"{len(df)}\")\n\n36473\n\n\n\n# Training Data Info\ndf\n\n\n  \n    \n\n\n\n\n\n\nid\nanchor\ntarget\ncontext\nscore\n\n\n\n\n0\n37d61fd2272659b1\nabatement\nabatement of pollution\nA47\n0.50\n\n\n1\n7b9652b17b68b7a4\nabatement\nact of abating\nA47\n0.75\n\n\n2\n36d72442aefd8232\nabatement\nactive catalyst\nA47\n0.25\n\n\n3\n5296b0c19e1ce60e\nabatement\neliminating process\nA47\n0.50\n\n\n4\n54c1e3b9184cb5b6\nabatement\nforest region\nA47\n0.00\n\n\n...\n...\n...\n...\n...\n...\n\n\n36468\n8e1386cbefd7f245\nwood article\nwooden article\nB44\n1.00\n\n\n36469\n42d9e032d1cd3242\nwood article\nwooden box\nB44\n0.50\n\n\n36470\n208654ccb9e14fa3\nwood article\nwooden handle\nB44\n0.50\n\n\n36471\n756ec035e694722b\nwood article\nwooden material\nB44\n0.75\n\n\n36472\n8d135da0b55b8c88\nwood article\nwooden substrate\nB44\n0.50\n\n\n\n\n\n36473 rows × 5 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\n\n# Training Data Summary\ndf.describe(include='object')\n\n\n  \n    \n\n\n\n\n\n\nid\nanchor\ntarget\ncontext\n\n\n\n\ncount\n36473\n36473\n36473\n36473\n\n\nunique\n36473\n733\n29340\n106\n\n\ntop\n37d61fd2272659b1\ncomponent composite coating\ncomposition\nH01\n\n\nfreq\n1\n152\n24\n2186\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\n# Test Data Length\nprint(f\"{len(eval_df)}\")\n\n36\n\n\n\n# Test Data Info\neval_df.head()\n\n\n  \n    \n\n\n\n\n\n\nid\nanchor\ntarget\ncontext\n\n\n\n\n0\n4112d61851461f60\nopc drum\ninorganic photoconductor drum\nG02\n\n\n1\n09e418c93a776564\nadjust gas flow\naltering gas flow\nF23\n\n\n2\n36baf228038e314b\nlower trunnion\nlower locating\nB60\n\n\n3\n1f37ead645e7f0c8\ncap component\nupper portion\nD06\n\n\n4\n71a5b6ad068d531f\nneural stimulation\nartificial neural network\nH04\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\n# Test Data Summary\neval_df.describe()\n\n\n  \n    \n\n\n\n\n\n\nid\nanchor\ntarget\ncontext\n\n\n\n\ncount\n36\n36\n36\n36\n\n\nunique\n36\n34\n36\n29\n\n\ntop\n4112d61851461f60\nel display\ninorganic photoconductor drum\nG02\n\n\nfreq\n1\n2\n1\n3"
  },
  {
    "objectID": "posts/fastai-lesson4/fastai-lesson4.html#data-representation-tokenization-numericalization-data-wrangling",
    "href": "posts/fastai-lesson4/fastai-lesson4.html#data-representation-tokenization-numericalization-data-wrangling",
    "title": "FastAI Lesson 4: Natural Language(NLP)",
    "section": "Data Representation (Tokenization, Numericalization, Data Wrangling)",
    "text": "Data Representation (Tokenization, Numericalization, Data Wrangling)\nModels require numbers as inputs -&gt; need some strategy of mapping words, characters etc to a numerical value\nTokenization: Split each text up into tokens\nNumericalization: Convert each token to a number\nAutokenizer:function for creating a tokenizer appropriate for the selected NLP model\nVocab: A special list in the tokenizer which contains a unique integer for every possible token string\nThe start of a new word is indicated by __\n\nfrom torch.utils.data import DataLoader\nimport warnings,transformers,logging,torch\nfrom transformers import TrainingArguments,Trainer\nfrom transformers import AutoModelForSequenceClassification,AutoTokenizer\nimport datasets\nfrom datasets import load_dataset, Dataset, DatasetDict\n\n# ignore hugging face warnings\nwarnings.simplefilter('ignore')\nlogging.disable(logging.WARNING)\n\n\n# Tokenization + Numericalization\n\n# - Models require numbers as inputs -&gt; need to convert text to numbers:\n# Tokenization - split each text into tokens\n# Numericalization - convert each word into a number\n# - This process depends on the model that is used. AutoTokenizer creates the appropriate tokenizer based on the selected model\n\n# Model\n# - This is a reasonable model to start nearly NLP problem. Replace small with large for a slower but more accurate model\n# once data exploration and experimentation is completed\nmodel_nm = 'microsoft/deberta-v3-small'\n\n# Tokenizer\nfrom transformers import AutoModelForSequenceClassification,AutoTokenizer\ntokz = AutoTokenizer.from_pretrained(model_nm)\n\n# Test Tokenizer\nprint(f\"Favorite Line from The Bear: {tokz.tokenize('Every Second Counts')}\")\nprint(f\"Uncommon words: {tokz.tokenize('A platypus is an ornithorhynchus anatinus.')}\")\n\n# function for tokenizing\ndef tok_func(x):\n  return tokz(x[\"input\"])\n\n\n\n\n\n\n\n\n\n\nFavorite Line from The Bear: ['▁Every', '▁Second', '▁Counts']\nUncommon words: ['▁A', '▁platypus', '▁is', '▁an', '▁or', 'ni', 'tho', 'rhynch', 'us', '▁an', 'at', 'inus', '.']\n\n\n\n# Combine Context + Anchor + Target\n\n\n# Method 1 - Baseline\ndf['input'] = 'CONTEXT: ' + df.context + '; TEXT1: ' + df.anchor + '; TEXT2: ' + df.target\neval_df['input'] = 'CONTEXT: ' + eval_df.context + '; TEXT1: ' + eval_df.anchor + '; TEXT2: ' + eval_df.target\n\n\n# Convert data to Transformer DataSet Data Structure\n# rename score to labels for training data -&gt; Transformers require a label column\nds = Dataset.from_pandas(df).rename_column('score', 'label')\neval_ds = Dataset.from_pandas(eval_df)\n\n\n# Tokenize Training Data\nprint(\"Original Input Data\")\nprint(ds)\n\n# test tokenizing function\n# tok_func(ds[0])\n\n# Tokenize all input data in parallel\ntok_ds = ds.map(tok_func, batched=True)\n\nprint(\"Tokenized Input Data\")\nprint(tok_ds)\n\n# check tokenized data\nprint(\"Check Tokenized Input Data\")\nprint(tok_ds[0])\n\nOriginal Input Data\nDataset({\n    features: ['id', 'anchor', 'target', 'context', 'label', 'input'],\n    num_rows: 36473\n})\nTokenized Input Data\nDataset({\n    features: ['id', 'anchor', 'target', 'context', 'label', 'input', 'input_ids', 'token_type_ids', 'attention_mask'],\n    num_rows: 36473\n})\nCheck Tokenized Input Data\n{'id': '37d61fd2272659b1', 'anchor': 'abatement', 'target': 'abatement of pollution', 'context': 'A47', 'label': 0.5, 'input': 'CONTEXT: A47; TEXT1: abatement; TEXT2: abatement of pollution', 'input_ids': [1, 20967, 104917, 294, 336, 5753, 346, 54453, 435, 294, 47284, 346, 54453, 445, 294, 47284, 265, 6435, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n\n\n\n\n\n\n# Tokenize Inference Data\nprint(\"Original Inference Data\")\nprint(eval_ds)\n\neval_ds = eval_ds.map(tok_func, batched=True)\n\nprint(\"Tokenized Inference Data\")\nprint(eval_ds)\n\n\nprint(\"Check Tokenized Inference Data\")\nprint(eval_ds[0])\n\nOriginal Inference Data\nDataset({\n    features: ['id', 'anchor', 'target', 'context', 'input'],\n    num_rows: 36\n})\nTokenized Inference Data\nDataset({\n    features: ['id', 'anchor', 'target', 'context', 'input', 'input_ids', 'token_type_ids', 'attention_mask'],\n    num_rows: 36\n})\nCheck Tokenized Inference Data\n{'id': '4112d61851461f60', 'anchor': 'opc drum', 'target': 'inorganic photoconductor drum', 'context': 'G02', 'input': 'CONTEXT: G02; TEXT1: opc drum; TEXT2: inorganic photoconductor drum', 'input_ids': [1, 20967, 104917, 294, 1098, 4159, 346, 54453, 435, 294, 8847, 1207, 8263, 346, 54453, 445, 294, 31553, 1456, 48133, 8263, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n\n\n\n\n\n\n# Tokenization Exploration\n\n# - Vocab -&gt; specal list in the tokenizer which contains a unique integer for every possible token string\n# - Input IDs are generated from the vocab list\nrow = tok_ds[0]\nprint(f\"Input: {row['input']}, Input IDs: {row['input_ids']}\")\n\n# Token for the word \"of\"\nprint(f\"Token for the word of: {tokz.vocab['▁of']}\")\n\nInput: CONTEXT: A47; TEXT1: abatement; TEXT2: abatement of pollution, Input IDs: [1, 20967, 104917, 294, 336, 5753, 346, 54453, 435, 294, 47284, 346, 54453, 445, 294, 47284, 265, 6435, 2]\nToken for the word of: 265"
  },
  {
    "objectID": "posts/fastai-lesson4/fastai-lesson4.html#datasets-training-validation-testinginference",
    "href": "posts/fastai-lesson4/fastai-lesson4.html#datasets-training-validation-testinginference",
    "title": "FastAI Lesson 4: Natural Language(NLP)",
    "section": "Datasets Training, Validation, Testing(Inference)",
    "text": "Datasets Training, Validation, Testing(Inference)\n\nTraining Dataset\nA set of data that the model uses to learn the weights needed to create a function that best approximates the data.\n\n\nValidation Dataset\n\nA dataset that is used to determine whether the model is under-fitting, overfitting or fitting the data (to some threshold degree). - This dataset is withheld from training and the model never sees it. The validation set is only used for evaluating the model during training and never used as inputs to train the model\nCreating validation datasets is an active area of research and engineering. See this post by Dr. Rachel Thomas on validation datasets - How (and why) to create a good validation set\nTransformers call the validation dataset in the DataSetDict object test\n\n\n\nTest(Inference Set)\n\nA dataset that is withheld from training and reporting metrics. The accuracy of the model on the test set is only checked after completing the entire training process -&gt; trying different models, training methods, data processing, optimizations etc.\nKaggle has a second test set which is a held out dataset used only at the end of competitions to assess predictions (private leaderboard).\n\n\n# Validation Set - subset of training dataset which the model doesn't see at all\n# - The validation set is used to check if model is underfit, overfit or just right\n# - The validation set is nost used as input to the training model\n# - Transformers use DatasetDict to store both the training and validation dataset\n# - fastai automatically creates the validation set for you if you don't have one and reports the metrics\n# (accuracy of model) using the validation set\n\n# 25% validation set, 75% training set using hugging face transformer\n# test refers to the validation set\ndds = tok_ds.train_test_split(0.25, seed=42)\n\nprint(\"Check Training-Validation Split\")\nprint(dds)\n\nCheck Training-Validation Split\nDatasetDict({\n    train: Dataset({\n        features: ['id', 'anchor', 'target', 'context', 'label', 'input', 'input_ids', 'token_type_ids', 'attention_mask'],\n        num_rows: 27354\n    })\n    test: Dataset({\n        features: ['id', 'anchor', 'target', 'context', 'label', 'input', 'input_ids', 'token_type_ids', 'attention_mask'],\n        num_rows: 9119\n    })\n})"
  },
  {
    "objectID": "posts/fastai-lesson4/fastai-lesson4.html#metrics",
    "href": "posts/fastai-lesson4/fastai-lesson4.html#metrics",
    "title": "FastAI Lesson 4: Natural Language(NLP)",
    "section": "Metrics",
    "text": "Metrics\n\nMeasurements that help evaluate how good the model is\nKaggle tells users the metric and how submissions are evaluated in the problem description. In industry and research problems developing metrics is more complicated. See this post by Dr. Rachel Thomas on metrics - The problem with metrics is a big problem for AI\nIn this challenge, Kaggle has stated that submissions are evaluated based on the Pearson Correlation Coefficient between the predicted and actual similarity scores.\nPearson’s Correlation Coefficient is represented with r and is one of the most widely used measures of the degree of relationship between two variables.\nr can vary between -1 indicating a perfect inverse correlation and +1 indicating a perfect positive correlation\nr is sensitive to outliers -&gt; outliers will dominate the data in a pearson correlation visualization producing skewed results.\n\n\n# Metrics - Correlation\n\n# function for returning the correlation between two variables\ndef corr(x,y):\n  return np.corrcoef(x,y)[0][1]\n\n# - Transformers expect metrics to be returned as a dict, since that way the trainer knows what label to use\ndef corr_d(eval_pred):\n   return {'pearson': corr(*eval_pred)}"
  },
  {
    "objectID": "posts/fastai-lesson4/fastai-lesson4.html#train-model",
    "href": "posts/fastai-lesson4/fastai-lesson4.html#train-model",
    "title": "FastAI Lesson 4: Natural Language(NLP)",
    "section": "Train Model",
    "text": "Train Model\n\n# Set up hyperparameter values\n\n# learning rate\nlr = 8e-5\n\n# batch size\nbs = 128\n\n# weight decay\nwd = 0.01\n\n# epochs\nepochs = 4\n\n\n# - Transformer uses Trainging Arguments class to set up arguments\nargs = TrainingArguments('outputs', learning_rate=lr, warmup_ratio=0.1, lr_scheduler_type='cosine', fp16=True,\n    evaluation_strategy=\"epoch\", per_device_train_batch_size=bs, per_device_eval_batch_size=bs*2,\n    num_train_epochs=epochs, weight_decay=wd, report_to='none')\n\n\n# create model and Trainer -&gt; trainer is a class combining the data and model together (like learner does in fastai)\nmodel = AutoModelForSequenceClassification.from_pretrained(model_nm, num_labels=1)\ntrainer = Trainer(model, args, train_dataset=dds['train'], eval_dataset=dds['test'],\n                  tokenizer=tokz, compute_metrics=corr_d)\n\n\n\n\n\n# Train the model\ntrainer.train()\n\n\n    \n      \n      \n      [856/856 03:37, Epoch 4/4]\n    \n    \n\n\n\nEpoch\nTraining Loss\nValidation Loss\nPearson\n\n\n\n\n1\nNo log\n0.028198\n0.803834\n\n\n2\nNo log\n0.022185\n0.823395\n\n\n3\n0.034200\n0.022253\n0.833714\n\n\n4\n0.034200\n0.022595\n0.834576\n\n\n\n\n\n\nTrainOutput(global_step=856, training_loss=0.025547120615700695, metrics={'train_runtime': 220.4986, 'train_samples_per_second': 496.221, 'train_steps_per_second': 3.882, 'total_flos': 717218678299260.0, 'train_loss': 0.025547120615700695, 'epoch': 4.0})"
  },
  {
    "objectID": "posts/fastai-lesson4/fastai-lesson4.html#evaluate-model-performance",
    "href": "posts/fastai-lesson4/fastai-lesson4.html#evaluate-model-performance",
    "title": "FastAI Lesson 4: Natural Language(NLP)",
    "section": "Evaluate Model Performance",
    "text": "Evaluate Model Performance\n\n# Evaluate Model on the Inference Data\npreds = trainer.predict(eval_ds).predictions.astype(float)\npreds = np.clip(preds, 0, 1)\npreds\n\n\n\n\narray([[0.4855957 ],\n       [0.69238281],\n       [0.54833984],\n       [0.34765625],\n       [0.        ],\n       [0.47460938],\n       [0.49023438],\n       [0.        ],\n       [0.34765625],\n       [1.        ],\n       [0.23352051],\n       [0.27612305],\n       [0.76757812],\n       [0.91113281],\n       [0.81347656],\n       [0.37280273],\n       [0.26318359],\n       [0.        ],\n       [0.63720703],\n       [0.34619141],\n       [0.40063477],\n       [0.23999023],\n       [0.14367676],\n       [0.22607422],\n       [0.60595703],\n       [0.        ],\n       [0.        ],\n       [0.        ],\n       [0.        ],\n       [0.57910156],\n       [0.30908203],\n       [0.07293701],\n       [0.71191406],\n       [0.53564453],\n       [0.42602539],\n       [0.24609375]])"
  },
  {
    "objectID": "posts/fastai-lesson4/fastai-lesson4.html#improving-iterating-on-the-model-for-kaggle-and-beyond",
    "href": "posts/fastai-lesson4/fastai-lesson4.html#improving-iterating-on-the-model-for-kaggle-and-beyond",
    "title": "FastAI Lesson 4: Natural Language(NLP)",
    "section": "Improving + Iterating on the Model for Kaggle and Beyond",
    "text": "Improving + Iterating on the Model for Kaggle and Beyond\nIn the previous code, I mostly followed a standard template for how to get a model running and producing values for a kaggle competition.\nThis section is based on Jeremy’s advice and experience for understanding how one might go about refining and finetune a model using transfer learning to win a kaggle competition. My goal was to try and beat the original base line evaluation I had in the previous section but I didn’t do enough experiments to develop a model that could match or produce a better result than my first attempt. In this section, I was more interested in getting a feel for the iteration and engineering workflow process of developing a model for a kaggle contest/data science challenge so I might revisit this post in the future to see if I can produce a better result once I gain more experience iterating and building models.\n\nProblem Statement\n\nCompare two words or short phrases and score them based on whether they’re similar or not based on the patent class they were used in.\nA score of 1 is considered that the two inputs have identical meaning\nA score of 0 is considered that the two inputs have totally different meaning\nA score in between ie. 0.5 means they’re somewhat similar but not identical\nProblem Type - NLP Classification Problem -&gt; classify document automatically into some category\n\n\n\nData Exploration: Data Meaning + Representation\nanchor - first phrase\ntarget - second phrase\ncontext - cpc classification which indicates the subject within which the similarity is to be scored\nscore - the similarity. A combination of one or more manual expert ratings"
  },
  {
    "objectID": "posts/fastai-lesson4/fastai-lesson4.html#exploratory-data-analysis-1",
    "href": "posts/fastai-lesson4/fastai-lesson4.html#exploratory-data-analysis-1",
    "title": "FastAI Lesson 4: Natural Language(NLP)",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\n\n# Distribution of values of Target\ndf.target.value_counts()\n\ncomposition                    24\ndata                           22\nmetal                          22\nmotor                          22\nassembly                       21\n                               ..\nswitching switch over valve     1\nswitching switch off valve      1\nswitching over valve            1\nswitching off valve             1\nwooden substrate                1\nName: target, Length: 29340, dtype: int64\n\n\n\n# Distribution of values of Anchor\ndf.anchor.value_counts()\n\ncomponent composite coating              152\nsheet supply roller                      150\nsource voltage                           140\nperfluoroalkyl group                     136\nel display                               135\n                                        ... \nplug nozzle                                2\nshannon                                    2\ndry coating composition1                   2\nperipheral nervous system stimulation      1\nconduct conducting material                1\nName: anchor, Length: 733, dtype: int64\n\n\n\n# Distribution of values of Context\ndf.context.value_counts()\n\nH01    2186\nH04    2177\nG01    1812\nA61    1477\nF16    1091\n       ... \nB03      47\nF17      33\nB31      24\nA62      23\nF26      18\nName: context, Length: 106, dtype: int64\n\n\n\n# Distribution of values of Section\n# Patent Section - first character of context is the section the patent was filed under\ndf['section'] = df.context.str[0]\ndf.section.value_counts()\n\nB    8019\nH    6195\nG    6013\nC    5288\nA    4094\nF    4054\nE    1531\nD    1279\nName: section, dtype: int64\n\n\n\neval_df['section'] = eval_df.context.str[0]\neval_df.section.value_counts()\n\nB    8\nG    7\nF    6\nH    5\nC    4\nA    3\nE    2\nD    1\nName: section, dtype: int64\n\n\n\n# Distribution of values of Score\ndf.score.hist()\n\n&lt;Axes: &gt;\n\n\n\n\n\n\n# Items that were identical\ndf[df.score==1]\n\n\n  \n    \n\n\n\n\n\n\nid\nanchor\ntarget\ncontext\nscore\ninput\nsection\n\n\n\n\n28\n473137168ebf7484\nabatement\nabating\nF24\n1.0\nCONTEXT: F24; TEXT1: abatement; TEXT2: abating\nF\n\n\n158\n621b048d70aa8867\nabsorbent properties\nabsorbent characteristics\nD01\n1.0\nCONTEXT: D01; TEXT1: absorbent properties; TEXT2: absorbent characteristics\nD\n\n\n161\nbc20a1c961cb073a\nabsorbent properties\nabsorption properties\nD01\n1.0\nCONTEXT: D01; TEXT1: absorbent properties; TEXT2: absorption properties\nD\n\n\n311\ne955700dffd68624\nacid absorption\nabsorption of acid\nB08\n1.0\nCONTEXT: B08; TEXT1: acid absorption; TEXT2: absorption of acid\nB\n\n\n315\n3a09aba546aac675\nacid absorption\nacid absorption\nB08\n1.0\nCONTEXT: B08; TEXT1: acid absorption; TEXT2: acid absorption\nB\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n36398\n913141526432f1d6\nwiring trough\nwiring troughs\nF16\n1.0\nCONTEXT: F16; TEXT1: wiring trough; TEXT2: wiring troughs\nF\n\n\n36435\nee0746f2a8ecef97\nwood article\nwood articles\nB05\n1.0\nCONTEXT: B05; TEXT1: wood article; TEXT2: wood articles\nB\n\n\n36440\necaf479135cf0dfd\nwood article\nwooden article\nB05\n1.0\nCONTEXT: B05; TEXT1: wood article; TEXT2: wooden article\nB\n\n\n36464\n8ceaa2b5c2d56250\nwood article\nwood article\nB44\n1.0\nCONTEXT: B44; TEXT1: wood article; TEXT2: wood article\nB\n\n\n36468\n8e1386cbefd7f245\nwood article\nwooden article\nB44\n1.0\nCONTEXT: B44; TEXT1: wood article; TEXT2: wooden article\nB\n\n\n\n\n\n1154 rows × 7 columns"
  },
  {
    "objectID": "posts/fastai-lesson4/fastai-lesson4.html#training-model---setup",
    "href": "posts/fastai-lesson4/fastai-lesson4.html#training-model---setup",
    "title": "FastAI Lesson 4: Natural Language(NLP)",
    "section": "Training Model - Setup",
    "text": "Training Model - Setup\n\n# Constants for Training\n\n# model\nmodel_nm = 'microsoft/deberta-v3-small'\n\n# tokenizer\ntokz = AutoTokenizer.from_pretrained(model_nm)\n\n# learning rate\nlr = 8e-5\n\n# batch size\nbs = 128\n\n# weight decay\nwd = 0.01\n\n# epochs\nepochs = 4\n\n\n# function that determines the split for validation and training data\n# set a default validation dataset split percentage as 25%\n# set a default training dataset split percentage as 75%\ndef get_dataSplit(vp):\n  anchors = df.anchor.unique()\n  print(f\"Unique Anchors: {len(anchors)}\")\n  np.random.seed(42)\n  np.random.shuffle(anchors)\n\n  # specify how much validation set we want\n  val_prop = vp\n  val_sz = int(len(anchors)*val_prop)\n  val_anchors = anchors[:val_sz]\n\n  # Find which rows match the valid anchors and get their indices\n  is_val = np.isin(df.anchor, val_anchors)\n  idxs = np.arange(len(df))\n\n  # Validation Data\n  val_idxs = idxs[ is_val]\n  print(f\"Validation Dataset Length: {len(val_idxs)}\")\n  # Training Data\n  trn_idxs = idxs[~is_val]\n  print(f\"Training Dataset Length: {len(trn_idxs)}\")\n\n  return (val_idxs, trn_idxs)\n\n\n# Function to tokenize training data\ndef get_dds(df, vp=0.25):\n  ds = Dataset.from_pandas(df).rename_column('score', 'label')\n  print(\"Original Input Dataset\")\n  print(ds)\n  eval_ds = Dataset.from_pandas(eval_df)\n  print(\"Original Inference Dataset\")\n  print(eval_ds)\n  inps = \"anchor\",\"target\",\"context\"\n  tok_ds = ds.map(tok_func, batched=True, remove_columns=inps+('input','id','section'))\n  print(\"Tokenized Input Dataset\")\n  print(tok_ds)\n  print(\"Tokenized Inference Dataset\")\n  print(eval_ds)\n  val_idxs, trn_idxs = get_dataSplit(vp)\n  return DatasetDict({\"train\":tok_ds.select(trn_idxs), \"test\": tok_ds.select(val_idxs)})\n\n\n# Function to get the model\ndef get_model():\n  return AutoModelForSequenceClassification.from_pretrained(model_nm, num_labels=1)\n\n\n# Function to create a Trainer\n# - Trainer -&gt; class which combines the data and model together (similar to Learner in FastAI)\ndef get_trainer(dds, model=None):\n\n    # initialize model\n    if model is None: model = get_model()\n\n    # Transformers require the TrainingArguments class to set up the arguments for the trainer\n    args = TrainingArguments('outputs', learning_rate=lr, warmup_ratio=0.1, lr_scheduler_type='cosine', fp16=True,\n        evaluation_strategy=\"epoch\", per_device_train_batch_size=bs, per_device_eval_batch_size=bs*2,\n        num_train_epochs=epochs, weight_decay=wd, report_to='none')\n\n    return Trainer(model, args, train_dataset=dds['train'], eval_dataset=dds['test'],\n                   tokenizer=tokz, compute_metrics=corr_d)"
  },
  {
    "objectID": "posts/fastai-lesson4/fastai-lesson4.html#data-representation",
    "href": "posts/fastai-lesson4/fastai-lesson4.html#data-representation",
    "title": "FastAI Lesson 4: Natural Language(NLP)",
    "section": "Data Representation",
    "text": "Data Representation\n\nCombine Context + Anchor + Target Experiments\n\nMethod 1 - Baseline\n\n# Method 1 - Baseline\ndf['input'] = 'CONTEXT: ' + df.context + '; TEXT1: ' + df.anchor + '; TEXT2: ' + df.target\neval_df['input'] = 'CONTEXT: ' + eval_df.context + '; TEXT1: ' + eval_df.anchor + '; TEXT2: ' + eval_df.target\n\ndds = get_dds(df)\n\n# create and train model\nget_trainer(dds).train()\n\nOriginal Input Dataset\nDataset({\n    features: ['id', 'anchor', 'target', 'context', 'label', 'input', 'section'],\n    num_rows: 36473\n})\nOriginal Inference Dataset\nDataset({\n    features: ['id', 'anchor', 'target', 'context', 'input', 'section'],\n    num_rows: 36\n})\nTokenized Input Dataset\nDataset({\n    features: ['label', 'input_ids', 'token_type_ids', 'attention_mask'],\n    num_rows: 36473\n})\nTokenized Inference Dataset\nDataset({\n    features: ['id', 'anchor', 'target', 'context', 'input', 'section'],\n    num_rows: 36\n})\nUnique Anchors: 733\nValidation Dataset Length: 9116\nTraining Dataset Length: 27357\n\n\n\n\n\n\n    \n      \n      \n      [856/856 03:36, Epoch 4/4]\n    \n    \n\n\n\nEpoch\nTraining Loss\nValidation Loss\nPearson\n\n\n\n\n1\nNo log\n0.026578\n0.796921\n\n\n2\nNo log\n0.023371\n0.818602\n\n\n3\n0.036700\n0.024409\n0.820100\n\n\n4\n0.036700\n0.023927\n0.819576\n\n\n\n\n\n\nTrainOutput(global_step=856, training_loss=0.02694343183642236, metrics={'train_runtime': 217.0934, 'train_samples_per_second': 504.059, 'train_steps_per_second': 3.943, 'total_flos': 723683047099410.0, 'train_loss': 0.02694343183642236, 'epoch': 4.0})\n\n\n\n\nMethod 2 - Separate Token\n\n# Method 2 - Separate Token\nsep = tokz.sep_token\nprint(f\"Separate Token: {sep}\")\ndf['input'] = df.context + sep + df.anchor + sep + df.target\neval_df['input'] = eval_df.context + sep + eval_df.anchor + sep + eval_df.target\n\ndds = get_dds(df)\n\n# create and train model\nget_trainer(dds).train()\n\nSeparate Token: [SEP]\nOriginal Input Dataset\nDataset({\n    features: ['id', 'anchor', 'target', 'context', 'label', 'input', 'section'],\n    num_rows: 36473\n})\nOriginal Inference Dataset\nDataset({\n    features: ['id', 'anchor', 'target', 'context', 'input', 'section'],\n    num_rows: 36\n})\nTokenized Input Dataset\nDataset({\n    features: ['label', 'input_ids', 'token_type_ids', 'attention_mask'],\n    num_rows: 36473\n})\nTokenized Inference Dataset\nDataset({\n    features: ['id', 'anchor', 'target', 'context', 'input', 'section'],\n    num_rows: 36\n})\nUnique Anchors: 733\nValidation Dataset Length: 9116\nTraining Dataset Length: 27357\n\n\n\n\n\n\n    \n      \n      \n      [856/856 03:10, Epoch 4/4]\n    \n    \n\n\n\nEpoch\nTraining Loss\nValidation Loss\nPearson\n\n\n\n\n1\nNo log\n0.026330\n0.792181\n\n\n2\nNo log\n0.025083\n0.809545\n\n\n3\n0.032000\n0.024699\n0.812255\n\n\n4\n0.032000\n0.024956\n0.812318\n\n\n\n\n\n\nTrainOutput(global_step=856, training_loss=0.024021479013924287, metrics={'train_runtime': 190.9271, 'train_samples_per_second': 573.14, 'train_steps_per_second': 4.483, 'total_flos': 468872298987930.0, 'train_loss': 0.024021479013924287, 'epoch': 4.0})\n\n\n\n\nMethod 3 - Change Token Separator\n\n# Method 3 - change the type of separator\nsep = \" [s] \"\ndf['input'] = df.context + sep + df.anchor + sep + df.target\neval_df['input'] = eval_df.context + sep + eval_df.anchor + sep + eval_df.target\n\ndds = get_dds(df)\n\n# create and train model\nget_trainer(dds).train()\n\nOriginal Input Dataset\nDataset({\n    features: ['id', 'anchor', 'target', 'context', 'label', 'input', 'section'],\n    num_rows: 36473\n})\nOriginal Inference Dataset\nDataset({\n    features: ['id', 'anchor', 'target', 'context', 'input', 'section'],\n    num_rows: 36\n})\nTokenized Input Dataset\nDataset({\n    features: ['label', 'input_ids', 'token_type_ids', 'attention_mask'],\n    num_rows: 36473\n})\nTokenized Inference Dataset\nDataset({\n    features: ['id', 'anchor', 'target', 'context', 'input', 'section'],\n    num_rows: 36\n})\nUnique Anchors: 733\nValidation Dataset Length: 9116\nTraining Dataset Length: 27357\n\n\n\n\n\n\n    \n      \n      \n      [856/856 03:17, Epoch 4/4]\n    \n    \n\n\n\nEpoch\nTraining Loss\nValidation Loss\nPearson\n\n\n\n\n1\nNo log\n0.027578\n0.789799\n\n\n2\nNo log\n0.025510\n0.814414\n\n\n3\n0.031600\n0.023443\n0.817882\n\n\n4\n0.031600\n0.024294\n0.817224\n\n\n\n\n\n\nTrainOutput(global_step=856, training_loss=0.023982213479336177, metrics={'train_runtime': 197.7627, 'train_samples_per_second': 553.33, 'train_steps_per_second': 4.328, 'total_flos': 582121520370810.0, 'train_loss': 0.023982213479336177, 'epoch': 4.0})\n\n\n\n\nMethod 4 - Method 3 + Lowercase\n\n# Method 4 - change to all lower case\ndf['input'] = df.context + sep + df.anchor + sep + df.target\ndf['input'] = df.input.str.lower()\neval_df['input'] = eval_df.context + sep + eval_df.anchor + sep + eval_df.target\neval_df['input'] = eval_df.input.str.lower()\n\ndds = get_dds(df)\nget_trainer(dds).train()\n\nOriginal Input Dataset\nDataset({\n    features: ['id', 'anchor', 'target', 'context', 'label', 'input', 'section'],\n    num_rows: 36473\n})\nOriginal Inference Dataset\nDataset({\n    features: ['id', 'anchor', 'target', 'context', 'input', 'section'],\n    num_rows: 36\n})\nTokenized Input Dataset\nDataset({\n    features: ['label', 'input_ids', 'token_type_ids', 'attention_mask'],\n    num_rows: 36473\n})\nTokenized Inference Dataset\nDataset({\n    features: ['id', 'anchor', 'target', 'context', 'input', 'section'],\n    num_rows: 36\n})\nUnique Anchors: 733\nValidation Dataset Length: 9116\nTraining Dataset Length: 27357\n\n\n\n\n\n\n    \n      \n      \n      [856/856 03:13, Epoch 4/4]\n    \n    \n\n\n\nEpoch\nTraining Loss\nValidation Loss\nPearson\n\n\n\n\n1\nNo log\n0.026599\n0.794113\n\n\n2\nNo log\n0.024963\n0.817053\n\n\n3\n0.031500\n0.023034\n0.818722\n\n\n4\n0.031500\n0.024056\n0.818191\n\n\n\n\n\n\nTrainOutput(global_step=856, training_loss=0.02395056954054075, metrics={'train_runtime': 193.6779, 'train_samples_per_second': 565.0, 'train_steps_per_second': 4.42, 'total_flos': 582121520370810.0, 'train_loss': 0.02395056954054075, 'epoch': 4.0})\n\n\n\n\nMethod 5 - Special Tokens\n\n# Method 5 - Special Tokens\ndf['sectok'] = '[' + df.section + ']'\neval_df['sectok'] = '[' + eval_df.section + ']'\nsectoks = list(df.sectok.unique())\ntokz.add_special_tokens({'additional_special_tokens': sectoks})\n\n8\n\n\n\ndf['input'] = df.sectok + sep + df.context + sep + df.anchor.str.lower() + sep + df.target\neval_df['input'] = eval_df.sectok + sep + eval_df.context + sep + eval_df.anchor.str.lower() + sep + eval_df.target\ndds = get_dds(df)\n\nOriginal Input Dataset\nDataset({\n    features: ['id', 'anchor', 'target', 'context', 'label', 'input', 'section', 'sectok'],\n    num_rows: 36473\n})\nOriginal Inference Dataset\nDataset({\n    features: ['id', 'anchor', 'target', 'context', 'input', 'section', 'sectok'],\n    num_rows: 36\n})\nTokenized Input Dataset\nDataset({\n    features: ['label', 'sectok', 'input_ids', 'token_type_ids', 'attention_mask'],\n    num_rows: 36473\n})\nTokenized Inference Dataset\nDataset({\n    features: ['id', 'anchor', 'target', 'context', 'input', 'section', 'sectok'],\n    num_rows: 36\n})\nUnique Anchors: 733\nValidation Dataset Length: 9116\nTraining Dataset Length: 27357\n\n\n\n\n\n\n# resize embedding matrix in model\nmodel = get_model()\nmodel.resize_token_embeddings(len(tokz))\n\nEmbedding(128009, 768)\n\n\n\n# train the model\ntrainer = get_trainer(dds, model=model)\ntrainer.train()\n\n\n    \n      \n      \n      [856/856 03:33, Epoch 4/4]\n    \n    \n\n\n\nEpoch\nTraining Loss\nValidation Loss\nPearson\n\n\n\n\n1\nNo log\n0.029230\n0.803785\n\n\n2\nNo log\n0.024406\n0.821807\n\n\n3\n0.031400\n0.023469\n0.820734\n\n\n4\n0.031400\n0.024289\n0.820907\n\n\n\n\n\n\nTrainOutput(global_step=856, training_loss=0.023841174406425976, metrics={'train_runtime': 214.1222, 'train_samples_per_second': 511.054, 'train_steps_per_second': 3.998, 'total_flos': 695370741753690.0, 'train_loss': 0.023841174406425976, 'epoch': 4.0})\n\n\n\n# Train on the entire training dataset before doing final tests with inference dataset\ndef final_get_trainer(dds, model=None):\n    # initialize model\n    if model is None:\n        model = get_model()\n\n    # Transformers require the TrainingArguments class to set up the arguments for the trainer\n    args = TrainingArguments('outputs', learning_rate=lr, warmup_ratio=0.1, lr_scheduler_type='cosine', fp16=True,\n        evaluation_strategy=\"epoch\", per_device_train_batch_size=bs, per_device_eval_batch_size=bs*2,\n        num_train_epochs=epochs, weight_decay=wd, report_to='none')\n\n    return Trainer(model, args, train_dataset=dds['train'], eval_dataset=dds['test'],\n                   tokenizer=tokz, compute_metrics=corr_d)\n\n\ndds = get_dds(df, 0.01)\ntrainer = final_get_trainer(dds, model=model)\ntrainer.train()\n\nOriginal Input Dataset\nDataset({\n    features: ['id', 'anchor', 'target', 'context', 'label', 'input', 'section', 'sectok'],\n    num_rows: 36473\n})\nOriginal Inference Dataset\nDataset({\n    features: ['id', 'anchor', 'target', 'context', 'input', 'section', 'sectok'],\n    num_rows: 36\n})\nTokenized Input Dataset\nDataset({\n    features: ['label', 'sectok', 'input_ids', 'token_type_ids', 'attention_mask'],\n    num_rows: 36473\n})\nTokenized Inference Dataset\nDataset({\n    features: ['id', 'anchor', 'target', 'context', 'input', 'section', 'sectok'],\n    num_rows: 36\n})\nUnique Anchors: 733\nValidation Dataset Length: 347\nTraining Dataset Length: 36126\n\n\n\n\n\n\n    \n      \n      \n      [1132/1132 04:31, Epoch 4/4]\n    \n    \n\n\n\nEpoch\nTraining Loss\nValidation Loss\nPearson\n\n\n\n\n1\nNo log\n0.021583\n0.836893\n\n\n2\n0.015400\n0.019293\n0.863323\n\n\n3\n0.015400\n0.018774\n0.860989\n\n\n4\n0.009500\n0.018202\n0.864261\n\n\n\n\n\n\nTrainOutput(global_step=1132, training_loss=0.011905024835161943, metrics={'train_runtime': 272.1518, 'train_samples_per_second': 530.968, 'train_steps_per_second': 4.159, 'total_flos': 918764683369440.0, 'train_loss': 0.011905024835161943, 'epoch': 4.0})"
  },
  {
    "objectID": "posts/fastai-lesson4/fastai-lesson4.html#evaluate-performance-of-model-on-inference-data",
    "href": "posts/fastai-lesson4/fastai-lesson4.html#evaluate-performance-of-model-on-inference-data",
    "title": "FastAI Lesson 4: Natural Language(NLP)",
    "section": "Evaluate Performance of Model on Inference Data",
    "text": "Evaluate Performance of Model on Inference Data\n\n# Test model on the evaluation data\npreds = trainer.predict(eval_ds).predictions.astype(float)\npreds = np.clip(preds, 0, 1)\npreds\n\n\n\n\narray([[0.57763672],\n       [0.57177734],\n       [0.47412109],\n       [0.34521484],\n       [0.        ],\n       [0.48852539],\n       [0.51611328],\n       [0.        ],\n       [0.3503418 ],\n       [0.64208984],\n       [0.33959961],\n       [0.28930664],\n       [0.65234375],\n       [0.71289062],\n       [0.56591797],\n       [0.37866211],\n       [0.30786133],\n       [0.05584717],\n       [0.61181641],\n       [0.4050293 ],\n       [0.58447266],\n       [0.26806641],\n       [0.28491211],\n       [0.29077148],\n       [0.53857422],\n       [0.        ],\n       [0.        ],\n       [0.        ],\n       [0.        ],\n       [0.59912109],\n       [0.35766602],\n       [0.07025146],\n       [0.68945312],\n       [0.48779297],\n       [0.41601562],\n       [0.29858398]])"
  },
  {
    "objectID": "posts/fastai-lesson4/fastai-lesson4.html#submission-for-kaggle-competition",
    "href": "posts/fastai-lesson4/fastai-lesson4.html#submission-for-kaggle-competition",
    "title": "FastAI Lesson 4: Natural Language(NLP)",
    "section": "Submission for Kaggle Competition",
    "text": "Submission for Kaggle Competition\n\n# Submit for kaggle competition\nimport datasets\n\nsubmission = datasets.Dataset.from_dict({\n    'id': eval_ds['id'],\n    'score': preds\n})\n\nsubmission.to_csv('submission.csv', index=False)\n\n\n\n\n1039"
  },
  {
    "objectID": "posts/fastai-lesson4/fastai-lesson4.html#validation-set-and-modeling-experiments",
    "href": "posts/fastai-lesson4/fastai-lesson4.html#validation-set-and-modeling-experiments",
    "title": "FastAI Lesson 4: Natural Language(NLP)",
    "section": "Validation Set and Modeling Experiments",
    "text": "Validation Set and Modeling Experiments\n\n# Function to model\ndef f(x):\n  return -3*x**2 + 2*x + 20\n\n# Function for visualization Data\ndef plot_function(f, min=-2.1, max=2.1, color='r'):\n    x = np.linspace(min,max, 100)[:,None]\n    plt.plot(x, f(x), color)\n\n\nprint(f\"Function we're trying to fit\")\nplot_function(f)\n\nFunction we're trying to fit\n\n\n\n\n\n\n# Functions for creating noisy data\nfrom numpy.random import normal,seed,uniform\nnp.random.seed(42)\n\ndef noise(x, scale):\n   return normal(scale=scale, size=x.shape)\n\ndef add_noise(x, mult, add):\n  return x * (1+noise(x,mult)) + noise(x,add)\n\n\n# Simulate Noisy Data\nx = np.linspace(-2, 2, num=20)[:,None]\ny = add_noise(f(x), 0.2, 1.3)\nplt.scatter(x,y);\n\n\n\n\n\n# Function for trying to find polynomial function that fits data\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import make_pipeline\n\ndef plot_poly(degree):\n    model = make_pipeline(PolynomialFeatures(degree), LinearRegression())\n    model.fit(x, y)\n    plt.scatter(x,y)\n    plot_function(model.predict)\n\n\n# Underfit\nprint(f\"Polynomial of Degree 1: Underfit\")\nplot_poly(1)\n\nPolynomial of Degree 1: Underfit\n\n\n\n\n\n\n# Overfit\nprint(f\"Polynomial of Degree 10: Overfit\")\nplot_poly(10)\n\nPolynomial of Degree 10: Overfit\n\n\n\n\n\n\n# Approximation of Close fit\nprint(f\"Polynmomial of Degree 2: A close approximation of exact fit\")\nplot_poly(2)\n\n# the original true function (one we're trying to fit)\nplot_function(f, color='b')\n\nPolynmomial of Degree 2: A close approximation of exact fit"
  },
  {
    "objectID": "posts/fastai-lesson4/fastai-lesson4.html#metric-experiments-correlation",
    "href": "posts/fastai-lesson4/fastai-lesson4.html#metric-experiments-correlation",
    "title": "FastAI Lesson 4: Natural Language(NLP)",
    "section": "Metric Experiments: Correlation",
    "text": "Metric Experiments: Correlation\n\nfrom sklearn.datasets import fetch_california_housing\nhousing = fetch_california_housing(as_frame=True)\nhousing = housing['data'].join(housing['target']).sample(1000, random_state=52)\nhousing.head()\n\n\n  \n    \n\n\n\n\n\n\nMedInc\nHouseAge\nAveRooms\nAveBedrms\nPopulation\nAveOccup\nLatitude\nLongitude\nMedHouseVal\n\n\n\n\n7506\n3.0550\n37.0\n5.152778\n1.048611\n729.0\n5.062500\n33.92\n-118.28\n1.054\n\n\n4720\n3.0862\n35.0\n4.697897\n1.055449\n1159.0\n2.216061\n34.05\n-118.37\n3.453\n\n\n12888\n2.5556\n24.0\n4.864905\n1.129222\n1631.0\n2.395007\n38.66\n-121.35\n1.057\n\n\n13344\n3.0057\n32.0\n4.212687\n0.936567\n1378.0\n5.141791\n34.05\n-117.64\n0.969\n\n\n7173\n1.9083\n42.0\n3.888554\n1.039157\n1535.0\n4.623494\n34.05\n-118.19\n1.192\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\n# correlation coefficients for every combination of columns\nnp.set_printoptions(precision=2, suppress=True)\nnp.corrcoef(housing, rowvar=False)\n\narray([[ 1.  , -0.12,  0.43, -0.08,  0.01, -0.07, -0.12,  0.04,  0.68],\n       [-0.12,  1.  , -0.17, -0.06, -0.31,  0.  ,  0.03, -0.13,  0.12],\n       [ 0.43, -0.17,  1.  ,  0.76, -0.09, -0.07,  0.12, -0.03,  0.21],\n       [-0.08, -0.06,  0.76,  1.  , -0.08, -0.07,  0.09,  0.  , -0.04],\n       [ 0.01, -0.31, -0.09, -0.08,  1.  ,  0.16, -0.15,  0.13,  0.  ],\n       [-0.07,  0.  , -0.07, -0.07,  0.16,  1.  , -0.16,  0.17, -0.27],\n       [-0.12,  0.03,  0.12,  0.09, -0.15, -0.16,  1.  , -0.93, -0.16],\n       [ 0.04, -0.13, -0.03,  0.  ,  0.13,  0.17, -0.93,  1.  , -0.03],\n       [ 0.68,  0.12,  0.21, -0.04,  0.  , -0.27, -0.16, -0.03,  1.  ]])\n\n\n\n# Correlation for two variables\nnp.corrcoef(housing.MedInc, housing.MedHouseVal)\n\narray([[1.  , 0.68],\n       [0.68, 1.  ]])\n\n\n\n# function for returning single coefficient correlation\n\ndef corr(x,y):\n  return np.corrcoef(x,y)[0][1]\n\n# test function\ncorr(housing.MedInc, housing.MedHouseVal)\n\n# function for visualizing correlation between two variables\ndef show_corr(df, a, b):\n    x,y = df[a],df[b]\n    plt.scatter(x,y, alpha=0.5, s=4)\n    plt.title(f'{a} vs {b}; r: {corr(x, y):.2f}')\n\n\nshow_corr(housing, 'MedInc', 'MedHouseVal')\n\n\n\n\n\nshow_corr(housing, 'MedInc', 'AveRooms')\n\n\n\n\n\nsubset = housing[housing.AveRooms &lt; 15]\nshow_corr(subset, 'MedInc', 'AveRooms')"
  },
  {
    "objectID": "posts/fastai-lesson4/fastai-lesson4.html#resources",
    "href": "posts/fastai-lesson4/fastai-lesson4.html#resources",
    "title": "FastAI Lesson 4: Natural Language(NLP)",
    "section": "Resources",
    "text": "Resources\n\nFastAI Lesson 4\nFastAI Chapter 10\nUS Patent Phrase to Phrase Matching\nGetting Started with NLP for Absolute Beginners\nIterate Like a Grandmaster\nHow (and why) to create a good validation dataset\nThe problem with metrics is a big problem for AI\nUtah CS 5340/6340 - Natural Language Processing\nHugging Face Transformers docs\nJeremy Howard FastAI Live Coding\nfast.ai docs"
  },
  {
    "objectID": "posts/fastai-lesson3/fastai-lesson3.html",
    "href": "posts/fastai-lesson3/fastai-lesson3.html",
    "title": "FastAI Lesson 3: Neural Network Foundations",
    "section": "",
    "text": "Finally back after a long hiatus. I took a break from posting and FASTAI to try Advent of Code and get back to interview prep but I’m back. Normally I only write a single summary for a blog post but this time I’m writing two summaries because Chapter 4 of the book goes deeper into the foundations of neural networks than the lesson video."
  },
  {
    "objectID": "posts/fastai-lesson3/fastai-lesson3.html#announcements",
    "href": "posts/fastai-lesson3/fastai-lesson3.html#announcements",
    "title": "FastAI Lesson 3: Neural Network Foundations",
    "section": "",
    "text": "Finally back after a long hiatus. I took a break from posting and FASTAI to try Advent of Code and get back to interview prep but I’m back. Normally I only write a single summary for a blog post but this time I’m writing two summaries because Chapter 4 of the book goes deeper into the foundations of neural networks than the lesson video."
  },
  {
    "objectID": "posts/fastai-lesson3/fastai-lesson3.html#acknowledgements",
    "href": "posts/fastai-lesson3/fastai-lesson3.html#acknowledgements",
    "title": "FastAI Lesson 3: Neural Network Foundations",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nAll of this code was written by Jeremy Howard and the authors of the FastAI book. My modification to their code was adding print statements and comments to understand what each line of code was doing."
  },
  {
    "objectID": "posts/fastai-lesson3/fastai-lesson3.html#summary",
    "href": "posts/fastai-lesson3/fastai-lesson3.html#summary",
    "title": "FastAI Lesson 3: Neural Network Foundations",
    "section": "Summary",
    "text": "Summary\n\nSummary: FastAI Lesson 3 Video\nIn this lesson, Jeremy gives a high level overview of pytorch, gradient descent and the evolution of models. Most of the lesson focuses on approximating a quadratic equation and building the intuition towards how a neural network works. Towards the end of the video, Jeremy introduces the Titanic dataset and how to do some basic modeling with excel.\n\n\nSummary: FastAI Chapter 4\nThe MNIST dataset was used as the case study to understand how pytorch and the fastai library work under the hood. I found this chapter to be a mind bender because I struggled to wrap my head around the concept of a tensor and tensor operations. I struggled with the meaning of rank and dimension of tensors because they do not have the same meaning as rank and dimension from linear algebra and calculus. By the end of this chapter, I understood the following concepts:\n\nwhat a tensor represents and performing operations on a tensor\nhow to set up a classifier using pytorch\nhow a training loop, optimizer, batch work in pytorch and fastai\nthe operations and functions that fastai provides a wrapper on top of\n\nI do agree with Jeremy that this book chapter might scare people away because of the technical and mathematical jargon. It’s definitely worth spending a few days to go over because it makes it much easier to understand how things work in the subsequent lessons."
  },
  {
    "objectID": "posts/fastai-lesson3/fastai-lesson3.html#jeremy-howards-advice",
    "href": "posts/fastai-lesson3/fastai-lesson3.html#jeremy-howards-advice",
    "title": "FastAI Lesson 3: Neural Network Foundations",
    "section": "Jeremy Howard’s Advice",
    "text": "Jeremy Howard’s Advice\n\nModel: Mathematical function consisting of a Matrix Multiply operation + nonlinearity (RELU, Sigmoid etc)\nThings to thing about when picking a class of model for a problem we’re trying to solve:\n\nHow fast is the model\nHow much memory does it consume\nHow accurate is it\n\nModels fit functions to data and try to recognize patterns in data that we give it"
  },
  {
    "objectID": "posts/fastai-lesson3/fastai-lesson3.html#terminology",
    "href": "posts/fastai-lesson3/fastai-lesson3.html#terminology",
    "title": "FastAI Lesson 3: Neural Network Foundations",
    "section": "Terminology",
    "text": "Terminology\nActivations(Neural Network) - Numbers that are calculated (both by linear and non-linear layers)\nParameters(Neural Network) - Numbers that are randomly initialized, and optimized (that is, the numbers that define the model)\nAxis(Numpy), Dimensions(PyTorch Tensors) - For a matrix, the rows and columns define the axis\nTensor Rank - The number of dimensions in a tensor\nRank Zero Tensor - Scalar\nRank One Tensor - Vector\nRank Two Tensor - Matrix\nNonlinearity (Activation Function) - one type of layer in a neural network. Typically a neural network alternates between a linear layer and non-linear layer. Occasionally people refer to a single layer = linear layer + nonlinearity\nRelu - Function that returns 0 for negative numbers and doesn’t change positive numbers\nMini-Batch - A small group of inputs and labels gathered together in two arrays. A gradient desccent step is updated on this batch rather than a whole epoch\nForward Pass - Applying the model to some input and computing the predictions\nLoss - A value that represents how well (or bad) the model is doing Gradient - The derivative(slope) of the loss with respect to some parameter of the model\nBackward Pass - Computing the gradients of the loss with respect to all model parameters\nGradient Descent - Taking a step in the directions opposite to the gradients to make the model parameters a little bit better\nLearning Rate - The size of the step we take when applying SGD to update the parameters of the model. Usually a very tiny model"
  },
  {
    "objectID": "posts/fastai-lesson3/fastai-lesson3.html#mnist-dataset",
    "href": "posts/fastai-lesson3/fastai-lesson3.html#mnist-dataset",
    "title": "FastAI Lesson 3: Neural Network Foundations",
    "section": "MNIST Dataset",
    "text": "MNIST Dataset\nMNIST is a dataset containing handwritten digits collected by NIST (National Institute of Standards and Technology) and turned into dataset by Yann Lecun and his colleagues. More information about the dataset can be found in Chapter 4 and MNIST Kaggle Competition.\nThe MNIST dataset follows traditional machine learning dataset layouts: Training Data and Validation Data each containing images associated with a particular digit between 0-9."
  },
  {
    "objectID": "posts/fastai-lesson3/fastai-lesson3.html#load-data-and-libraries",
    "href": "posts/fastai-lesson3/fastai-lesson3.html#load-data-and-libraries",
    "title": "FastAI Lesson 3: Neural Network Foundations",
    "section": "Load Data and Libraries",
    "text": "Load Data and Libraries\n\n# load libraries and imports\n!pip install -Uqq fastai duckduckgo_search\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport torch\nfrom PIL import Image\nfrom functools import partial\n\n# import fastai libraries\nfrom fastai.vision.all import *\n\n# update grayscale colormap for matplotlib\nmatplotlib.rc('image', cmap='Greys')\n\n\n# LOAD MNIST DATA\npath = untar_data(URLs.MNIST_SAMPLE)\nPath.BASE_PATH = path\nprint(f\"Contents of MNIST DATA: {path.ls()}\")\n\n# MNIST Training Data\nprint(f\"MNIST Training Data Directory Contents: {(path/'train').ls()}\")\n\n# MNIST Training Data for 3s and 7s in sorted order\nthrees = (path/'train'/'3').ls().sorted()\nsevens = (path/'train'/'7').ls().sorted()\n# print(f\"Sorted Training Data for 3: {threes}\")\n\n# Training Data Example\nim3_path = threes[1]\nim3 = Image.open(im3_path)\nprint(f\"Test Image of 3 from 3 training data set\")\nim3\n\nContents of MNIST DATA: [Path('valid'), Path('labels.csv'), Path('train')]\nMNIST Training Data Directory Contents: [Path('train/7'), Path('train/3')]\nTest Image of 3 from 3 training data set"
  },
  {
    "objectID": "posts/fastai-lesson3/fastai-lesson3.html#data-processing",
    "href": "posts/fastai-lesson3/fastai-lesson3.html#data-processing",
    "title": "FastAI Lesson 3: Neural Network Foundations",
    "section": "Data Processing",
    "text": "Data Processing\n\nRepresenting Images as Numbers\nImages are represented on computers as an array of pixels where each index contains a list of 3 numbers between 0-255 corresponding to a particular color according to RGB. Assembling all of these colors together we get an image.\nThe MNIST images are represented differently: each index in the image array contains a number between 0-255 where 0 represents white and 255 black. All other values between 0-255 represent a different shade of gray. A unique digit image in the MNIST data is then defined by the black and gray pixels that together outline and define the digit. The size of an image in the MNIST data is 28 x 28 which is 784 pixels in the Image Array.\nIn the following examples, [4:10, 4:10] means the following: request rows from index 4 (included) to 10(not included) and the same for the columns. Numpy and Pytorch index from top to bottom and left to right.\nIn the image slice below, we select a part of the digit with just the top part and then color code the slice based on the values in the slice with their mapping in the gray scale (0-255) where 0 represents white and 255 black.\n\n# MNIST image dimensions are 28 x 28 = 784 pixel array\nprint(f\"im3 represented as an array of numbers using numpy array\")\narray(im3)\n\nim3 represented as an array of numbers using numpy array\n\n\narray([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0],\n       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0],\n       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0],\n       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0],\n       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0],\n       [  0,   0,   0,   0,   0,   0,   0,   0,   0,  29, 150, 195, 254,\n        255, 254, 176, 193, 150,  96,   0,   0,   0,   0,   0,   0,   0,\n          0,   0],\n       [  0,   0,   0,   0,   0,   0,   0,  48, 166, 224, 253, 253, 234,\n        196, 253, 253, 253, 253, 233,   0,   0,   0,   0,   0,   0,   0,\n          0,   0],\n       [  0,   0,   0,   0,   0,  93, 244, 249, 253, 187,  46,  10,   8,\n          4,  10, 194, 253, 253, 233,   0,   0,   0,   0,   0,   0,   0,\n          0,   0],\n       [  0,   0,   0,   0,   0, 107, 253, 253, 230,  48,   0,   0,   0,\n          0,   0, 192, 253, 253, 156,   0,   0,   0,   0,   0,   0,   0,\n          0,   0],\n       [  0,   0,   0,   0,   0,   3,  20,  20,  15,   0,   0,   0,   0,\n          0,  43, 224, 253, 245,  74,   0,   0,   0,   0,   0,   0,   0,\n          0,   0],\n       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0, 249, 253, 245, 126,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0],\n       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  14, 101,\n        223, 253, 248, 124,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0],\n       [  0,   0,   0,   0,   0,   0,   0,   0,   0,  11, 166, 239, 253,\n        253, 253, 187,  30,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0],\n       [  0,   0,   0,   0,   0,   0,   0,   0,   0,  16, 248, 250, 253,\n        253, 253, 253, 232, 213, 111,   2,   0,   0,   0,   0,   0,   0,\n          0,   0],\n       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  43,  98,\n         98, 208, 253, 253, 253, 253, 187,  22,   0,   0,   0,   0,   0,\n          0,   0],\n       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   9,  51, 119, 253, 253, 253,  76,   0,   0,   0,   0,   0,\n          0,   0],\n       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0,   0,   1, 183, 253, 253, 139,   0,   0,   0,   0,   0,\n          0,   0],\n       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0,   0,   0, 182, 253, 253, 104,   0,   0,   0,   0,   0,\n          0,   0],\n       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0,   0,  85, 249, 253, 253,  36,   0,   0,   0,   0,   0,\n          0,   0],\n       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0,  60, 214, 253, 253, 173,  11,   0,   0,   0,   0,   0,\n          0,   0],\n       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,  98, 247, 253, 253, 226,   9,   0,   0,   0,   0,   0,   0,\n          0,   0],\n       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  42,\n        150, 252, 253, 253, 233,  53,   0,   0,   0,   0,   0,   0,   0,\n          0,   0],\n       [  0,   0,   0,   0,   0,   0,  42, 115,  42,  60, 115, 159, 240,\n        253, 253, 250, 175,  25,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0],\n       [  0,   0,   0,   0,   0,   0, 187, 253, 253, 253, 253, 253, 253,\n        253, 197,  86,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0],\n       [  0,   0,   0,   0,   0,   0, 103, 253, 253, 253, 253, 253, 232,\n         67,   1,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0],\n       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0],\n       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0],\n       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0]], dtype=uint8)\n\n\n\n# Slice of im3\n# [4:10, 4:10] - get rows and columns starting from 4(included) to 10 (excluded)\n# Numpy Array representation\narray(im3)[4:10,4:10]\n\narray([[  0,   0,   0,   0,   0,   0],\n       [  0,   0,   0,   0,   0,  29],\n       [  0,   0,   0,  48, 166, 224],\n       [  0,  93, 244, 249, 253, 187],\n       [  0, 107, 253, 253, 230,  48],\n       [  0,   3,  20,  20,  15,   0]], dtype=uint8)\n\n\n\n# MNIST image dimensions are 28 x 28 = 784 pixel array\nprint(f\"im3 represented as an array of numbers using tensors\")\ntensor(im3)\n\nim3 represented as an array of numbers using tensors\n\n\ntensor([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n        [  0,   0,   0,   0,   0,   0,   0,   0,   0,  29, 150, 195, 254, 255,\n         254, 176, 193, 150,  96,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n        [  0,   0,   0,   0,   0,   0,   0,  48, 166, 224, 253, 253, 234, 196,\n         253, 253, 253, 253, 233,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n        [  0,   0,   0,   0,   0,  93, 244, 249, 253, 187,  46,  10,   8,   4,\n          10, 194, 253, 253, 233,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n        [  0,   0,   0,   0,   0, 107, 253, 253, 230,  48,   0,   0,   0,   0,\n           0, 192, 253, 253, 156,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n        [  0,   0,   0,   0,   0,   3,  20,  20,  15,   0,   0,   0,   0,   0,\n          43, 224, 253, 245,  74,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         249, 253, 245, 126,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  14, 101, 223,\n         253, 248, 124,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n        [  0,   0,   0,   0,   0,   0,   0,   0,   0,  11, 166, 239, 253, 253,\n         253, 187,  30,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n        [  0,   0,   0,   0,   0,   0,   0,   0,   0,  16, 248, 250, 253, 253,\n         253, 253, 232, 213, 111,   2,   0,   0,   0,   0,   0,   0,   0,   0],\n        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  43,  98,  98,\n         208, 253, 253, 253, 253, 187,  22,   0,   0,   0,   0,   0,   0,   0],\n        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n           9,  51, 119, 253, 253, 253,  76,   0,   0,   0,   0,   0,   0,   0],\n        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n           0,   0,   1, 183, 253, 253, 139,   0,   0,   0,   0,   0,   0,   0],\n        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n           0,   0,   0, 182, 253, 253, 104,   0,   0,   0,   0,   0,   0,   0],\n        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n           0,   0,  85, 249, 253, 253,  36,   0,   0,   0,   0,   0,   0,   0],\n        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n           0,  60, 214, 253, 253, 173,  11,   0,   0,   0,   0,   0,   0,   0],\n        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          98, 247, 253, 253, 226,   9,   0,   0,   0,   0,   0,   0,   0,   0],\n        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  42, 150,\n         252, 253, 253, 233,  53,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n        [  0,   0,   0,   0,   0,   0,  42, 115,  42,  60, 115, 159, 240, 253,\n         253, 250, 175,  25,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n        [  0,   0,   0,   0,   0,   0, 187, 253, 253, 253, 253, 253, 253, 253,\n         197,  86,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n        [  0,   0,   0,   0,   0,   0, 103, 253, 253, 253, 253, 253, 232,  67,\n           1,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0]],\n       dtype=torch.uint8)\n\n\n\n# Slice of im3\n# [4:10, 4:10] - get rows and columns starting from 4(included) to 10 (excluded)\n# Tensor representation\ntensor(im3)[4:10, 4:10]\n\ntensor([[  0,   0,   0,   0,   0,   0],\n        [  0,   0,   0,   0,   0,  29],\n        [  0,   0,   0,  48, 166, 224],\n        [  0,  93, 244, 249, 253, 187],\n        [  0, 107, 253, 253, 230,  48],\n        [  0,   3,  20,  20,  15,   0]], dtype=torch.uint8)\n\n\n\n# slice data to obtain the top part of the number and color code data to show digit outline\n# slice data rows: 4(included)-15(excluded)\n# slice data columns: 4(included)-22(excluded)\nim3_t = tensor(im3)\ndf = pd.DataFrame(im3_t[4:15,4:22])\ndf.style.set_properties(**{'font-size':'6pt'}).background_gradient('Greys')\n\n\n\n\n\n\n \n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n\n\n\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n0\n0\n0\n0\n0\n29\n150\n195\n254\n255\n254\n176\n193\n150\n96\n0\n0\n0\n\n\n2\n0\n0\n0\n48\n166\n224\n253\n253\n234\n196\n253\n253\n253\n253\n233\n0\n0\n0\n\n\n3\n0\n93\n244\n249\n253\n187\n46\n10\n8\n4\n10\n194\n253\n253\n233\n0\n0\n0\n\n\n4\n0\n107\n253\n253\n230\n48\n0\n0\n0\n0\n0\n192\n253\n253\n156\n0\n0\n0\n\n\n5\n0\n3\n20\n20\n15\n0\n0\n0\n0\n0\n43\n224\n253\n245\n74\n0\n0\n0\n\n\n6\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n249\n253\n245\n126\n0\n0\n0\n0\n\n\n7\n0\n0\n0\n0\n0\n0\n0\n14\n101\n223\n253\n248\n124\n0\n0\n0\n0\n0\n\n\n8\n0\n0\n0\n0\n0\n11\n166\n239\n253\n253\n253\n187\n30\n0\n0\n0\n0\n0\n\n\n9\n0\n0\n0\n0\n0\n16\n248\n250\n253\n253\n253\n253\n232\n213\n111\n2\n0\n0\n\n\n10\n0\n0\n0\n0\n0\n0\n0\n43\n98\n98\n208\n253\n253\n253\n253\n187\n22\n0"
  },
  {
    "objectID": "posts/fastai-lesson3/fastai-lesson3.html#baseline-model-pixel-similarity",
    "href": "posts/fastai-lesson3/fastai-lesson3.html#baseline-model-pixel-similarity",
    "title": "FastAI Lesson 3: Neural Network Foundations",
    "section": "Baseline Model: Pixel Similarity",
    "text": "Baseline Model: Pixel Similarity\nThe problem we’re trying to solve is the following: How do we write a computer program to be able to distinguish between images of handwritten 3 and 7 digits.\nThe first approach we try is Pixel Similarity. The FASTAI book authors define this as the following:\n\nTake the average pixel value for every pixel of the 3 images and do the same for the 7 images. These averages will produce a baseline image 3 and image 7.\nGo through every image in the 3 and 7 images and compare them to the baseline images to see which digit they are most similar to\n\n\n# Create a list of tensors for each image in 3 and 7 directories\nthree_tensors = [tensor(Image.open(o)) for o in threes]\nseven_tensors = [tensor(Image.open(o)) for o in sevens]\n\nprint(f\"Number of images in threes: {len(threes)}\")\nprint(f\"Number of images in three tensors: {len(three_tensors)}\")\nprint(f\"Number of images in sevens: {len(sevens)}\")\nprint(f\"Number of images in seven tensors: {len(seven_tensors)}\")\n\n# verify images\nshow_image(three_tensors[1])\nshow_image(seven_tensors[1])\n\nNumber of images in threes: 6131\nNumber of images in three tensors: 6131\nNumber of images in sevens: 6265\nNumber of images in seven tensors: 6265\n\n\n&lt;Axes: &gt;\n\n\n\n\n\n\n\n\nThe way I understand tensors is that they are data structures for storing information. The way I understand the stack operation is storing all the images into a pile of images that we then then can average all the pixel values for each pixel index in the image.\n\n# Compute the average intensity of each pixel across all images for 3 and 7\nstacked_sevens = torch.stack(seven_tensors).float() / 255\nstacked_threes = torch.stack(three_tensors).float() / 255\n\nprint(f\"stacked_sevens shape: {stacked_sevens.shape}\")\nprint(f\"stacked_sevens tensor rank: {len(stacked_sevens.shape)}\")\nprint(f\"stacked_threes shape: {stacked_threes.shape}\")\nprint(f\"stacked_threes tensor rank: {len(stacked_threes.shape)}\")\n\nstacked_sevens shape: torch.Size([6265, 28, 28])\nstacked_sevens tensor rank: 3\nstacked_threes shape: torch.Size([6131, 28, 28])\nstacked_threes tensor rank: 3\n\n\nIn this step, we take the list of tensor images and condense them down into a new image where each pixel index in this new image is the average of all the values at a particular index.\n\n # Average of all image tensors by taking mean along the 0 dimension (collapse all the rows into a single row) of stacked 3 rank tensors\nmean3 = stacked_threes.mean(0)\nmean7 = stacked_sevens.mean(0)\nprint(f\"mean3 shape: {mean3.shape}\")\nprint(f\"mean3 tensor rank: {len(mean3.shape)}\")\nprint(f\"mean7 shape: {mean7.shape}\")\nprint(f\"mean7 tensor rank: {len(mean7.shape)}\")\nshow_image(mean3)\nshow_image(mean7)\n\nmean3 shape: torch.Size([28, 28])\nmean3 tensor rank: 2\nmean7 shape: torch.Size([28, 28])\nmean7 tensor rank: 2\n\n\n&lt;Axes: &gt;"
  },
  {
    "objectID": "posts/fastai-lesson3/fastai-lesson3.html#measuring-distance",
    "href": "posts/fastai-lesson3/fastai-lesson3.html#measuring-distance",
    "title": "FastAI Lesson 3: Neural Network Foundations",
    "section": "Measuring Distance",
    "text": "Measuring Distance\nTo compare the baseline image with a randomly chosen image from one of the datasets we need to measure the difference between pixels such that we have a standardized form so that the differences accurately reflect what pixels are dark and light when comparing the two images.\n\n# Mean Absolute Difference (L1 Norm)\na_3 = stacked_threes[1]\na_7 = stacked_sevens[1]\n\ndist_3_abs = (a_3 - mean3).abs().mean()\ndist_7_abs = (a_7 - mean7).abs().mean()\n\nprint(f\"Mean Absolute Difference between 3 image and ideal 3 image: {dist_3_abs}\")\nprint(f\"Mean Absolute Difference between 7 image and ideal 7 image: {dist_7_abs}\")\n\n# see how close 3 is to ideal 7\ndist_test_abs = (a_3 - mean7).abs().mean()\nprint(f\"Mean Absolute Difference between 3 image and ideal 7 image: {dist_test_abs}\")\n\nMean Absolute Difference between 3 image and ideal 3 image: 0.11143654584884644\nMean Absolute Difference between 7 image and ideal 7 image: 0.13037648797035217\nMean Absolute Difference between 3 image and ideal 7 image: 0.15861910581588745\n\n\n\n# Root Mean Squared Error (L2 Norm)\na_3 = stacked_threes[1]\na_7 = stacked_sevens[1]\n\ndist_3_sqr = ((a_3 - mean3)**2).mean().sqrt()\ndist_7_sqr = ((a_7 - mean7)**2).mean().sqrt()\ndist_test_sqr = ((a_3 - mean7)**2).mean().sqrt()\nprint(f\"Root Mean Squared Difference between 3 image and ideal 3 image: {dist_3_sqr}\")\nprint(f\"Root Mean Squared Difference between 7 image and ideal 7 image: {dist_7_sqr}\")\n\n# see how close 3 is to ideal 7\nprint(f\"Root Mean Squared Difference between 3 image and ideal 7 image: {dist_test_sqr}\")\n\nRoot Mean Squared Difference between 3 image and ideal 3 image: 0.20208320021629333\nRoot Mean Squared Difference between 7 image and ideal 7 image: 0.2584923207759857\nRoot Mean Squared Difference between 3 image and ideal 7 image: 0.30210891366004944\n\n\n\n# Pytorch Mean Squared Error and Mean Absolute Value Loss\nprint(f\"Pytorch Mean Absolute Value Loss between 3 image and ideal 7 image: {F.l1_loss(a_3.float(), mean7)}\")\nprint(f\"Pytorch Mean Squared Error Loss between 3 image and ideal 7 image: {F.mse_loss(a_3, mean7).sqrt()}\")\n\nPytorch Mean Absolute Value Loss between 3 image and ideal 7 image: 0.15861910581588745\nPytorch Mean Squared Error Loss between 3 image and ideal 7 image: 0.30210891366004944"
  },
  {
    "objectID": "posts/fastai-lesson3/fastai-lesson3.html#pytorch-numpy",
    "href": "posts/fastai-lesson3/fastai-lesson3.html#pytorch-numpy",
    "title": "FastAI Lesson 3: Neural Network Foundations",
    "section": "Pytorch + Numpy",
    "text": "Pytorch + Numpy\nThe main difference between pytorch and numpy is that pytorch supports using the GPU and calculating gradients which numpy does not.\n\ndata = [[1, 2, 3], [4, 5, 6]]\n# numpy array\narr = array(data)\n# tensor\ntns = tensor(data)\n\n# select a row\nprint(f\"select the second row of tensor: {tns[1]}\")\n\n# select a column\nprint(f\"select the second column of tensor: {tns[:1]}\")\n\n# slicing\nprint(f\"select slice of tensor: {tns[1, 1:3]}\")\n\n# addition\nprint(f\"Addition with tensors: {tns + 1}\")\n\n# types\nprint(f\"tensor type: {tns.type()}\")\n\n# scale and update tensor type\nprint(f\"changing tensor type from int to float: {tns * 1.5}\")\n\nselect the second row of tensor: tensor([4, 5, 6])\nselect the second column of tensor: tensor([[1, 2, 3]])\nselect slice of tensor: tensor([5, 6])\nAddition with tensors: tensor([[2, 3, 4],\n        [5, 6, 7]])\ntensor type: torch.LongTensor\nchanging tensor type from int to float: tensor([[1.5000, 3.0000, 4.5000],\n        [6.0000, 7.5000, 9.0000]])"
  },
  {
    "objectID": "posts/fastai-lesson3/fastai-lesson3.html#computing-metrics",
    "href": "posts/fastai-lesson3/fastai-lesson3.html#computing-metrics",
    "title": "FastAI Lesson 3: Neural Network Foundations",
    "section": "Computing Metrics",
    "text": "Computing Metrics\nA metric is a number that is calculated based on the predictions of the model and the correct labels and inform us how good the model is. For classification models, accuracy is a popular metric.\n\n# create validation data set\nvalid_3_tens = torch.stack([tensor(Image.open(o))\n                            for o in (path/'valid'/'3').ls()])\nvalid_3_tens = valid_3_tens.float() / 255\nvalid_7_tens = torch.stack([tensor(Image.open(o))\n                            for o in (path/'valid'/'7').ls()])\nvalid_7_tens = valid_7_tens.float() / 255\n\nprint(f\"3 validation set shape: {valid_3_tens.shape}\")\nprint(f\"3 validation set tensor rank: {len(valid_3_tens.shape)}\")\nprint(f\"7 validation set shape: {valid_7_tens.shape}\")\nprint(f\"7 validation set tensor rank: {len(valid_7_tens.shape)}\")\n\n3 validation set shape: torch.Size([1010, 28, 28])\n3 validation set tensor rank: 3\n7 validation set shape: torch.Size([1028, 28, 28])\n7 validation set tensor rank: 3\n\n\n\n# MNIST Data Distance Function\ndef mnist_distance(a, b):\n  # (-1, -2) represent a range of axes. Tell Pytorch to take the mean ranging over the values\n  # indexed by the last two axes of the tensor (horizontal and vertical dimensions of the image)\n  # leaves only the first tensor axis which indexes over images and the final size -&gt; averaged the intensity of all the pixels\n  # in the image\n  return (a - b).abs().mean((-1,-2))\nprint(f\"Distance function measuring the distance between 3 image and ideal 3 image: {mnist_distance(a_3, mean3)}\")\n\n# measure distance between validation set 3 and ideal 3 tensor\nvalid_3_dist = mnist_distance(valid_3_tens, mean3)\nprint(f\"Distance between validation set 3 image and ideal training data set 3 image: {valid_3_dist}\")\nprint(f\"Valid 3 distance tensor shape: {valid_3_dist.shape}\")\nprint(f\"Valid 3 distance tensor rank: {len(valid_3_dist.shape)}\")\n\nDistance function measuring the distance between 3 image and ideal 3 image: 0.11143654584884644\nDistance between validation set 3 image and ideal training data set 3 image: tensor([0.1663, 0.1148, 0.1325,  ..., 0.1173, 0.1100, 0.1460])\nValid 3 distance tensor shape: torch.Size([1010])\nValid 3 distance tensor rank: 1\n\n\n\n# check if image is a 3\ndef is_3(x):\n  return mnist_distance(x, mean3) &lt; mnist_distance(x, mean7)\n\nprint(f\"Check if 3 image is actually a 3 image as a boolean: {is_3(a_3)}\")\n# 1.0 - true\n# 0.0 - false\nprint(f\"Check if 3 image is actually a 3 image as a float: {is_3(a_3).float()}\")\n\n# check all 3 images\nprint(f\"check if all 3 images in validation set are 3 images: {is_3(valid_3_tens)}\")\n\nCheck if 3 image is actually a 3 image as a boolean: True\nCheck if 3 image is actually a 3 image as a float: 1.0\ncheck if all 3 images in validation set are 3 images: tensor([False,  True,  True,  ...,  True,  True,  True])\n\n\n\n# Compute Accuracy of 3 and 7 Images\naccuracy_3s = is_3(valid_3_tens).float() .mean()\naccuracy_7s = (1 - is_3(valid_7_tens).float()).mean()\n\nprint(f\"Model accuracy for classifying 3 images: {accuracy_3s}\")\nprint(f\"Model accuracy for classifying 7 images: {accuracy_7s}\")\nprint(f\"Average model accuracy for classifying 3 and 7 images: {(accuracy_3s+accuracy_7s)/2}\")\n\nModel accuracy for classifying 3 images: 0.9168316721916199\nModel accuracy for classifying 7 images: 0.9854085445404053\nAverage model accuracy for classifying 3 and 7 images: 0.951120138168335\n\n\n\n# function for plotting\nfrom ipywidgets import interact\nfrom fastai.basics import *\n\nplt.rc('figure', dpi=90)\n\ndef plot_function(f, title=None, min=-2.1, max=2.1, color='r', ylim=None):\n    x = torch.linspace(min,max, 100)[:,None]\n    if ylim: plt.ylim(ylim)\n    plt.plot(x, f(x), color)\n    if title is not None: plt.title(title)"
  },
  {
    "objectID": "posts/fastai-lesson3/fastai-lesson3.html#fit-a-function-with-gradient-descent",
    "href": "posts/fastai-lesson3/fastai-lesson3.html#fit-a-function-with-gradient-descent",
    "title": "FastAI Lesson 3: Neural Network Foundations",
    "section": "Fit a Function with Gradient Descent",
    "text": "Fit a Function with Gradient Descent\n\nA neural network can be thought of as a mathematical function\nA simple neural network does the following things:\n\n\nMultiplies each input by a number of values(parameters).\nSums up results for each group values\nReplaces the negative numbers with zeroes\n\n\nThese steps define a single layer. The three steps are repeated using the outputs of the previous layer as inputs to the next layer. The initial parameters are selected randomly\n\n\n# Quadratic Function\n# - The quadratic function below is the one we are trying to fit(approximate)\n\ndef f(x):\n  return 3*x**2 + 2*x + 1\nplot_function(f, \"$3x^2 + 2x + 1$\")\n\n\n\n\n\n# General Quadratic Function\ndef quad(a, b, c, x):\n  return a*x**2 + b*x + c\n\n\n# Function to make quadratic functions\n\n# partial allows for fixing particular values\nfrom functools import partial\n\ndef mk_quad(a, b, c):\n  return partial(quad, a, b, c)\n\nf2 = mk_quad(3, 2, 1)\n\n# test partial and quadratic\nprint(f\"Quadtratic Function: f(10.5) = {f(10.5)}\")\nprint(f\"General Quadratic Function: f(10.5) = {f2(10.5)}\")\n\nQuadtratic Function: f(10.5) = 352.75\nGeneral Quadratic Function: f(10.5) = 352.75\n\n\n\n# Test and plot quadratic function using make quad function\nf2 = mk_quad(3, 2, 1)\nplot_function(f2)\n\n\n\n\n\n# Simulate noisy measurements of quadratic function f\n\n# noisy data generation functions\ndef noise(x, scale):\n  return np.random.normal(scale=scale, size=x.shape)\n\ndef add_noise(x, mult, add):\n  return x * (1 + noise(x, mult)) + noise(x,add)\n\n\n# Create noisy data based on the the quadratic function f(x)\nnp.random.seed(42)\n\nx = torch.linspace(-2, 2, steps=20)[:,None]\ny = add_noise(f(x), 0.15, 1.5)\n\nprint(f\"First 5 x values: {x[:5]}\")\nprint(f\"First 5 y values: {y[:5]}\")\n\nFirst 5 x values: tensor([[-2.0000],\n        [-1.7895],\n        [-1.5789],\n        [-1.3684],\n        [-1.1579]])\nFirst 5 y values: tensor([[11.8690],\n        [ 6.5433],\n        [ 5.9396],\n        [ 2.6304],\n        [ 1.7947]], dtype=torch.float64)\n\n\n\n# plot noisy data\nplt.scatter(x, y)\n\n&lt;matplotlib.collections.PathCollection at 0x78344829a260&gt;\n\n\n\n\n\n\n# Find a, b, c values that create a function that fits noisy data\n@interact(a=1.1, b=1.1, c=1.1)\ndef plot_quad(a, b, c):\n  plt.scatter(x, y)\n  plot_function(mk_quad(a,b,c), ylim=(-3,13))\n\n\n\n\n\nMetric (Loss Function)\n\nnumerical way of measuring whether function is fitting data better or worse\nMean Absolute Error (MAE) is a metric that measures the distance from each data point to the curve\n\n\ndef mae(preds, acts):\n  return (torch.abs(preds - acts)).mean()\n\n\n# Find a, b, c values that create a function that fits noisy data using metrics\n@interact(a=1.1, b=1.1, c=1.1)\ndef plot_quad(a, b, c):\n    f = mk_quad(a,b,c)\n    plt.scatter(x,y)\n    loss = mae(f(x), y)\n    plot_function(f, ylim=(-3,12), title=f\"MAE: {loss:.2f}\")\n\n\n\n\n\n\nAutomating Gradient Descent\n\nCalculus can be used to determine how much each neural network parameter should be increased or decreased to fit the data using the derivative(gradient).\nGiven the gradient of the mean absolute error with respect to parameters a, b, c then we know how adjusting a parameters such as a will change the value of the mean absolute error\nIf a paramaeter has a negative gradient then increasing the parameter will decrease the mean absolute error\nGoal is to minimize the value of the metric function we use for the loss function.\n\nSteps: 1. Find the gradient of MAE for each parameter 2. Adjust parameters a bit in the opposite direction to the sign of the gradient\n\n# Function for generating MAE for quadratic functions\ndef quad_mae(params):\n  f = mk_quad(*params)\n  return mae(f(x), y)\n\n# test quad_mae\nquad_mae([1.1, 1.1, 1.1])\n\ntensor(2.4219, dtype=torch.float64)\n\n\n\n# starting vector containing arbitrary position\nabc = torch.tensor([1.1, 1.1, 1.1])\n\n\n# tell pytorch to calculate gradient for parameters\nabc.requires_grad_()\n\ntensor([1.1000, 1.1000, 1.1000], requires_grad=True)\n\n\n\n# calculate MAE\n# - When doing gradient descent we are trying to minimize the metric which is called the loss\nloss = quad_mae(abc)\nloss\n\ntensor(2.4219, dtype=torch.float64, grad_fn=&lt;MeanBackward0&gt;)\n\n\n\n# calculate gradients\nloss.backward()\n\n\n# gradients with respect to each parameter (a, b, c)\nabc.grad\n\ntensor([-1.3529, -0.0316, -0.5000])\n\n\n\n# - gradients are a bit low -&gt; increase gradients a little bit will decrease loss\n# - update abc vector by decreasing it by gradient * learning rate\n# - torch.no_grad turns of gradient calculation when we update the vector since\n# abc -= abc.grad * learning rate is not part of quadtratic function model and we\n# do not want the derivatives to include that calculation\nwith torch.no_grad():\n    abc -= abc.grad * 0.01\n    loss = quad_mae(abc)\nprint(f'loss={loss:.2f}')\n\nloss=2.40\n\n\n\n# compute loss for 10 iterations\nfor i in range(10):\n    loss = quad_mae(abc)\n    loss.backward()\n    with torch.no_grad():\n      abc -= abc.grad*0.01\n    print(f'step={i}; loss={loss:.2f}')\n\nstep=0; loss=2.40\nstep=1; loss=2.36\nstep=2; loss=2.30\nstep=3; loss=2.21\nstep=4; loss=2.11\nstep=5; loss=1.98\nstep=6; loss=1.85\nstep=7; loss=1.72\nstep=8; loss=1.58\nstep=9; loss=1.46\n\n\n\n\nNeural Network\n\nNeural Networks can approximate any computable function given enough parameters\nA comptuable function covers any type of problem\nNeural Networks approximate a function using the following steps:\n\n\nMatrix multiplication - multiply things together and add them up\nThe function max(x, 0) which replaces all negative numbers with 0\n\n\n# - f(x) = max(x, 0) is represented as torch.clip(x, 0)\ndef rectified_linear(m, b, x):\n  y = m*x + b\n  return torch.clip(y, 0)\n\n\n# plot rectified linear\nplot_function(partial(rectified_linear, 1,1))\n\n\n\n\n\n# plot rectified linear using pytorch RELU function\nimport torch.nn.functional as F\n\ndef rectified_linear2(m,b,x):\n  return F.relu(m*x+b)\n\nplot_function(partial(rectified_linear2, 1,1))\n\n\n\n\n\n# interactive relu\n@interact(m=1.5, b=1.5)\ndef plot_relu(m, b):\n    plot_function(partial(rectified_linear, m,b), ylim=(-1,4))\n\n\n\n\n\n# Combine RELUs\ndef double_relu(m1,b1,m2,b2,x):\n    return rectified_linear(m1,b1,x) + rectified_linear(m2,b2,x)\n\n@interact(m1=-1.5, b1=-1.5, m2=1.5, b2=1.5)\ndef plot_double_relu(m1, b1, m2, b2):\n    plot_function(partial(double_relu, m1,b1,m2,b2), ylim=(-1,6))"
  },
  {
    "objectID": "posts/fastai-lesson3/fastai-lesson3.html#stochastic-gradient-descent",
    "href": "posts/fastai-lesson3/fastai-lesson3.html#stochastic-gradient-descent",
    "title": "FastAI Lesson 3: Neural Network Foundations",
    "section": "Stochastic Gradient Descent",
    "text": "Stochastic Gradient Descent\nIn the Pixel Similarity approach from above, we don’t have a way for the model to learn and improve its accuracy. Instead of trying to compare an image with an ideal image, we can use Stochastic Gradient Descent (SGD) to look at each individual pixel and come up with a set of weights with large weights associated with pixels that are the most black for a particular label (ie. digit in MNIST data) using Arthur Samuel’s definition of machine learning\n\nStochastic Gradient Descent Steps for an Image Classifier\n\ninitialize weights\nCalculate Predictions\nBased on the predictions, calculate how good the model is (its loss)\nCalculate the gradient which measures for each weight, how changing that weight would change the loss\nStep (change) all the weights based on Step 4\nGo back to Step 2 and repeat the process\nIterate until you decide to stop the training process until you decide the model is good enough for your problem\n\nThe problem below is an example from the FastAI Book simulating a roller coaster and trying to find a function that best fits the data to understand how speed changes over time. There has been some discussion on the FastAI discord and forums about the loss value being too high (see Chapter 4 SGD Discussion) because of the data not being centered similar to the way Jeremy centered the Titantic data in Lesson 5. The book examples are slightly older and different from how the 2022 FastAI course is running so I’d recommend going over the course notebook examples first and then checking the relevant book chapters mentioned.\n\n# Roller Coaster Problem with Stochastic Gradient Descent\n# generate data\ntime = torch.arange(0,20).float()\nspeed = torch.randn(20) * 3 + 0.75 * (time - 9.5)**2 + 1\nplt.scatter(time,speed)\n\n# use SGD to find a function that fits the data for the rollercoaster data\n# t - the time when we are measuring the rollercoaster speed\n# params - the values that define which quadratic we're trying\ndef f(t, params):\n    a,b,c = params\n    return a * (t**2) + (b * t) + c\n\n# Define a loss function - returns a value based on a prediction and a target\n# where lower values of the functions correspond to better predictions. Need\n#  loss function to return lower values when predictions are more accurate, as\n#  SGD is trying to minimize this loss. For continuous data, Mean Square Error is\n#  frequently used\ndef mse(preds, targets):\n  return ((preds-targets)**2).mean()\n\n# SGD Process\n# function for visualizing how close our predictions are to targets\ndef show_preds(preds, ax=None):\n    if ax is None: ax=plt.subplots()[1]\n    ax.scatter(time, speed)\n    ax.scatter(time, to_np(preds), color='red')\n    ax.set_ylim(-300,100)\n\n# 1. initialize weights\nparams = torch.randn(3).requires_grad_()\nprint(f\"The parameter values after initialization: {params}\")\norig_params = params.clone()\nprint(f\"The original parameters: {orig_params}\")\n\n# 2. Calculate Predictions\npreds = f(time, params)\nshow_preds(preds)\n\n# 3. Based on the predictions, calculate how good the model is (its loss)\nloss = mse(preds, speed)\nprint(f\"Loss value: {loss}\")\n\n# 4. Calculate the gradient which measures for each weight, how changing that weight would change the loss\nloss.backward()\nprint(f\"Gradient values for each argument: {params.grad}\")\n# Calculate approximation of how parameters need to change\nprint(f\"Test a new gradient with a learning rate of 1e^-5: {params.grad * 1e-5}\")\nprint(f\"Parameter values after computing the gradient: {params}\")\n\n# 5. Step (change) all the weights based on Step 4\nlearning_rate = 1e-5\nparams.data -= learning_rate * params.grad.data\nparams.grad = None\n# Check if loss has improved\npreds = f(time, params)\nmse(preds, speed)\nshow_preds(preds)\n\n# 6. Go back to Step 2 and repeat the process\ndef apply_step(params, prn=True):\n    preds = f(time, params)\n    loss = mse(preds, speed)\n    loss.backward()\n    params.data -= learning_rate * params.grad.data\n    params.grad = None\n    if prn:\n      print(f\"Loss Value: {loss.item()}\")\n    return preds\n\nfor i in range(10):\n  apply_step(params)\nparams = orig_params.detach().requires_grad_()\n\n# 7. Iterate until you decide to stop the training process until you decide the model is good enough for your problem\n_,axs = plt.subplots(1,4,figsize=(12,3))\nfor ax in axs: show_preds(apply_step(params, False), ax)\nplt.tight_layout()\n\nThe parameter values after initialization: tensor([-1.7438,  0.5342, -0.5521], requires_grad=True)\nThe original parameters: tensor([-1.7438,  0.5342, -0.5521], grad_fn=&lt;CloneBackward0&gt;)\nLoss value: 97225.4375\nGradient values for each argument: tensor([-104254.5703,   -6678.1953,    -473.9090])\nTest a new gradient with a learning rate of 1e^-5: tensor([-1.0425, -0.0668, -0.0047])\nParameter values after computing the gradient: tensor([-1.7438,  0.5342, -0.5521], requires_grad=True)\nLoss Value: 18918.27734375\nLoss Value: 4100.1640625\nLoss Value: 1296.121337890625\nLoss Value: 765.5072631835938\nLoss Value: 665.0955810546875\nLoss Value: 646.0914306640625\nLoss Value: 642.4918823242188\nLoss Value: 641.8074951171875\nLoss Value: 641.6746215820312\nLoss Value: 641.6461791992188"
  },
  {
    "objectID": "posts/fastai-lesson3/fastai-lesson3.html#building-the-mnist-image-classifier",
    "href": "posts/fastai-lesson3/fastai-lesson3.html#building-the-mnist-image-classifier",
    "title": "FastAI Lesson 3: Neural Network Foundations",
    "section": "Building the MNIST Image Classifier",
    "text": "Building the MNIST Image Classifier\n\nTraining Dataset\n\n# Build Training Dataset\n# -1 view function -&gt; make this axis as big as necessary to fit all the data\ntrain_x = torch.cat([stacked_threes, stacked_sevens]).view(-1, 28*28)\ntrain_y = tensor([1]*len(threes) + [0]*len(sevens)).unsqueeze(1)\n\nprint(f\"train_x data shape: {train_x.shape}\")\nprint(f\"train_x data tensor rank: {len(train_x.shape)}\")\nprint(f\"train_y data shape: {train_y.shape}\")\nprint(f\"train_y data tensor rank: {len(train_y.shape)}\")\n\ntrain_x data shape: torch.Size([12396, 784])\ntrain_x data tensor rank: 2\ntrain_y data shape: torch.Size([12396, 1])\ntrain_y data tensor rank: 2\n\n\n\n\nValidation Dataset\n\n# Build Validation Dataset\nvalid_x = torch.cat([valid_3_tens, valid_7_tens]).view(-1, 28*28)\nvalid_y = tensor([1]*len(valid_3_tens) + [0]*len(valid_7_tens)).unsqueeze(1)\nprint(f\"valid_x data shape: {valid_x.shape}\")\nprint(f\"valid_x data tensor rank: {len(valid_x.shape)}\")\nprint(f\"valid_y data shape: {valid_y.shape}\")\nprint(f\"valid_y data tensor rank: {len(valid_y.shape)}\")\n\nvalid_x data shape: torch.Size([2038, 784])\nvalid_x data tensor rank: 2\nvalid_y data shape: torch.Size([2038, 1])\nvalid_y data tensor rank: 2\n\n\n\n# Build a Dataset object(training data) for PyTorch\n# Dataset is required to return a tuple of (x, y) when indexed\ndset = list(zip(train_x,train_y))\nx,y = dset[0]\nprint(f\"datset x shape: {x.shape}\")\nprint(f\"dataset x tensor rank: {len(x.shape)}\")\nprint(f\"datset y shape: {y.shape}\")\nprint(f\"dataset y tensor rank: {len(y.shape)}\")\n\ndatset x shape: torch.Size([784])\ndataset x tensor rank: 1\ndatset y shape: torch.Size([1])\ndataset y tensor rank: 1\n\n\n\n# Build a Dataset object(validation data) for PyTorch\n# Dataset is required to return a tuple of (x, y) when indexed\nvalid_dset = list(zip(valid_x,valid_y))\n\n\n\nInitialize Model Weights\n\n# Initialize model weights\ndef init_params(size, std=1.0):\n  return (torch.randn(size)*std).requires_grad_()\n\nweights = init_params((28 * 28,1))\n\n# Initialize a Bias Value\n# need a bias to ensure its not 0 when the pixels are 0\nbias = init_params(1)\n\n\n# Calculate a prediction for one image\nprint(f\"Prediction for a single image from training data: {(train_x[0]*weights.T).sum() + bias}\")\n\nPrediction for a single image from training data: tensor([14.6702], grad_fn=&lt;AddBackward0&gt;)\n\n\n\n\nLinear Classifier\n\n# Matrix Multiplication\n#  Create a linear combination for the prediction values\n# compute predictions for all the images in the training data\ndef linear1(xb):\n  return xb@weights + bias\npreds = linear1(train_x)\nprint(f\"Predictions\")\nprint(preds)\nprint(f\"Prediction shape: {preds.shape}\")\nprint(f\"Prediction tensor rank: {len(preds.shape)}\")\n\nPredictions\ntensor([[14.6702],\n        [23.8813],\n        [21.2188],\n        ...,\n        [ 5.4375],\n        [ 9.9480],\n        [ 3.1540]], grad_fn=&lt;AddBackward0&gt;)\nPrediction shape: torch.Size([12396, 1])\nPrediction tensor rank: 2\n\n\n\n# Check accuracy of prediction\ncorrects = (preds&gt;0.0).float() == train_y\nprint(f\"Accuracy of Predictions: {corrects}\")\nprint(f\"Average accuracy of all predictions: {corrects.float().mean().item()}\")\n\nAccuracy of Predictions: tensor([[ True],\n        [ True],\n        [ True],\n        ...,\n        [False],\n        [False],\n        [False]])\nAverage accuracy of all predictions: 0.5291222929954529\n\n\n\n# Improve Accuracy\n# test to see if we can improve accuracy with a small change\nwith torch.no_grad():\n  weights[0] *= 1.0001\npreds = linear1(train_x)\nprint(f\"Average accuracy after updating the weights: {((preds&gt;0.0).float() == train_y).float().mean().item()}\")\n\n# there is no change because the change in weights is so small that (y_new - y_old) is very close to 0 ie.\n# the gradient is almost 0 everywhere\n\n# need to find a loss function which when our weights result in slightly better predictions produces a slightly better loss\n\nAverage accuracy after updating the weights: 0.5291222929954529\n\n\n\n\nLoss Function\n\n# Build a Loss Function\n# loss function receive predictions from the model about the images\n# the purpose of the loss function is to measure the difference between predicted values and true values ie the labels\ntrgts  = tensor([1,0,1])\nprds   = tensor([0.9, 0.4, 0.2])\n\n# first attempt at a loss function\ndef mnist_loss(predictions, targets):\n  # measures how distant each predictions is from 1 if it should be 1, how distant it is from 0 if it should be 0\n  # and takes the mean of all the distances\n    return torch.where(targets==1, 1-predictions, predictions).mean()\n\n# need a scalar for the final loss -&gt; the lower the loss value the better\n# indicates accurate predictions are more confident and when inaccurate predictions are less confident\nprint(f\"Test loss function: {mnist_loss(prds,trgts)}\")\n\n# issue with this loss function is that it assumes all predictions are between 0 and 1\n# sigmoid function always outputs a number between 0 and 1\ndef sigmoid(x):\n  return 1/(1+torch.exp(-x))\n\n# second attempt at a loss function using sigmoid\ndef mnist_loss_sigmoid(predictions, targets):\n    predictions = predictions.sigmoid()\n    return torch.where(targets==1, 1-predictions, predictions).mean()\n\nprint(f\"Test loss function with sigmoid: {mnist_loss_sigmoid(prds,trgts)}\")\n\nTest loss function: 0.43333330750465393\nTest loss function with sigmoid: 0.44596806168556213\n\n\n\n\nSGD + Mini Batches\n\n# SGD + Mini Batches\n# Optimization Step - updating the weights based on gradients\n# need to calculate loss over one or more data items\n# Solution: Calculate the average loss for a data items at a time (mini-batch)\n# Batch Size - number of items in mini batch\n\n# use fastai to build a dataloader object to shuffle data\n# randomly shuffle data on every epoch before creating mini batches\ncoll = range(15)\ndl = DataLoader(coll, batch_size=5, shuffle=True)\nprint(f\"Example of randomly generated mini-batch of batch size 5: {list(dl)}\")\n\nExample of randomly generated mini-batch of batch size 5: [tensor([ 8,  4,  9,  5, 14]), tensor([ 6,  0, 11,  7, 12]), tensor([ 2, 10,  3, 13,  1])]\n\n\n\n\nMNIST Model Training Loop using SGD\n\n# Build a training loop for a model\n# Initialize weights and bias randomly\nweights = init_params((28*28,1))\nbias = init_params(1)\n\n# Create DataLoader for the training data\ndl = DataLoader(dset, batch_size=256)\nxb,yb = first(dl)\nprint(f\"xb shape: {xb.shape}\")\nprint(f\"xb tensor rank: {len(xb.shape)}\")\nprint(f\"yb shape: {yb.shape}\")\nprint(f\"yb tensor rank: {len(yb.shape)}\")\n\n# Create DataLoader for the validation data\nvalid_dl = DataLoader(valid_dset, batch_size=256)\n\n# Create a Mini-Batch of batch size 4 for testing\nbatch = train_x[:4]\nprint(f\"batch shape: {batch.shape}\")\nprint(f\"batch tensor rank: {len(batch.shape)}\")\n\n# Create Predictions\n# preds = linear1(batch)\n# print(f\"predictions: {preds}\")\n\n# Measure loss\n# loss = mnist_loss_sigmoid(preds, train_y[:4])\n# print(f\"Loss: {loss}\")\n\n# Compute Gradients\n# loss.backward()\n# print(f\"weights gradient shape: {weights.grad.shape}\")\n# print(f\"Average of weight gradients: {weights.grad.mean()}\")\n# print(f\"bias gradient: {bias.grad}\")\n\n# function for calculating gradient\ndef calc_grad(xb, yb, model):\n    preds = model(xb)\n    loss = mnist_loss(preds, yb)\n    loss.backward()\n\nprint(f\"Test calculating gradients: {calc_grad(batch, train_y[:4], linear1)}\")\nprint(f\"Average of weight gradients: {weights.grad.mean()}\")\nprint(f\"bias gradient: {bias.grad}\")\n\n# loss.backward adds the gradients of loss to any gradients that are currently stored\n# so have to set the current gradients to 0 first\n\n# training loop for an epoch\ndef train_epoch(model, lr, params):\n    for xb,yb in dl:\n        calc_grad(xb, yb, model)\n        for p in params:\n            p.data -= p.grad*lr\n            p.grad.zero_()\n\n# check accuracy at this point\n# print(f\"Check Accuracy at this point: {(preds&gt;0.0).float() == train_y[:4]}\")\n\n# function for calculating validation accuracy\ndef batch_accuracy(xb, yb):\n    preds = xb.sigmoid()\n    correct = (preds&gt;0.5) == yb\n    return correct.float().mean()\n\nprint(f\"Testing batch accuracy: {batch_accuracy(linear1(batch), train_y[:4])}\")\n\n# put batches together to create a validation epoch\ndef validate_epoch(model):\n    accs = [batch_accuracy(model(xb), yb) for xb,yb in valid_dl]\n    return round(torch.stack(accs).mean().item(), 4)\n\nprint(f\"Test validation epoch: {validate_epoch(linear1)}\")\n\n# train for 1 epoch and see if things improve\nlr = 1.\nparams = weights, bias\ntrain_epoch(linear1, lr, params)\nprint(f\"check if accuracy has improved from earlier: {validate_epoch(linear1)}\")\n\n# train for a few epochs\nfor i in range(20):\n    train_epoch(linear1, lr, params)\n    print(validate_epoch(linear1), end=' ')\n\nxb shape: torch.Size([256, 784])\nxb tensor rank: 2\nyb shape: torch.Size([256, 1])\nyb tensor rank: 2\nbatch shape: torch.Size([4, 784])\nbatch tensor rank: 2\nTest calculating gradients: None\nAverage of weight gradients: -0.15112045407295227\nbias gradient: tensor([-1.])\nTesting batch accuracy: 0.0\nTest validation epoch: 0.4707\ncheck if accuracy has improved from earlier: 0.9483\n0.9538 0.9543 0.9528 0.9528 0.9528 0.9533 0.9533 0.9533 0.9533 0.9533 0.9533 0.9533 0.9533 0.9533 0.9538 0.9538 0.9533 0.9533 0.9533 0.9533"
  },
  {
    "objectID": "posts/fastai-lesson3/fastai-lesson3.html#pytorch-setup-for-sgd-pytorch-optimizer",
    "href": "posts/fastai-lesson3/fastai-lesson3.html#pytorch-setup-for-sgd-pytorch-optimizer",
    "title": "FastAI Lesson 3: Neural Network Foundations",
    "section": "PyTorch setup for SGD + Pytorch Optimizer",
    "text": "PyTorch setup for SGD + Pytorch Optimizer\n\n# Build SGD Functionality - PyTorch Optimizer\n\n# intialize weights and biases in a single pytorch class\nlinear_model = nn.Linear(28 * 28,1)\nw,b = linear_model.parameters()\nprint(f\"weights shape: {w.shape}\")\nprint(f\"weight tensor rank: {len(w.shape)}\")\nprint(f\"bias shape: {b.shape}\")\nprint(f\"bias tensor rank: {len(b.shape)}\")\n\n# Pytorch Optimizer Setup\nclass BasicOptim:\n    def __init__(self,params,lr): self.params,self.lr = list(params),lr\n\n    def step(self, *args, **kwargs):\n        for p in self.params: p.data -= p.grad.data * self.lr\n\n    def zero_grad(self, *args, **kwargs):\n        for p in self.params: p.grad = None\n\nweights shape: torch.Size([1, 784])\nweight tensor rank: 2\nbias shape: torch.Size([1])\nbias tensor rank: 1\n\n\n\n# define training epoch function that use SGD\nopt = BasicOptim(linear_model.parameters(), lr)\n\ndef train_epoch(model):\n    for xb,yb in dl:\n        calc_grad(xb, yb, model)\n        opt.step()\n        opt.zero_grad()\n\n# check validation epoch\nprint(f\"Check accuracy after adding SGD: {validate_epoch(linear_model)}\")\n\n# Update Model Training\ndef train_model(model, epochs):\n    for i in range(epochs):\n        train_epoch(model)\n        print(validate_epoch(model), end=' ')\n\nprint(f\"Test training model: {train_model(linear_model, 20)}\")\n\nCheck accuracy after adding SGD: 0.7631\n0.9533 0.9533 0.9533 0.9533 0.9533 0.9533 0.9533 0.9533 0.9533 0.9533 0.9533 0.9533 0.9533 0.9533 0.9533 0.9533 0.9533 0.9533 0.9533 0.9533 Test training model: None"
  },
  {
    "objectID": "posts/fastai-lesson3/fastai-lesson3.html#train-mnist-model-using-fastai-library",
    "href": "posts/fastai-lesson3/fastai-lesson3.html#train-mnist-model-using-fastai-library",
    "title": "FastAI Lesson 3: Neural Network Foundations",
    "section": "Train MNIST Model using FastAI Library",
    "text": "Train MNIST Model using FastAI Library\n\n# Modify Model Training Code with FastAI\n\n# define model information that uses SGD\n# linear_model = nn.Linear(28 * 28,1)\n# opt = SGD(linear_model.parameters(), lr)\n# train_model(linear_model, 20)\n\n# Define new DataLoaders\ndls = DataLoaders(dl, valid_dl)\n\n# Define a general purpose Learner class\nlearn = Learner(dls, nn.Linear(28*28,1), opt_func=SGD,\n                loss_func=mnist_loss_sigmoid, metrics=batch_accuracy)\n\n# Train Model with Learner.fit\nlearn.fit(10, lr=lr)\n\n\n\n\n\n\n    \n      \n      50.00% [5/10 00:00&lt;00:00]\n    \n    \n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nbatch_accuracy\ntime\n\n\n\n\n0\n0.637277\n0.502527\n0.495584\n00:00\n\n\n1\n0.352531\n0.309337\n0.688911\n00:00\n\n\n2\n0.136486\n0.154489\n0.861629\n00:00\n\n\n3\n0.063793\n0.097085\n0.918057\n00:00\n\n\n4\n0.036680\n0.072898\n0.937193\n00:00\n\n\n\n\n\n    \n      \n      37.50% [3/8 00:00&lt;00:00 0.0259]\n    \n    \n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nbatch_accuracy\ntime\n\n\n\n\n0\n0.637277\n0.502527\n0.495584\n00:00\n\n\n1\n0.352531\n0.309337\n0.688911\n00:00\n\n\n2\n0.136486\n0.154489\n0.861629\n00:00\n\n\n3\n0.063793\n0.097085\n0.918057\n00:00\n\n\n4\n0.036680\n0.072898\n0.937193\n00:00\n\n\n5\n0.025862\n0.059382\n0.950442\n00:00\n\n\n6\n0.021269\n0.050823\n0.957802\n00:00\n\n\n7\n0.019119\n0.045053\n0.963690\n00:00\n\n\n8\n0.017956\n0.040945\n0.965653\n00:00\n\n\n9\n0.017213\n0.037881\n0.967615\n00:00"
  },
  {
    "objectID": "posts/fastai-lesson3/fastai-lesson3.html#neural-networks",
    "href": "posts/fastai-lesson3/fastai-lesson3.html#neural-networks",
    "title": "FastAI Lesson 3: Neural Network Foundations",
    "section": "Neural Networks",
    "text": "Neural Networks\nLinear classifiers are limited in what they can do. To handle more complex functions we need to add a nonlinear function between two linear classifiers. This is what defines a neural network.\n\n# define a simple neural network\n\n# randomly intialize weights and biases\nw1 = init_params((28 * 28,30))\nb1 = init_params(30)\nw2 = init_params((30,1))\nb2 = init_params(1)\n\n# def simple_net(xb):\n#     res = xb@w1 + b1\n#     # Rectified Linear Unit - RELU -&gt; replaces every negative number with 0\n#     res = res.max(tensor(0.0))\n#     res = res@w2 + b2\n#     return res\n\n\n# Pytorch version of a simple neural network\nsimple_net = nn.Sequential(\n    nn.Linear(28 * 28,30),\n    nn.ReLU(),\n    nn.Linear(30,1)\n)\n\n\n# Apply Simplenet to MNIST data\nlearn = Learner(dls, simple_net, opt_func=SGD,\n                loss_func=mnist_loss_sigmoid, metrics=batch_accuracy)\n\nlearn.fit(40, 0.1)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nbatch_accuracy\ntime\n\n\n\n\n0\n0.298695\n0.416960\n0.504907\n00:00\n\n\n1\n0.142627\n0.227632\n0.810108\n00:00\n\n\n2\n0.080331\n0.116664\n0.913150\n00:00\n\n\n3\n0.053554\n0.079320\n0.938175\n00:00\n\n\n4\n0.040960\n0.061973\n0.954367\n00:00\n\n\n5\n0.034424\n0.052130\n0.962218\n00:00\n\n\n6\n0.030614\n0.045893\n0.964671\n00:00\n\n\n7\n0.028108\n0.041631\n0.966143\n00:00\n\n\n8\n0.026289\n0.038535\n0.968106\n00:00\n\n\n9\n0.024872\n0.036179\n0.969578\n00:00\n\n\n10\n0.023721\n0.034313\n0.971050\n00:00\n\n\n11\n0.022760\n0.032791\n0.972031\n00:00\n\n\n12\n0.021941\n0.031517\n0.972031\n00:00\n\n\n13\n0.021234\n0.030427\n0.973013\n00:00\n\n\n14\n0.020615\n0.029480\n0.973994\n00:00\n\n\n15\n0.020067\n0.028645\n0.976448\n00:00\n\n\n16\n0.019578\n0.027902\n0.977429\n00:00\n\n\n17\n0.019137\n0.027234\n0.977429\n00:00\n\n\n18\n0.018736\n0.026632\n0.977920\n00:00\n\n\n19\n0.018370\n0.026085\n0.978410\n00:00\n\n\n20\n0.018033\n0.025586\n0.979392\n00:00\n\n\n21\n0.017722\n0.025129\n0.979882\n00:00\n\n\n22\n0.017433\n0.024710\n0.980373\n00:00\n\n\n23\n0.017163\n0.024323\n0.980864\n00:00\n\n\n24\n0.016911\n0.023966\n0.981354\n00:00\n\n\n25\n0.016674\n0.023635\n0.981354\n00:00\n\n\n26\n0.016451\n0.023328\n0.981354\n00:00\n\n\n27\n0.016240\n0.023042\n0.981845\n00:00\n\n\n28\n0.016040\n0.022775\n0.981354\n00:00\n\n\n29\n0.015850\n0.022527\n0.981845\n00:00\n\n\n30\n0.015670\n0.022294\n0.981845\n00:00\n\n\n31\n0.015498\n0.022077\n0.982336\n00:00\n\n\n32\n0.015334\n0.021873\n0.982336\n00:00\n\n\n33\n0.015177\n0.021682\n0.982826\n00:00\n\n\n34\n0.015026\n0.021502\n0.982826\n00:00\n\n\n35\n0.014882\n0.021333\n0.982826\n00:00\n\n\n36\n0.014743\n0.021173\n0.982826\n00:00\n\n\n37\n0.014609\n0.021022\n0.982826\n00:00\n\n\n38\n0.014480\n0.020880\n0.983317\n00:00\n\n\n39\n0.014356\n0.020745\n0.983317\n00:00\n\n\n\n\n\n\n# visualize the accuracy of model using simplenet\n# y axis - accuracy\n# x-axis number of epochs\nplt.plot(L(learn.recorder.values).itemgot(2));\n\n\n\n\n\nprint(f\"Final Accuracy of model on the MNIST dataset: {learn.recorder.values[-1][2]}\")\n\nFinal Accuracy of model on the MNIST dataset: 0.983316957950592"
  },
  {
    "objectID": "posts/fastai-lesson3/fastai-lesson3.html#going-deeper-into-deep-learning-neural-networks",
    "href": "posts/fastai-lesson3/fastai-lesson3.html#going-deeper-into-deep-learning-neural-networks",
    "title": "FastAI Lesson 3: Neural Network Foundations",
    "section": "Going Deeper into Deep Learning + Neural Networks",
    "text": "Going Deeper into Deep Learning + Neural Networks\nThe following code below is an 18 layer resnet model with nearly 100% accuracy on the MNIST data. The above code was a simple neural network with 2 layers so the results of resnet-18 on this data show that accuracy improves as we add more layers. One thing to consider are the trade off’s mentioned by Jeremy in the lecture video\n\n# Going Deeper into Deep Learning + Neural Networks\ndls = ImageDataLoaders.from_folder(path)\nlearn = vision_learner(dls, resnet18, pretrained=False,\n                    loss_func=F.cross_entropy, metrics=accuracy)\nlearn.fit_one_cycle(1, 0.1)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.100045\n0.014498\n0.994603\n00:16"
  },
  {
    "objectID": "posts/fastai-lesson3/fastai-lesson3.html#resources",
    "href": "posts/fastai-lesson3/fastai-lesson3.html#resources",
    "title": "FastAI Lesson 3: Neural Network Foundations",
    "section": "Resources",
    "text": "Resources\n\nFastAI Lesson 3\nFastAI Chapter 4\nHow Does a Neural Net Really Work\nWhich Image Models are Best\n3Blue1Brown Neural Networks\nAndrej Karpathy Neural Networks Zero to Hero\nUnderstanding Dimensions in PyTorch\nUnderstanding Numpy Axis\nPyTorch docs\nJeremy Howard FastAI Live Coding\nfast.ai docs"
  },
  {
    "objectID": "posts/fastai-lesson0/index.html",
    "href": "posts/fastai-lesson0/index.html",
    "title": "FastAI Lesson 0: How to FastAI",
    "section": "",
    "text": "I’ve recently started FastAI Practical Deep Learning for Coders to learn about machine learning. To hold myself accountable and finish the course I am going to try to write a blog post for every video in the series starting with Lesson 0. By doing this my goal is to not only learn about deep learning and track my progress but reinforce my learning by trying to explain what I’ve learned for people new to machine learning like me. Lesson 0 isn’t officially presented in the course until Lesson 3 but it provides valuable advice for how to complete FastAI and actually learn how to write deep learning code.\nLesson 0 doesn’t have any code examples or notebooks but I took Jeremy’s advice about blogging and interacting with the community by setting up this Quarto blog as the Lesson 0 project. Lesson 0 was created in 2020 before Twitter became X and closed off to people who didn’t have an account. Since then, some of the ML community and other academic communities in CS have migrated to Discord , Mastodon, and Bluesky. I’d recommend checking out the FastAI Forums, FastAI Discord, and BlueSky."
  },
  {
    "objectID": "posts/fastai-lesson0/index.html#jeremy-howards-advice",
    "href": "posts/fastai-lesson0/index.html#jeremy-howards-advice",
    "title": "FastAI Lesson 0: How to FastAI",
    "section": "Jeremy Howard’s Advice",
    "text": "Jeremy Howard’s Advice\n\nCommit to finish the course\nTry to finish one really good project to show off what you’ve learned in the course\nShare and blog about your work"
  },
  {
    "objectID": "posts/fastai-lesson0/index.html#how-to-watch-a-fastai-lesson",
    "href": "posts/fastai-lesson0/index.html#how-to-watch-a-fastai-lesson",
    "title": "FastAI Lesson 0: How to FastAI",
    "section": "How to Watch A FastAI Lesson",
    "text": "How to Watch A FastAI Lesson\nThis advice was presented by Jeremy Howard in Lesson 0\n\nWatch FastAI Lecture\nRun notebooks and code presented in the lecture and experiment with code\nReproduce notebook from a clean notebook (Jeremy provides clean notebooks on github)\nRepeat with a different dataset (could be personal project)"
  },
  {
    "objectID": "posts/fastai-lesson0/index.html#resources",
    "href": "posts/fastai-lesson0/index.html#resources",
    "title": "FastAI Lesson 0: How to FastAI",
    "section": "Resources",
    "text": "Resources\n\nFastAI Lesson 0\nChristine Mcleavey’s Blog\nQuarto\nBluesky\nfast.ai Forum"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is my first blog post. Welcome!\nI decided to start a blog inspired by Jeremy Howard’s Lecture 0 from the FastAI Course. Jeremy and FastAI used to maintain a blogging and publication tool called FastPages but transitioned to a new publication system called Quarto.\nThis blog is written in Quarto. Why Quarto? In the past I had attempted to used Medium and Substack but never fully spent the time to learn the tools to write a blog post. I decided to learn and publish with Quarto because of its capabilities to produce high quality scientific and technical documents with the ability to embed code and visualizations for review. Another reason why I like Quarto is how easy it is to write markdown and publish a blog.\nGoing forward I hope to use Quarto to write about my programming projects, thoughts,and improve my writing."
  },
  {
    "objectID": "posts/fastai-lesson1/fastai-lesson1.html",
    "href": "posts/fastai-lesson1/fastai-lesson1.html",
    "title": "FastAI Lesson 1: Getting Started",
    "section": "",
    "text": "In this lesson, Jeremy walks through a bird classification example which back in 2015 was considered the bleeding edge of state of the art. 8 years later, it’s hard to believe that I can run this on my own local machine. Even though my training is in computer science, I found some of the definitions and the code in this lesson challenging which is where Chapter 1 was really helpful. I would strongly recommend checking out the definitions in this chapter because they are still used today when talking about more advanced models like GPT-4."
  },
  {
    "objectID": "posts/fastai-lesson1/fastai-lesson1.html#summary",
    "href": "posts/fastai-lesson1/fastai-lesson1.html#summary",
    "title": "FastAI Lesson 1: Getting Started",
    "section": "",
    "text": "In this lesson, Jeremy walks through a bird classification example which back in 2015 was considered the bleeding edge of state of the art. 8 years later, it’s hard to believe that I can run this on my own local machine. Even though my training is in computer science, I found some of the definitions and the code in this lesson challenging which is where Chapter 1 was really helpful. I would strongly recommend checking out the definitions in this chapter because they are still used today when talking about more advanced models like GPT-4."
  },
  {
    "objectID": "posts/fastai-lesson1/fastai-lesson1.html#terminology",
    "href": "posts/fastai-lesson1/fastai-lesson1.html#terminology",
    "title": "FastAI Lesson 1: Getting Started",
    "section": "Terminology",
    "text": "Terminology\n\nMachine Learning - The training of programs developed by teaching a computer learn from experience rather than coding the individual steps\nDeep Learning - A computer technique that uses layers of neural networks to extract and transform data. Neural networks layers are trained by algorithms that minimize their errors and improve their accuracy. This is how a network learns to perform a specific task. Deep learning is a sub-discipline of machine learning.\nDataset - A bunch of data ie. images, sounds, text, files or anything else.\nLabel - The data we’re trying to predict ie. dog or cat.\nIndependent Variable - Data that does not include labels\nDependent Variable - the correct label ie. dog or cat. Also called targets.\nArchitecture - The structure of the model we’re trying to fit. A mathematical function that we’re passing the input data and parameters to.\nParameters - The values in the model that change what task it can do and are updated through model training. In Arthur Samuel’s definitions the synonym for parameters is weights which has a different meaning in modern deep learning.\nParameter(Weight) Assignment - Particular choice of values for parameters.\nWeights - A particular type of model parameter.\nFit - Update the model parameters such that the predictions of the model using the input data match the target labels.\nTrain - Synoym for fit.\nPretrained Model - A model that has already been trained, generally using a large dataset and will be fine-tuned ie. resnet class of models.\nFine-Tune - Update a pretrained model for a different task.\nEpoch - One complete pass through the input data.\nLoss - A measure of how good the model is, chosen to drive training via Stochastic Gradient Descent (SGD).\nMetric - A measurement of how good the model is, using the validation set, chosen for human consumption.\nValidation Set - A set of data held out from training, used only for measuring how good the model is.\nTraining Set - The data used for fitting the model; does not include any data from the validation set.\nOverfitting - Training a model in such a way that it remembers specific features of the input data rather than generalizing.\nConvolutional Neural Network (CNN) - A type of neural network that works particularly well for computer vision tasks.\nTransfer Learning - Using a pretrained model for a task different to what it was originally trained for.\nHead Layer - When using a pretrained model, replace the last layer with one or more layers with randomized weights of an appropriate size for the dataset you are working with. This customizes a model specifically for your task when using a pretrained model."
  },
  {
    "objectID": "posts/fastai-lesson1/fastai-lesson1.html#limitations-of-machine-learning",
    "href": "posts/fastai-lesson1/fastai-lesson1.html#limitations-of-machine-learning",
    "title": "FastAI Lesson 1: Getting Started",
    "section": "Limitations of Machine Learning",
    "text": "Limitations of Machine Learning\n\nA model cannot be created without data\nA model can only learn to operate on patterns seen in input data used to train it\nThis learning approach only creates predictions not recommend actions\nWe need labels + data (pictures of dogs and cats that have labels saying which ones are dogs and which ones are cats)"
  },
  {
    "objectID": "posts/fastai-lesson1/fastai-lesson1.html#is-it-a-bird-example",
    "href": "posts/fastai-lesson1/fastai-lesson1.html#is-it-a-bird-example",
    "title": "FastAI Lesson 1: Getting Started",
    "section": "Is It a Bird? Example",
    "text": "Is It a Bird? Example\nThe following code in this example is created and written by Jeremy Howard and FastAI as found in the example Is it a bird? Creating a model from your own data. My modification was adding comments for myself to the data block section so that I could understand what each part of the datablock is doing.\n\nimport os \niskaggle = os.environ.get('KAGGLE_KERNEL_RUN_TYPE', '')\nif iskaggle:\n    !pip install -Uqq fastai duckduckgo_search\n\n\nStep 1: Gather Data\n\nfrom duckduckgo_search import ddg_images \nfrom fastcore.all import * \n\n# helper function for searching for images \ndef search_images(term, max_images=30):\n    print(f\"Searching for '{term}'\")\n    return L(ddg_images(term, max_results=max_images)).itemgot('image')\n\n\nurls = search_images('bird photos', max_images=1)\nurls[0]\n\nSearching for 'bird photos'\n\n\n/opt/conda/lib/python3.10/site-packages/duckduckgo_search/compat.py:40: UserWarning: ddg_images is deprecated. Use DDGS().images() generator\n  warnings.warn(\"ddg_images is deprecated. Use DDGS().images() generator\")\n\n\n'https://images.alphacoders.com/492/492674.jpg'\n\n\n\nfrom fastdownload import download_url \nfrom fastai.vision.all import *\ndest = 'bird.jpg'\ndownload_url(urls[0], dest, show_progress=False)\nim = Image.open(dest)\nim.to_thumb(256, 256)\n\n\n/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version &gt;=1.16.5 and &lt;1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version &gt;={np_minversion} and &lt;{np_maxversion}\"\n\n\n\n\n\n\ndownload_url(search_images('forest photos', max_images=1)[0], 'forest.jpg', \n             show_progress=False)\nImage.open('forest.jpg').to_thumb(256, 256)\n\nSearching for 'forest photos'\n\n\n/opt/conda/lib/python3.10/site-packages/duckduckgo_search/compat.py:40: UserWarning: ddg_images is deprecated. Use DDGS().images() generator\n  warnings.warn(\"ddg_images is deprecated. Use DDGS().images() generator\")\n\n\n\n\n\n\nsearches = 'forest', 'bird'\npath = Path('bird_or_not')\nfrom time import sleep \n\nfor o in searches:\n    dest = (path/o)\n    dest.mkdir(exist_ok=True, parents=True)\n    download_images(dest, urls=search_images(f'{o} photo'))\n    sleep(10) # sleep between searches to avoid spamming server\n    download_images(dest, urls=search_images(f'{o} sun photo'))\n    sleep(10) # sleep between searches to avoid spamming server \n    download_images(dest, urls=search_images(f'{o} shade photo'))\n    sleep(10) # sleep between searches to avoid spamming server \n    resize_images(path/o, max_size=400, dest=path/o)\n\nSearching for 'forest photo'\nSearching for 'forest sun photo'\nSearching for 'forest shade photo'\nSearching for 'bird photo'\nSearching for 'bird sun photo'\nSearching for 'bird shade photo'\n\n\n/opt/conda/lib/python3.10/site-packages/duckduckgo_search/compat.py:40: UserWarning: ddg_images is deprecated. Use DDGS().images() generator\n  warnings.warn(\"ddg_images is deprecated. Use DDGS().images() generator\")\n\n\n\n\nStep 2: Train Model\n\n# remove images that failed to download properly \nfailed = verify_images(get_image_files(path))\nfailed.map(Path.unlink)\nlen(failed)\n\n3\n\n\n\n# split data into training set, validation set \ndls = DataBlock(\n    # specify input type(image), output type(category aka label)\n    blocks=(ImageBlock, CategoryBlock),\n    get_items=get_image_files,\n    # split data into 80% training data, and 20% validation data\n    splitter=RandomSplitter(valid_pct=0.2, seed=42),\n    # define the label \n    get_y=parent_label,\n    # standardize and resize all images to 192 x 192\n    item_tfms=[Resize(192, method='squish')]\n).dataloaders(path, bs=32)\n\ndls.show_batch(max_n=6)\n\n\n\n\n\n# train resnet on the data\nlearn = vision_learner(dls, resnet18, metrics=error_rate)\nlearn.fine_tune(3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n0.744936\n0.402673\n0.156250\n00:01\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n0.178611\n0.097689\n0.031250\n00:01\n\n\n1\n0.117756\n0.136063\n0.031250\n00:01\n\n\n2\n0.079709\n0.154602\n0.031250\n00:01\n\n\n\n\n\n\n\nStep 3: Test Model\n\nis_bird,_,probs = learn.predict(PILImage.create('bird.jpg'))\nprint(f\"This is a : {is_bird}.\")\nprint(f\"Probability it's a bird: {probs[0]:.4f}\")\n\n\n\n\n\n\n\n\nThis is a : bird.\nProbability it's a bird: 0.9996"
  },
  {
    "objectID": "posts/fastai-lesson1/fastai-lesson1.html#picasso-cubist-painting-or-georges-braque-cubist-painting",
    "href": "posts/fastai-lesson1/fastai-lesson1.html#picasso-cubist-painting-or-georges-braque-cubist-painting",
    "title": "FastAI Lesson 1: Getting Started",
    "section": "Picasso Cubist Painting or Georges Braque Cubist Painting",
    "text": "Picasso Cubist Painting or Georges Braque Cubist Painting\nThis code was written by me based on Jeremy and FastAI’s bird example. I picked this example because of an interest in art, the similarity between Picasso and Braque’s cubist art, and the question of whether a computer would be able to tell the difference between the two artists when shown a random cubist painting by either Picasso or Braque.\n\nStep 1: Gather Data\n\nurls = search_images('Pablo Picasso Cubist Painting', max_images=1)\nurls[0]\n\nSearching for 'Pablo Picasso Cubist Painting'\n\n\n/opt/conda/lib/python3.10/site-packages/duckduckgo_search/compat.py:40: UserWarning: ddg_images is deprecated. Use DDGS().images() generator\n  warnings.warn(\"ddg_images is deprecated. Use DDGS().images() generator\")\n\n\n'http://www.baaqii.com/promanage/productimage/OP/OP0288.jpg'\n\n\n\nfrom fastdownload import download_url \nfrom fastai.vision.all import *\ndest = 'picasso.jpg'\ndownload_url(urls[0], dest, show_progress=False)\nim = Image.open(dest)\nim.to_thumb(256, 256)\n\n\n\n\n\ndownload_url(search_images('Georges Braque Cubist Painting', max_images=1)[0], \n             'braque.jpg', \n             show_progress=False)\nImage.open('braque.jpg').to_thumb(256, 256)\n\nSearching for 'George Braque Cubist Painting'\n\n\n\n\n\n\nsearches = 'Pablo Picasso', 'Georges Braque'\npath = Path('picasso_or_braque')\nfrom time import sleep \n\nfor o in searches:\n    dest = (path/o)\n    dest.mkdir(exist_ok=True, parents=True)\n    download_images(dest, urls=search_images(f'{o} cubist painting'))\n    sleep(10) # sleep between searches to avoid spamming server\n    download_images(dest, urls=search_images(f'{o} fauvist painting'))\n    sleep(10) # sleep between searches to avoid spamming server \n    download_images(dest, urls=search_images(f'{o} geometric painting'))\n    sleep(10) # sleep between searches to avoid spamming server \n    resize_images(path/o, max_size=400, dest=path/o)\n\nSearching for 'Pablo Picasso cubist painting'\nSearching for 'Pablo Picasso fauvist painting'\nSearching for 'Pablo Picasso geometric painting'\nSearching for 'George Braque cubist painting'\nSearching for 'George Braque fauvist painting'\nSearching for 'George Braque geometric painting'\n\n\n\n\nStep 2: Train Model\n\n# remove images that failed to download properly \nfailed = verify_images(get_image_files(path))\nfailed.map(Path.unlink)\nlen(failed)\n\n2\n\n\n\n# split data into training set, validation set \ndls = DataBlock(\n    # specify input type(image), output type(category aka label)\n    blocks=(ImageBlock, CategoryBlock),\n    get_items=get_image_files,\n    # split data into 80% training data, and 20% validation data\n    splitter=RandomSplitter(valid_pct=0.2, seed=42),\n    # define the label \n    get_y=parent_label,\n    # standardize and resize all images to 192 x 192\n    item_tfms=[Resize(192, method='squish')]\n).dataloaders(path, bs=32)\n\ndls.show_batch(max_n=6)\n\n\n\n\n\n# train resnet on the data\nlearn = vision_learner(dls, resnet18, metrics=error_rate)\nlearn.fine_tune(3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n0.963409\n0.409861\n0.157143\n00:01\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n0.281930\n0.232642\n0.057143\n00:01\n\n\n1\n0.227495\n0.084337\n0.028571\n00:01\n\n\n2\n0.163790\n0.051675\n0.014286\n00:01\n\n\n\n\n\n\n\nStep 3: Test Model\nThere is something wrong here because the probability is incorrect. I can’t figure out whether I messed up in the data phase, architecture choice (resnet34 instead of resnet18), or a programming mistake. My guess is that the program should have a probability of choosing the right result around the same percentage as the bird example. If anyone knows what the issue might be, please reach out.\n\nis_picasso,_,probs = learn.predict(PILImage.create('picasso.jpg'))\nprint(f\"This is a: {is_picasso}.\")\nprint(f\"Probability it's a picasso: {probs[0]:.4f}\")\n\n\n\n\n\n\n\n\nThis is a: Pablo Picasso.\nProbability it's a picasso: 0.0006"
  },
  {
    "objectID": "posts/fastai-lesson1/fastai-lesson1.html#resources",
    "href": "posts/fastai-lesson1/fastai-lesson1.html#resources",
    "title": "FastAI Lesson 1: Getting Started",
    "section": "Resources",
    "text": "Resources\n\nFastAI Lesson 1\nFastAI Chapter 1\nJupyter Notebook 101\nIs it a bird? Creating a model from your own data\nfastai computer vision tutorial"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Posts",
    "section": "",
    "text": "FastAI Lesson 5: From-scratch model\n\n\n\n\n\n\n\nlearning\n\n\nfastai\n\n\ndeep learning\n\n\n\n\nFastAI Lesson 5 Notes\n\n\n\n\n\n\nJan 17, 2024\n\n\nPranav Rajan\n\n\n\n\n\n\n  \n\n\n\n\nFastAI Lesson 4: Natural Language(NLP)\n\n\n\n\n\n\n\nlearning\n\n\nfastai\n\n\ndeep learning\n\n\n\n\nFastAI Lesson 4 Notes\n\n\n\n\n\n\nJan 9, 2024\n\n\nPranav Rajan\n\n\n\n\n\n\n  \n\n\n\n\nFastAI Lesson 3: Neural Network Foundations\n\n\n\n\n\n\n\nlearning\n\n\nfastai\n\n\ndeep learning\n\n\n\n\nFastAI Lesson 3 Notes\n\n\n\n\n\n\nDec 18, 2023\n\n\nPranav Rajan\n\n\n\n\n\n\n  \n\n\n\n\nFastAI Lesson 2: Production\n\n\n\n\n\n\n\nlearning\n\n\nfastai\n\n\ndeep learning\n\n\n\n\nFastAI Lesson 2 Notes\n\n\n\n\n\n\nNov 30, 2023\n\n\nPranav Rajan\n\n\n\n\n\n\n  \n\n\n\n\nFastAI Lesson 1: Getting Started\n\n\n\n\n\n\n\nlearning\n\n\nfastai\n\n\ndeep learning\n\n\n\n\nFastAI Lesson 1 Notes\n\n\n\n\n\n\nNov 21, 2023\n\n\nPranav Rajan\n\n\n\n\n\n\n  \n\n\n\n\nFastAI Lesson 0: How to FastAI\n\n\n\n\n\n\n\nlearning\n\n\nfastai\n\n\ndeep learning\n\n\n\n\nFastAI Lesson 0 Notes\n\n\n\n\n\n\nNov 19, 2023\n\n\nPranav Rajan\n\n\n\n\n\n\n  \n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\n\nnews\n\n\n\n\nFirst Blog Post\n\n\n\n\n\n\nNov 19, 2023\n\n\nPranav Rajan\n\n\n\n\n\n\nNo matching items"
  }
]